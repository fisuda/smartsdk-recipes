{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome\n\n\nThis site is dedicated to the documentation of the recipes brought by\n\nSmartSDK\n to launch, test and experiment different\n\nFIWARE Generic Enablers\n and combinations of them known as common use cases.\n\n\nRecipes will be based on \nDocker\n and maybe some complementary scripts. For simple cases, you may be able to run them on a single Docker host using \ndocker-compose\n. However, the more interesting scenarios are aimed to be run in a \nDocker Swarm\n cluster.\n\n\nRecipes are organized in folders respecting the \nFIWARE Chapters\n and the \nGeneric Enablers\n they are aimed to work with. Shared functionality can be found in the \nutils\n folder.\n\n\nGetting started\n\n\nHead first to the \nInstallation\n section and follow the instructions to get the code and set-up your environment.\n\n\nThen, feel free to explore the different recipes and their referenced parts of the corresponding FIWARE documentation.\n\n\nDon't miss the \nContributing\n section in case you'd like to improve this repo.\n\n\nAlso, you may want to have a look at the \nTools\n section, specially if you want to test the recipes in a local environment.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "This site is dedicated to the documentation of the recipes brought by SmartSDK  to launch, test and experiment different FIWARE Generic Enablers  and combinations of them known as common use cases.  Recipes will be based on  Docker  and maybe some complementary scripts. For simple cases, you may be able to run them on a single Docker host using  docker-compose . However, the more interesting scenarios are aimed to be run in a  Docker Swarm  cluster.  Recipes are organized in folders respecting the  FIWARE Chapters  and the  Generic Enablers  they are aimed to work with. Shared functionality can be found in the  utils  folder.", 
            "title": "Welcome"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Head first to the  Installation  section and follow the instructions to get the code and set-up your environment.  Then, feel free to explore the different recipes and their referenced parts of the corresponding FIWARE documentation.  Don't miss the  Contributing  section in case you'd like to improve this repo.  Also, you may want to have a look at the  Tools  section, specially if you want to test the recipes in a local environment.", 
            "title": "Getting started"
        }, 
        {
            "location": "/installation/", 
            "text": "Getting the Recipes\n\n\nGet the latest version from the git repository.\n\n\n$ git clone https://github.com/smartsdk/smartsdk-recipes\n\n\n\nRequirements\n\n\nThe recipes are prepared to run using the latest \nDocker\n version (minimum 1.13+, ideally 17.06.0+). To install Docker refer to the \nInstallation instructions\n.\n\n\nFor some testing and walkthroughs, you may also need to install \ncurl\n if it's not already available in your system.\n\n\nFinally, you should install \nVirtualBox\n if you want to create clusters in your local environment to test the recipes (see next section).\n\n\nNote For Windows Users: Many of the walkthroughs and verification steps are designed to run tools typically found in a Linux/macOS environment. Therefore, you will need to consider compatible workarounds from time to time.\n\n\nPreparing a Local Swarm Cluster\n\n\nCreating the Cluster\n\n\nAlthough you can run most (if not all) of the recipes using \ndocker-compose\n, the recipes are tailored to be deployed as services on Docker Swarm Clusters.\n\n\nYou can turn your local Docker client into a single-node Swarm cluster by simply running\n\n\n$ docker swarm init\n\n\n\nHowever, things get more interesting when you're actually working on a multi-node cluster.\n\n\nThe fastest way to create one is using \nminiswarm\n. Getting started is as simple as:\n\n\n# First-time only to install miniswarm\n$ curl -sSL https://raw.githubusercontent.com/aelsabbahy/miniswarm/master/miniswarm -o /usr/local/bin/miniswarm\n$ chmod +rx /usr/local/bin/miniswarm\n\n# Every time you create/destroy a swarm\n$ miniswarm start 3\n$ miniswarm delete\n\n\n\nOtherwise, you can create your own using \ndocker-machine\n.\n\n\nCreating the networks\n\n\nFor convenience reasons, most if not all of the recipes will be using overlay networks to connect the services. We have agreed-upon the convention of having at least two overlay networks available: \"backend\" and \"frontend\". The latest typically connects services that need some exposure to the outside world.\n\n\nIf you want to buy you some time, you can now create the two networks before starting the trials with the recipes. This can be done by running the following commands:\n\n\n$ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} backend\n$ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} frontend\n\n\n\nOr, if you are lazy, there is a script in the \ntools\n folder.\n\n\n$ sh tools/create_networks.sh\n\n\n\nAgain, this is a convention to simplify the experimentation with the recipes. In the end, you may want to edit the recipes to adapt to your specific networking needs.\n\n\nOn virtualised environments\n\n\nIf you are running the recipes in a virtualised environment such as your FIWARE\nLab, if at some point you experience problems with the connectivity of the containers to the outside world, chances are that the cause of the package dropping is due to a mismatch of the \nMTU\n settings.\n\n\nIn FIWARE Lab, the default MTU for the vm's bridge is set to 1400, hence you will notice that this is the default MTU for the networks used in the recipes. If you need to change that value, feel free to set a \nDOCKER_MTU\n env variable with the value you want before you create the networks.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#getting-the-recipes", 
            "text": "Get the latest version from the git repository.  $ git clone https://github.com/smartsdk/smartsdk-recipes", 
            "title": "Getting the Recipes"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "The recipes are prepared to run using the latest  Docker  version (minimum 1.13+, ideally 17.06.0+). To install Docker refer to the  Installation instructions .  For some testing and walkthroughs, you may also need to install  curl  if it's not already available in your system.  Finally, you should install  VirtualBox  if you want to create clusters in your local environment to test the recipes (see next section).  Note For Windows Users: Many of the walkthroughs and verification steps are designed to run tools typically found in a Linux/macOS environment. Therefore, you will need to consider compatible workarounds from time to time.", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#preparing-a-local-swarm-cluster", 
            "text": "", 
            "title": "Preparing a Local Swarm Cluster"
        }, 
        {
            "location": "/installation/#creating-the-cluster", 
            "text": "Although you can run most (if not all) of the recipes using  docker-compose , the recipes are tailored to be deployed as services on Docker Swarm Clusters.  You can turn your local Docker client into a single-node Swarm cluster by simply running  $ docker swarm init  However, things get more interesting when you're actually working on a multi-node cluster.  The fastest way to create one is using  miniswarm . Getting started is as simple as:  # First-time only to install miniswarm\n$ curl -sSL https://raw.githubusercontent.com/aelsabbahy/miniswarm/master/miniswarm -o /usr/local/bin/miniswarm\n$ chmod +rx /usr/local/bin/miniswarm\n\n# Every time you create/destroy a swarm\n$ miniswarm start 3\n$ miniswarm delete  Otherwise, you can create your own using  docker-machine .", 
            "title": "Creating the Cluster"
        }, 
        {
            "location": "/installation/#creating-the-networks", 
            "text": "For convenience reasons, most if not all of the recipes will be using overlay networks to connect the services. We have agreed-upon the convention of having at least two overlay networks available: \"backend\" and \"frontend\". The latest typically connects services that need some exposure to the outside world.  If you want to buy you some time, you can now create the two networks before starting the trials with the recipes. This can be done by running the following commands:  $ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} backend\n$ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} frontend  Or, if you are lazy, there is a script in the  tools  folder.  $ sh tools/create_networks.sh  Again, this is a convention to simplify the experimentation with the recipes. In the end, you may want to edit the recipes to adapt to your specific networking needs.", 
            "title": "Creating the networks"
        }, 
        {
            "location": "/installation/#on-virtualised-environments", 
            "text": "If you are running the recipes in a virtualised environment such as your FIWARE\nLab, if at some point you experience problems with the connectivity of the containers to the outside world, chances are that the cause of the package dropping is due to a mismatch of the  MTU  settings.  In FIWARE Lab, the default MTU for the vm's bridge is set to 1400, hence you will notice that this is the default MTU for the networks used in the recipes. If you need to change that value, feel free to set a  DOCKER_MTU  env variable with the value you want before you create the networks.", 
            "title": "On virtualised environments"
        }, 
        {
            "location": "/data-management/context-broker/readme/", 
            "text": "Orion\n\n\nHere you can find recipes aimed at different usages of the Orion Context Broker. We assume you are already familiar with Orion. If not, refer to the \nofficial documentation\n.\n\n\nThe easiest and simplest way to try Orion is as explained in \nOrion's official docker image docs\n using  \nthis docker-compose\n file. But here, we will explore a distributed configuration for this Generic Enabler.\n\n\nIf you are new to docker as well you probably want to start looking at the \nsimple recipe\n to have a first view of how a single-host simple scenario looks like.\n\n\nInstructions on how to prepare your environment to test these recipes are given in \nhttps://github.com/smartsdk/smartsdk-recipes\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/context-broker/readme/#orion", 
            "text": "Here you can find recipes aimed at different usages of the Orion Context Broker. We assume you are already familiar with Orion. If not, refer to the  official documentation .  The easiest and simplest way to try Orion is as explained in  Orion's official docker image docs  using   this docker-compose  file. But here, we will explore a distributed configuration for this Generic Enabler.  If you are new to docker as well you probably want to start looking at the  simple recipe  to have a first view of how a single-host simple scenario looks like.  Instructions on how to prepare your environment to test these recipes are given in  https://github.com/smartsdk/smartsdk-recipes .", 
            "title": "Orion"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/", 
            "text": "Single Host Scenario\n\n\nIntroduction\n\n\nThis simple recipe triggers an \nOrion Context Broker\n instance backed with a \nMongoDB\n instance everything running \non an single host\n.\n\n\n\n\nBoth services will be running in docker containers, defined in the \n./docker-compose.yml\n file.\n\n\nData will be persisted in a local folder defined by the value of \nDATA_PATH\n variable in the \n.env\n file.\n\n\nHow to use\n\n\nThis recipes has some default values, but optionally you can explore different configurations by modifying the \n.env\n file, the \ndocker-compose.yml\n or even the \nscripts/setup.sh\n.\n\n\nThen, from this folder simply run:\n\n\n$ docker-compose up -d\n\n\n\nHow to validate\n\n\nBefore testing make sure docker finished downloading the images and spinning-off the containers. You can check that by running:\n\n\n$ docker ps\n\n\n\nYou should see the two containers listed and with status \"up\".\n\n\nThen, to test if orion is truly up and running run:\n\n\n$ sh ../query.sh\n\n\n\nIt should return something like:\n\n\n{\n\"orion\" : {\n  \"version\" : \"1.6.0-next\",\n  \"uptime\" : \"0 d, 0 h, 5 m, 24 s\",\n  \"git_hash\" : \"61be6c26c59469621a664d7aeb1490d6363cad38\",\n  \"compile_time\" : \"Tue Jan 24 10:52:30 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]", 
            "title": "Simple"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#single-host-scenario", 
            "text": "", 
            "title": "Single Host Scenario"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#introduction", 
            "text": "This simple recipe triggers an  Orion Context Broker  instance backed with a  MongoDB  instance everything running  on an single host .   Both services will be running in docker containers, defined in the  ./docker-compose.yml  file.  Data will be persisted in a local folder defined by the value of  DATA_PATH  variable in the  .env  file.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#how-to-use", 
            "text": "This recipes has some default values, but optionally you can explore different configurations by modifying the  .env  file, the  docker-compose.yml  or even the  scripts/setup.sh .  Then, from this folder simply run:  $ docker-compose up -d", 
            "title": "How to use"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#how-to-validate", 
            "text": "Before testing make sure docker finished downloading the images and spinning-off the containers. You can check that by running:  $ docker ps  You should see the two containers listed and with status \"up\".  Then, to test if orion is truly up and running run:  $ sh ../query.sh  It should return something like:  {\n\"orion\" : {\n  \"version\" : \"1.6.0-next\",\n  \"uptime\" : \"0 d, 0 h, 5 m, 24 s\",\n  \"git_hash\" : \"61be6c26c59469621a664d7aeb1490d6363cad38\",\n  \"compile_time\" : \"Tue Jan 24 10:52:30 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]", 
            "title": "How to validate"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/", 
            "text": "Orion in HA\n\n\nThis recipe shows how to deploy an scalable \nOrion Context Broker\n service backed with an scalable \nreplica set\n of MongoDB instances.\n\n\nAll elements will be running in docker containers, defined in docker-compose files. Actually, this recipe focuses on the deployment of the Orion frontend, reusing the \nmongodb replica recipe\n for its backend.\n\n\nThe final deployment is represented by the following picture:\n\n\n\n\nIMPORTANT:\n As explained in the \nmongo replica recipe\n, that recipe is not ready for production deployments for security reasons. Look at the \n\"Further Improvements\"\n section for more details.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\n$ miniswarm start 3\n$ eval $(docker-machine env ms-manager0)\n\n\n\nThen, simply run...\n\n\n$ sh deploy_back.sh\n\n\n\nWait some time until the backend is ready and then...\n\n\n$ sh deploy_front.sh\n\n\n\nAt some point, your deployment should look like this...\n\n\n$ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nnrxbm6k0a2yn  orion-backend_mongo             global      3/3       mongo:3.2\nrgws8vumqye2  orion-backend_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\nzk7nu592vsde  orion_orion                     replicated  3/3       fiware/orion:1.3.0\n\n\n\nAs shown above, if you see \n3/3\n in the replicas column it means the 3 replicas are up and running.\n\n\nA walkthrough\n\n\nYou can check the distribution of the containers of a service (a.k.a tasks) through the swarm running the following...\n\n\n$ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE               ERROR  PORTS\nwwgt3q6nqqg3  orion_orion.1  fiware/orion:1.3.0  ms-worker0   Running        Running 9 minutes ago          \nl1wavgqra8ry  orion_orion.2  fiware/orion:1.3.0  ms-worker1   Running        Running 9 minutes ago          \nz20v0pnym8ky  orion_orion.3  fiware/orion:1.3.0  ms-manager0  Running        Running 25 minutes ago\n\n\n\nThe good news is that, as you can see from the above output, by default docker already took care of deploying all the replicas of the service \ncontext-broker_orion\n to different hosts.\n\n\nOf course, with the use of labels, constraints or deploying mode you have the power to customize the distribution of tasks among swarm nodes. You can see the \nmongo replica recipe\n to understand the deployment of the \nmongo-replica_mongo\n service.\n\n\nNow, let's query Orion to check it's truly up and running. The question now is... where is Orion actually running? We'll cover the network internals later, but for now let's query the manager node...\n\n\n$ sh ../query.sh $(docker-machine ip ms-manager0)\n\n\n\nYou will get something like...\n\n\n{\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 18 m, 13 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]\n\n\n\nThanks to the docker swarm internal routing mesh, you can actually perform the previous query to any node of the swarm, it will be redirected to a node where the request on port 1026 can be attended (i.e, any node running Orion).\n\n\nLet's insert some data...\n\n\n$ sh ../insert.sh $(docker-machine ip ms-worker1)\n\n\n\nAnd check it's there...\n\n\n$ sh ../query.sh $(docker-machine ip ms-worker0)\n...\n[\n    {\n        \"id\": \"Room1\",\n        \"pressure\": {\n            \"metadata\": {},\n            \"type\": \"Integer\",\n            \"value\": 720\n        },\n        \"temperature\": {\n            \"metadata\": {},\n            \"type\": \"Float\",\n            \"value\": 23\n        },\n        \"type\": \"Room\"\n    }\n]\n\n\n\nYes, you can query any of the three nodes.\n\n\nSwarm's internal load balancer will be load-balancing in a round-robin approach all the requests for an orion service among the orion tasks running in the swarm.\n\n\nRescaling Orion\n\n\nScaling up and down orion is a simple as runnnig something like...\n\n\n$ docker service scale orion_orion=2\n\n\n\n(this maps to the \n\"replicas\"\n argument in the docker-compose)\n\n\nConsequently, one of the nodes (ms-worker1 in my case) is no longer running Orion...\n\n\n$ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE           ERROR  PORTS\n2tibpye24o5q  orion_orion.2  fiware/orion:1.3.0  ms-manager0  Running        Running 11 minutes ago         \nw9zmn8pp61ql  orion_orion.3  fiware/orion:1.3.0  ms-worker0   Running        Running 11 minutes ago\n\n\n\nBut still responds to the querying as mentioned above...\n\n\n$ sh ../query.sh $(docker-machine ip ms-worker1)\n{\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 14 m, 30 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]\n\n\n\nYou can see the \nmongo replica recipe\n to see how to scale the mongodb backend. But basically, due to the fact that it's a \"global\" service, you can scale it down like shown before. However, scaling it up might require adding a new node to the swarm because there can be only one instance per node.\n\n\nDealing with failures\n\n\nDocker is taking care of the reconciliation of the services in case a container goes down. Let's show this by running the following (always on the manager node):\n\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\nabc5e37037f0        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   2 minutes ago       Up 2 minutes        1026/tcp            orion_orion.1.o9ebbardwvzn1gr11pmf61er8\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n\n\nSuppose orion container goes down...\n\n\n$ docker rm -f abc5e37037f0\n\n\n\nYou will see it gone, but after a while it will automatically come back.\n\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS                  PORTS               NAMES\n60ba3f431d9d        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   6 seconds ago       Up Less than a second   1026/tcp            orion_orion.1.uj1gghehb2s1gnoestup2ugs5\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                            orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour        27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n\n\nEven if a whole node goes down, the service will remain working because you had both redundant orion instances and redundant db replicas.\n\n\n$ docker-machine rm ms-worker0\n\n\n\nYou will still get replies to...\n\n\n$ sh ../query.sh $(docker-machine ip ms-manager0)\n$ sh ../query.sh $(docker-machine ip ms-worker1)\n\n\n\nNetworks considerations\n\n\nIn this case, all containers are attached to the same overlay network (backend) over which they communicate to each other. However, if you have a different configuration and are running any of the containers behind a firewall, remember to keep traffic open for TCP at ports 1026 (Orion's default) and 27017 (Mongo's default).\n\n\nWhen containers (tasks) of a service are launched, they get assigned an IP address in this overlay network. Other services of your application's architecture should not be relying on these IPs because they may change (for example, due to a dynamic rescheduling). The good think is that docker creates a virtual ip for the service as a whole, so all traffic to this address will be load-balanced to the tasks adresses.\n\n\nThanks to swarms docker internal DNS you can also use the name of the service to connect to. If you look at the \ndocker-compose.yml\n file of this recipe, orion is started with the name of the mongo service as \ndbhost\n param (regardless if it was a single mongo instance of a whole replica-set).\n\n\nHowever, to access the container from outside of the overlay network (for example from the host) you would need to access the ip of the container's interface to the \ndocker_gwbridge\n. It seem there's no easy way to get that information from the outside (see \nthis open issue\n. In the walkthrough, we queried orion through one of the swarm nodes because we rely on docker ingress network routing the traffic all the way to one of the containerized orion services.\n\n\nOpen interesting issues:\n\n\n\n\nhttps://github.com/docker/swarm/issues/1106\n\n\nhttps://github.com/docker/docker/issues/27082\n\n\nhttps://github.com/docker/docker/issues/29816\n\n\nhttps://github.com/docker/docker/issues/26696\n\n\nhttps://github.com/docker/docker/issues/23813\n\n\n\n\nMore info about docker network internals can be read at:\n\n\n\n\nDocker Reference Architecture", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#orion-in-ha", 
            "text": "This recipe shows how to deploy an scalable  Orion Context Broker  service backed with an scalable  replica set  of MongoDB instances.  All elements will be running in docker containers, defined in docker-compose files. Actually, this recipe focuses on the deployment of the Orion frontend, reusing the  mongodb replica recipe  for its backend.  The final deployment is represented by the following picture:   IMPORTANT:  As explained in the  mongo replica recipe , that recipe is not ready for production deployments for security reasons. Look at the  \"Further Improvements\"  section for more details.", 
            "title": "Orion in HA"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  $ miniswarm start 3\n$ eval $(docker-machine env ms-manager0)  Then, simply run...  $ sh deploy_back.sh  Wait some time until the backend is ready and then...  $ sh deploy_front.sh  At some point, your deployment should look like this...  $ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nnrxbm6k0a2yn  orion-backend_mongo             global      3/3       mongo:3.2\nrgws8vumqye2  orion-backend_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\nzk7nu592vsde  orion_orion                     replicated  3/3       fiware/orion:1.3.0  As shown above, if you see  3/3  in the replicas column it means the 3 replicas are up and running.", 
            "title": "How to use"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#a-walkthrough", 
            "text": "You can check the distribution of the containers of a service (a.k.a tasks) through the swarm running the following...  $ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE               ERROR  PORTS\nwwgt3q6nqqg3  orion_orion.1  fiware/orion:1.3.0  ms-worker0   Running        Running 9 minutes ago          \nl1wavgqra8ry  orion_orion.2  fiware/orion:1.3.0  ms-worker1   Running        Running 9 minutes ago          \nz20v0pnym8ky  orion_orion.3  fiware/orion:1.3.0  ms-manager0  Running        Running 25 minutes ago  The good news is that, as you can see from the above output, by default docker already took care of deploying all the replicas of the service  context-broker_orion  to different hosts.  Of course, with the use of labels, constraints or deploying mode you have the power to customize the distribution of tasks among swarm nodes. You can see the  mongo replica recipe  to understand the deployment of the  mongo-replica_mongo  service.  Now, let's query Orion to check it's truly up and running. The question now is... where is Orion actually running? We'll cover the network internals later, but for now let's query the manager node...  $ sh ../query.sh $(docker-machine ip ms-manager0)  You will get something like...  {\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 18 m, 13 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]  Thanks to the docker swarm internal routing mesh, you can actually perform the previous query to any node of the swarm, it will be redirected to a node where the request on port 1026 can be attended (i.e, any node running Orion).  Let's insert some data...  $ sh ../insert.sh $(docker-machine ip ms-worker1)  And check it's there...  $ sh ../query.sh $(docker-machine ip ms-worker0)\n...\n[\n    {\n        \"id\": \"Room1\",\n        \"pressure\": {\n            \"metadata\": {},\n            \"type\": \"Integer\",\n            \"value\": 720\n        },\n        \"temperature\": {\n            \"metadata\": {},\n            \"type\": \"Float\",\n            \"value\": 23\n        },\n        \"type\": \"Room\"\n    }\n]  Yes, you can query any of the three nodes.  Swarm's internal load balancer will be load-balancing in a round-robin approach all the requests for an orion service among the orion tasks running in the swarm.", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#rescaling-orion", 
            "text": "Scaling up and down orion is a simple as runnnig something like...  $ docker service scale orion_orion=2  (this maps to the  \"replicas\"  argument in the docker-compose)  Consequently, one of the nodes (ms-worker1 in my case) is no longer running Orion...  $ docker service ps orion_orion\nID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE           ERROR  PORTS\n2tibpye24o5q  orion_orion.2  fiware/orion:1.3.0  ms-manager0  Running        Running 11 minutes ago         \nw9zmn8pp61ql  orion_orion.3  fiware/orion:1.3.0  ms-worker0   Running        Running 11 minutes ago  But still responds to the querying as mentioned above...  $ sh ../query.sh $(docker-machine ip ms-worker1)\n{\n  \"orion\" : {\n  \"version\" : \"1.3.0\",\n  \"uptime\" : \"0 d, 0 h, 14 m, 30 s\",\n  \"git_hash\" : \"cb6813f044607bc01895296223a27e4466ab0913\",\n  \"compile_time\" : \"Fri Sep 2 08:19:12 UTC 2016\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"ba19f7d3be65\"\n}\n}\n[]  You can see the  mongo replica recipe  to see how to scale the mongodb backend. But basically, due to the fact that it's a \"global\" service, you can scale it down like shown before. However, scaling it up might require adding a new node to the swarm because there can be only one instance per node.", 
            "title": "Rescaling Orion"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#dealing-with-failures", 
            "text": "Docker is taking care of the reconciliation of the services in case a container goes down. Let's show this by running the following (always on the manager node):  $ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\nabc5e37037f0        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   2 minutes ago       Up 2 minutes        1026/tcp            orion_orion.1.o9ebbardwvzn1gr11pmf61er8\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz  Suppose orion container goes down...  $ docker rm -f abc5e37037f0  You will see it gone, but after a while it will automatically come back.  $ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                        orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour    27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n$ docker ps\nCONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS                  PORTS               NAMES\n60ba3f431d9d        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \"/usr/bin/contextB...\"   6 seconds ago       Up Less than a second   1026/tcp            orion_orion.1.uj1gghehb2s1gnoestup2ugs5\n1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \"python /src/repli...\"   About an hour ago   Up About an hour                            orion-backend_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \"/usr/bin/mongod -...\"   About an hour ago   Up About an hour        27017/tcp           orion-backend_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz  Even if a whole node goes down, the service will remain working because you had both redundant orion instances and redundant db replicas.  $ docker-machine rm ms-worker0  You will still get replies to...  $ sh ../query.sh $(docker-machine ip ms-manager0)\n$ sh ../query.sh $(docker-machine ip ms-worker1)", 
            "title": "Dealing with failures"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#networks-considerations", 
            "text": "In this case, all containers are attached to the same overlay network (backend) over which they communicate to each other. However, if you have a different configuration and are running any of the containers behind a firewall, remember to keep traffic open for TCP at ports 1026 (Orion's default) and 27017 (Mongo's default).  When containers (tasks) of a service are launched, they get assigned an IP address in this overlay network. Other services of your application's architecture should not be relying on these IPs because they may change (for example, due to a dynamic rescheduling). The good think is that docker creates a virtual ip for the service as a whole, so all traffic to this address will be load-balanced to the tasks adresses.  Thanks to swarms docker internal DNS you can also use the name of the service to connect to. If you look at the  docker-compose.yml  file of this recipe, orion is started with the name of the mongo service as  dbhost  param (regardless if it was a single mongo instance of a whole replica-set).  However, to access the container from outside of the overlay network (for example from the host) you would need to access the ip of the container's interface to the  docker_gwbridge . It seem there's no easy way to get that information from the outside (see  this open issue . In the walkthrough, we queried orion through one of the swarm nodes because we rely on docker ingress network routing the traffic all the way to one of the containerized orion services.", 
            "title": "Networks considerations"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#open-interesting-issues", 
            "text": "https://github.com/docker/swarm/issues/1106  https://github.com/docker/docker/issues/27082  https://github.com/docker/docker/issues/29816  https://github.com/docker/docker/issues/26696  https://github.com/docker/docker/issues/23813   More info about docker network internals can be read at:   Docker Reference Architecture", 
            "title": "Open interesting issues:"
        }, 
        {
            "location": "/data-management/sth/readme/", 
            "text": "Comet\n\n\nHere you can find recipes aimed at different usages of Comet, the Reference Implementation of the STH Generic Enabler. We assume you are already familiar with Comet. If not, refer to the \nofficial documentation\n.\n\n\nThe easiest and simplest way to try Comet is to follow the \nStandalone Walkthrough\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given in \nhttps://github.com/smartsdk/smartsdk-recipes\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/sth/readme/#comet", 
            "text": "Here you can find recipes aimed at different usages of Comet, the Reference Implementation of the STH Generic Enabler. We assume you are already familiar with Comet. If not, refer to the  official documentation .  The easiest and simplest way to try Comet is to follow the  Standalone Walkthrough .  Instructions on how to prepare your environment to test these recipes are given in  https://github.com/smartsdk/smartsdk-recipes .", 
            "title": "Comet"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/", 
            "text": "Standalone\n\n\nIntroduction\n\n\nThe idea of this standalone walkthrough is to test and showcase the Comet Generic Enabler within a simple notification-based scenario, like the one illustrated below.\n\n\n\n\nA walkthrough\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\nminiswarm start 3\neval $(docker-machine env ms-manager0)\n\n\n\nTo start the whole stack simply run, as usual:\n\n\ndocker stack deploy -c docker-compose.yml comet\n\n\n\nThen, wait until you see all the replicas up and running:\n\n\ndocker service ls\nID            NAME               MODE        REPLICAS  IMAGE\n1ysxmrxrqvp4  comet_comet-mongo  replicated  1/1       mongo:3.2\n8s9acybjxo0m  comet_orion        replicated  1/1       fiware/orion:latest\nra84eex0zsd0  comet_comet        replicated  3/3       telefonicaiot/fiware-sth-comet:latest\nxg8ds3szkoi7  comet_orion-mongo  replicated  1/1       mongo:3.2\n\n\n\nNow let's start some checkups. For convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.\n\n\nORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)\n\n\n\nLet's start some checkups, first making sure Orion is up and running.\n\n\nsh ../../context-broker/query.sh $ORION\n{\n\"orion\" : {\n  \"version\" : \"1.7.0-next\",\n  \"uptime\" : \"0 d, 0 h, 1 m, 39 s\",\n  \"git_hash\" : \"f710ee525f0fa55f665e578e309fc716c12cfd99\",\n  \"compile_time\" : \"Wed Feb 22 10:14:18 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]\n\n\n\nLet's insert some simple data (Room1 measurements):\n\n\nsh ../../context-broker/insert.sh $ORION\n\n\n\nNow, let's subscribe Comet to the notifications of changes in temperature of Room1.\n\n\nsh ../subscribe.sh $COMET\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58b98c0cdb69948641065907\",\n    \"duration\" : \"PT24H\"\n  }\n}\n\n\n\nLet's update the temperature value in Orion...\n\n\nsh ../../context-broker/update.sh $ORION\n\n\n\nAnd check you can see the Short-Term-Historical view of both measurements.\n\n\nsh ../query_sth.sh $COMET\n{\n    \"contextResponses\": [\n        {\n            \"contextElement\": {\n                \"attributes\": [\n                    {\n                        \"name\": \"temperature\",\n                        \"values\": [\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 23,\n                                \"recvTime\": \"2017-03-03T15:30:20.650Z\"\n                            },\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 29.3,\n                                \"recvTime\": \"2017-03-03T15:32:48.741Z\"\n                            }\n                        ]\n                    }\n                ],\n                \"id\": \"Room1\",\n                \"isPattern\": false,\n                \"type\": \"Room\"\n            },\n            \"statusCode\": {\n                \"code\": \"200\",\n                \"reasonPhrase\": \"OK\"\n            }\n        }\n    ]\n}", 
            "title": "Standalone"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#standalone", 
            "text": "", 
            "title": "Standalone"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#introduction", 
            "text": "The idea of this standalone walkthrough is to test and showcase the Comet Generic Enabler within a simple notification-based scenario, like the one illustrated below.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#a-walkthrough", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  miniswarm start 3\neval $(docker-machine env ms-manager0)  To start the whole stack simply run, as usual:  docker stack deploy -c docker-compose.yml comet  Then, wait until you see all the replicas up and running:  docker service ls\nID            NAME               MODE        REPLICAS  IMAGE\n1ysxmrxrqvp4  comet_comet-mongo  replicated  1/1       mongo:3.2\n8s9acybjxo0m  comet_orion        replicated  1/1       fiware/orion:latest\nra84eex0zsd0  comet_comet        replicated  3/3       telefonicaiot/fiware-sth-comet:latest\nxg8ds3szkoi7  comet_orion-mongo  replicated  1/1       mongo:3.2  Now let's start some checkups. For convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.  ORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)  Let's start some checkups, first making sure Orion is up and running.  sh ../../context-broker/query.sh $ORION\n{\n\"orion\" : {\n  \"version\" : \"1.7.0-next\",\n  \"uptime\" : \"0 d, 0 h, 1 m, 39 s\",\n  \"git_hash\" : \"f710ee525f0fa55f665e578e309fc716c12cfd99\",\n  \"compile_time\" : \"Wed Feb 22 10:14:18 UTC 2017\",\n  \"compiled_by\" : \"root\",\n  \"compiled_in\" : \"b99744612d0b\"\n}\n}\n[]  Let's insert some simple data (Room1 measurements):  sh ../../context-broker/insert.sh $ORION  Now, let's subscribe Comet to the notifications of changes in temperature of Room1.  sh ../subscribe.sh $COMET\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58b98c0cdb69948641065907\",\n    \"duration\" : \"PT24H\"\n  }\n}  Let's update the temperature value in Orion...  sh ../../context-broker/update.sh $ORION  And check you can see the Short-Term-Historical view of both measurements.  sh ../query_sth.sh $COMET\n{\n    \"contextResponses\": [\n        {\n            \"contextElement\": {\n                \"attributes\": [\n                    {\n                        \"name\": \"temperature\",\n                        \"values\": [\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 23,\n                                \"recvTime\": \"2017-03-03T15:30:20.650Z\"\n                            },\n                            {\n                                \"attrType\": \"Float\",\n                                \"attrValue\": 29.3,\n                                \"recvTime\": \"2017-03-03T15:32:48.741Z\"\n                            }\n                        ]\n                    }\n                ],\n                \"id\": \"Room1\",\n                \"isPattern\": false,\n                \"type\": \"Room\"\n            },\n            \"statusCode\": {\n                \"code\": \"200\",\n                \"reasonPhrase\": \"OK\"\n            }\n        }\n    ]\n}", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/sth/ha/readme/", 
            "text": "HA\n\n\nIntroduction\n\n\nLet's test a deployment of Comet with multiple replicas in both its front-end and backend. The idea now is to get the scenario illustrated below.\n\n\n\n\nLater, this could be combined for example with an \nHA deployment of Orion Context Broker\n.\n\n\nA walkthrough\n\n\nFirst, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\nminiswarm start 3\neval $(docker-machine env ms-manager0)\n\n\n\nTo start the comet's backend simply run...\n\n\nsh deploy_back.sh\n\n\n\nAfter a while, when the replica-set is ready, you can deploy comet by running...\n\n\nsh deploy_front.sh\n\n\n\nNow, as usual, a brief test to confirm everything is properly connected. As a source of notifications, we have deployed Orion in the swarm (see \nOrion in HA\n for example).\n\n\nFor convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.\n\n\nORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)\n\n\n\nInsert:\n\n\nsh ../../context-broker/insert.sh $ORION\nsh ../../context-broker/query.sh $ORION\n...\n\n\n\nSubscribe:\n\n\nsh ../subscribe.sh $ORION\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58bd1940b97cc713f5eacdb7\",\n    \"duration\" : \"PT24H\"\n  }\n}\n\n\n\nUpdate:\n\n\nsh ../../context-broker/update.sh $ORION\n\n\n\nAnd voila:\n\n\nsh ../query_sth.sh $COMET\n{\n\"contextResponses\": [\n    {\n        \"contextElement\": {\n            \"attributes\": [\n                {\n                    \"name\": \"temperature\",\n                    \"values\": [\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 23,\n                            \"recvTime\": \"2017-03-06T08:09:36.493Z\"\n                        },\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 29.3,\n                            \"recvTime\": \"2017-03-06T08:11:14.044Z\"\n                        }\n                    ]\n                }\n            ],\n            \"id\": \"Room1\",\n            \"isPattern\": false,\n            \"type\": \"Room\"\n        },\n        \"statusCode\": {\n            \"code\": \"200\",\n            \"reasonPhrase\": \"OK\"\n        }\n    }\n]\n}", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#ha", 
            "text": "", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#introduction", 
            "text": "Let's test a deployment of Comet with multiple replicas in both its front-end and backend. The idea now is to get the scenario illustrated below.   Later, this could be combined for example with an  HA deployment of Orion Context Broker .", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#a-walkthrough", 
            "text": "First, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  miniswarm start 3\neval $(docker-machine env ms-manager0)  To start the comet's backend simply run...  sh deploy_back.sh  After a while, when the replica-set is ready, you can deploy comet by running...  sh deploy_front.sh  Now, as usual, a brief test to confirm everything is properly connected. As a source of notifications, we have deployed Orion in the swarm (see  Orion in HA  for example).  For convenience, let's save the IP address of the Orion and Comet services. In this scenario, since both are deployed on Swarm exposing their services ports, only one entry-point to the Swarm's ingress network will suffice.  ORION=http://$(docker-machine ip ms-manager0)\nCOMET=http://$(docker-machine ip ms-manager0)  Insert:  sh ../../context-broker/insert.sh $ORION\nsh ../../context-broker/query.sh $ORION\n...  Subscribe:  sh ../subscribe.sh $ORION\n{\n  \"subscribeResponse\" : {\n    \"subscriptionId\" : \"58bd1940b97cc713f5eacdb7\",\n    \"duration\" : \"PT24H\"\n  }\n}  Update:  sh ../../context-broker/update.sh $ORION  And voila:  sh ../query_sth.sh $COMET\n{\n\"contextResponses\": [\n    {\n        \"contextElement\": {\n            \"attributes\": [\n                {\n                    \"name\": \"temperature\",\n                    \"values\": [\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 23,\n                            \"recvTime\": \"2017-03-06T08:09:36.493Z\"\n                        },\n                        {\n                            \"attrType\": \"Float\",\n                            \"attrValue\": 29.3,\n                            \"recvTime\": \"2017-03-06T08:11:14.044Z\"\n                        }\n                    ]\n                }\n            ],\n            \"id\": \"Room1\",\n            \"isPattern\": false,\n            \"type\": \"Room\"\n        },\n        \"statusCode\": {\n            \"code\": \"200\",\n            \"reasonPhrase\": \"OK\"\n        }\n    }\n]\n}", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/cygnus/readme/", 
            "text": "Cygnus\n\n\nHere you can find recipes aimed at different usages of Cygnus, in particular, cygnus-ngsi. We assume you are already familiar with it, but if not, refer to the \nofficial documentation\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given in the \nInstallation\n section of the docs.\n\n\nSome Considerations regarding HA\n\n\nThe first thing to note is that when we talk about high availability while using cygnus, we refer to the availability of data processed by cygnus agents before it's dropped to the final storage solution. So, as you can read from the \nofficial documentation\n, different sinks can give you persistence in different storage solutions (mongodb, mysql, hdfs, etc). Keeping the persisted data in HA mode is a different challenge and the implementation will depend on the used solution. For the case of MongoDB, you can have a look at the \nMongoDB Replicaset Recipe\n. So, this recipes will show you how to connect to some backends, but how you manage them is up to you.\n\n\nMoreover, we will be discussing the deployment of the agent as a single configurable entity. But note that within an agent, there exist multiple available configurations (using single and multiple sources, channels and sinks), as described in \nAdvanced Cygnus Architectures\n. How you setup those internal advanced architectures and the advantages of each will not be covered here, since this is already discussed in the official documentation.\n\n\nThat being said, in order to deploy Cygnus, we need to understand whether it's a stateless or stateful service. It turns out that source and sink parts of the agent do not persist data, however, the channels do, at least for a short period of time until data is processed and taken out of the channel by the sink[s]. As you can read from Cygnus and Flume's documentation, channels come in the form of MemoryChannel and FileChannel.\n\n\nIt's easy to see that MemoryChannel and Batching has a potential for data loss, for example, in the case when an agent crashes before events were taken out of the channel. In fact, to avoid storage access for each event, Cygnus comes with default values of batch size and batch flush timeouts. As a side note, it'd be nice if this could be dynamically changed according to dynamic demands, but this is an interesting point to investigate at a later point.\n\n\nTherefore, the FileChannel could be used, at the cost of a higher latency, to give persistence to the \"inflight data\" and this way prevent a complete loss if there happens to be a software failure/crash at the agent. Note however that the location of this file within the container and not customizable, hence a container crash will cause those un-flushed values to be lost.\n\n\nOne could explore ways to persist those channels somewhere else, or share channels across different containers. Or maybe looking for Message-based solutions such as Solace to be used at the Channel level. But this of course, would involve some updates to Cygnus which are beyond the scope of this project.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/cygnus/readme/#cygnus", 
            "text": "Here you can find recipes aimed at different usages of Cygnus, in particular, cygnus-ngsi. We assume you are already familiar with it, but if not, refer to the  official documentation .  Instructions on how to prepare your environment to test these recipes are given in the  Installation  section of the docs.", 
            "title": "Cygnus"
        }, 
        {
            "location": "/data-management/cygnus/readme/#some-considerations-regarding-ha", 
            "text": "The first thing to note is that when we talk about high availability while using cygnus, we refer to the availability of data processed by cygnus agents before it's dropped to the final storage solution. So, as you can read from the  official documentation , different sinks can give you persistence in different storage solutions (mongodb, mysql, hdfs, etc). Keeping the persisted data in HA mode is a different challenge and the implementation will depend on the used solution. For the case of MongoDB, you can have a look at the  MongoDB Replicaset Recipe . So, this recipes will show you how to connect to some backends, but how you manage them is up to you.  Moreover, we will be discussing the deployment of the agent as a single configurable entity. But note that within an agent, there exist multiple available configurations (using single and multiple sources, channels and sinks), as described in  Advanced Cygnus Architectures . How you setup those internal advanced architectures and the advantages of each will not be covered here, since this is already discussed in the official documentation.  That being said, in order to deploy Cygnus, we need to understand whether it's a stateless or stateful service. It turns out that source and sink parts of the agent do not persist data, however, the channels do, at least for a short period of time until data is processed and taken out of the channel by the sink[s]. As you can read from Cygnus and Flume's documentation, channels come in the form of MemoryChannel and FileChannel.  It's easy to see that MemoryChannel and Batching has a potential for data loss, for example, in the case when an agent crashes before events were taken out of the channel. In fact, to avoid storage access for each event, Cygnus comes with default values of batch size and batch flush timeouts. As a side note, it'd be nice if this could be dynamically changed according to dynamic demands, but this is an interesting point to investigate at a later point.  Therefore, the FileChannel could be used, at the cost of a higher latency, to give persistence to the \"inflight data\" and this way prevent a complete loss if there happens to be a software failure/crash at the agent. Note however that the location of this file within the container and not customizable, hence a container crash will cause those un-flushed values to be lost.  One could explore ways to persist those channels somewhere else, or share channels across different containers. Or maybe looking for Message-based solutions such as Solace to be used at the Channel level. But this of course, would involve some updates to Cygnus which are beyond the scope of this project.", 
            "title": "Some Considerations regarding HA"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/", 
            "text": "Getting started\n\n\nThis recipe will show you how to deploy a default cygnus-ngsi configuration with a MySQL backend. Note that this generic enabler can actually be deployed with \nmany other backends\n.\n\n\nThis recipe in particular requires the use of \ndocker \"configs\"\n and hence depends on a docker-compose file version \"3.3\", supported in docker versions 17.06.0+.\n\n\nInstructions on how to prepare your environment to test these recipes are given in the \nInstallation\n section of the docs. Assuming you have created a 3-nodes Swarm setup, this deployment will look as follows...\n\n\n\n\nAs you may already know \nfrom the docs\n, in order to configure cygnus, you need to provide a specific agent configuration file. In this case, you can customize the \ncygnus_agent.conf\n and \ncartodb_keys.conf\n files within the \nconf\n folder. The content of these files will be loaded by docker into their corresponding configs, which will be available for all the replicas of the cygnus service.\n\n\nIf you inspect the \ndocker-compose.yml\n you will realize that you can customize the values of the MySQL user and password by setting the environment varialbes \nCYGNUS_MYSQL_USER\n and \nCYGNUS_MYSQL_PASS\n.\n\n\nTo launch the example as it is, simply run:\n\n\ndocker stack deploy -c docker-compose.yml cygnus\n\n\n\nAfter a couple of minutes you should be able to see the two services up and running.\n\n\n$ docker service ls\nID                  NAME                   MODE                REPLICAS            IMAGE                       PORTS\nl3h1fsk36v35        cygnus_mysql           replicated          3/3                 mysql:latest                *:3306-\n3306/tcp\nvmju1turlizr        cygnus_cygnus-common   replicated          3/3                 fiware/cygnus-ngsi:latest   *:5050-\n5050/tcp\n\n\n\nFor illustration purposes, let's send an NGSI notification to cygnu's entrypoint using the simple \nnotification.sh\n script.\n\n\n$ sh notification.sh http://0.0.0.0:5050/notify\n*   Trying 0.0.0.0...\n* TCP_NODELAY set\n* Connected to 0.0.0.0 (127.0.0.1) port 5050 (#0)\n\n POST /notify HTTP/1.1\n\n Host: 0.0.0.0:5050\n\n User-Agent: curl/7.54.0\n\n Content-Type: application/json; charset=utf-8\n\n Accept: application/json\n\n Fiware-Service: default\n\n Fiware-ServicePath: /\n\n Content-Length: 607\n\n\n* upload completely sent off: 607 out of 607 bytes\n\n HTTP/1.1 200 OK\n\n Transfer-Encoding: chunked\n\n Server: Jetty(6.1.26)\n\n\n* Connection #0 to host 0.0.0.0 left intact\n\n\n\nBy now, the data sent by the script has been processed by cygnus and will be available in the configured sink (MySQL in this case).\n\n\nHaving cygnus running as a service on a Docker Swarm cluster, scaling it can be achieved as with any other docker service. For more details, refer to the \nOrion recipe\n to see how this can be done with Docker. Otherwise, refer to the \nDocker service docs\n.\n\n\nWhat if I wanted a different backend?\n\n\nIf you wanted to try a different backend for your cygnus deployment, there are 3 steps you need to follow.\n\n\n\n\nConfigure your \ncygnus_agent.conf\n according to your needs. More info \nin the docs\n.\n\n\nUpdate the \ndocker-compose.yml\n, specifically the environment variables configured for the cygnus service.\n    For example, if you wanted to use MongoDB instead of MySQL, you'll need to use variables CYGNUS_MONGO_USER and CYGNUS_MONGO_PASS. For a complete list of required variables, refer to the \ncygnus docs\n.\n\n\nUpdate the \ndocker-compose.yml\n, removing the definition of the mysql service and introducing the one of your preference. Also, don't forget to update the \ndepends_on:\n section of cygnus with the name of your new service.", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#getting-started", 
            "text": "This recipe will show you how to deploy a default cygnus-ngsi configuration with a MySQL backend. Note that this generic enabler can actually be deployed with  many other backends .  This recipe in particular requires the use of  docker \"configs\"  and hence depends on a docker-compose file version \"3.3\", supported in docker versions 17.06.0+.  Instructions on how to prepare your environment to test these recipes are given in the  Installation  section of the docs. Assuming you have created a 3-nodes Swarm setup, this deployment will look as follows...   As you may already know  from the docs , in order to configure cygnus, you need to provide a specific agent configuration file. In this case, you can customize the  cygnus_agent.conf  and  cartodb_keys.conf  files within the  conf  folder. The content of these files will be loaded by docker into their corresponding configs, which will be available for all the replicas of the cygnus service.  If you inspect the  docker-compose.yml  you will realize that you can customize the values of the MySQL user and password by setting the environment varialbes  CYGNUS_MYSQL_USER  and  CYGNUS_MYSQL_PASS .  To launch the example as it is, simply run:  docker stack deploy -c docker-compose.yml cygnus  After a couple of minutes you should be able to see the two services up and running.  $ docker service ls\nID                  NAME                   MODE                REPLICAS            IMAGE                       PORTS\nl3h1fsk36v35        cygnus_mysql           replicated          3/3                 mysql:latest                *:3306- 3306/tcp\nvmju1turlizr        cygnus_cygnus-common   replicated          3/3                 fiware/cygnus-ngsi:latest   *:5050- 5050/tcp  For illustration purposes, let's send an NGSI notification to cygnu's entrypoint using the simple  notification.sh  script.  $ sh notification.sh http://0.0.0.0:5050/notify\n*   Trying 0.0.0.0...\n* TCP_NODELAY set\n* Connected to 0.0.0.0 (127.0.0.1) port 5050 (#0)  POST /notify HTTP/1.1  Host: 0.0.0.0:5050  User-Agent: curl/7.54.0  Content-Type: application/json; charset=utf-8  Accept: application/json  Fiware-Service: default  Fiware-ServicePath: /  Content-Length: 607 \n* upload completely sent off: 607 out of 607 bytes  HTTP/1.1 200 OK  Transfer-Encoding: chunked  Server: Jetty(6.1.26) \n* Connection #0 to host 0.0.0.0 left intact  By now, the data sent by the script has been processed by cygnus and will be available in the configured sink (MySQL in this case).  Having cygnus running as a service on a Docker Swarm cluster, scaling it can be achieved as with any other docker service. For more details, refer to the  Orion recipe  to see how this can be done with Docker. Otherwise, refer to the  Docker service docs .", 
            "title": "Getting started"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#what-if-i-wanted-a-different-backend", 
            "text": "If you wanted to try a different backend for your cygnus deployment, there are 3 steps you need to follow.   Configure your  cygnus_agent.conf  according to your needs. More info  in the docs .  Update the  docker-compose.yml , specifically the environment variables configured for the cygnus service.\n    For example, if you wanted to use MongoDB instead of MySQL, you'll need to use variables CYGNUS_MONGO_USER and CYGNUS_MONGO_PASS. For a complete list of required variables, refer to the  cygnus docs .  Update the  docker-compose.yml , removing the definition of the mysql service and introducing the one of your preference. Also, don't forget to update the  depends_on:  section of cygnus with the name of your new service.", 
            "title": "What if I wanted a different backend?"
        }, 
        {
            "location": "/data-management/quantumleap/readme/", 
            "text": "QuantumLeap\n\n\nIntroduction\n\n\nHere you can find recipes aimed at different usages of QuantumLeap. We assume you are already familiar with it, otherwise refer to the \nofficial documentation\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given in the \ninstallation section\n.\n\n\nHA Deployment overview\n\n\n\n\nA Simple Walkthrough\n\n\nBefore Starting\n\n\nBefore we launch the stack, you need to define a domain for the entrypoint of your docker cluster. Save that domain in an environment variable like this:\n\n\n$ export CLUSTER_DOMAIN=mydomain.com\n\n\n\nIf you are just testing locally and don't own one, you can fake it editing your /etc/hosts file to add an entry that points to the IP of any of the nodes of your Swarm Cluster. See the example below.\n\n\n# End of /etc/hosts file\n192.168.99.100  mydomain.com\n192.168.99.100  crate.mydomain.com\n\n\n\nNote we've included one entry for crate because we'll be accessing the CrateDB cluster UI through the \nTraefik\n proxy.\n\n\nDeploy\n\n\nNow, we're ready to launch the stack with the name \"ql\".\n\n\nIf you want to deploy the basic stack of QuantumLeap you can simply run...\n\n\n$ docker stack deploy -c docker-compose ql\n\n\n\nOtherwise, if you'd like to include some extra services such as Grafana for data visualisation, you can integrate the addons present in the \ndocker-compose-addons.yml\n. Unfortunately docker is currently not directly supporting \nmultiple compose files to do a single deploy\n. Hence the suggested way to proceed is the following...\n\n\n# First we merge the two compose files using docker-compose\n$ docker-compose -f docker-compose.yml -f docker-compose-addons.yml config \n ql.yml\n# Now we deploy the \"ql\" stack from the generated ql.yml file.\n$ docker stack deploy -c ql.yml ql\n\n\n\nWait until you see all instances up and running (this might take some minutes).\n\n\n$ docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE                             PORTS\n2vbj18blsqje        ql_traefik          global              1/1                 traefik:1.3.5-alpine              *:80-\n80/tcp,*:443-\n443/tcp,*:4200-\n4200/tcp,*:4300-\n4300/tcp,*:8080-\n8080/tcp\nbvs32e81jcns        ql_viz              replicated          1/1                 dockersamples/visualizer:latest   *:8282-\n8080/tcp\ne8kyp4vylvev        ql_quantumleap      replicated          1/1                 smartsdk/quantumleap:latest       *:8668-\n8668/tcp\nignls7l57hzn        ql_crate            global              3/3                 crate:1.0.5                       \ntfszxc2fcmxx        ql_grafana          replicated          1/1                 grafana/grafana:latest            *:3000-\n3000/tcp\n\n\n\nNow you are ready to scale services according to your needs using simple docker service scale command as explained in \nthe official docs\n.\n\n\nExplore\n\n\nNow, if you open your explorer to http://crate.mydomain.com you should see the CRATE.IO dashboard. In the \"cluster\" tab you should see the same number of nodes you have in the swarm cluster.\n\n\nFor a quick test, you can use the \ninsert.sh\n script in this folder.\n\n\n$ sh insert.sh IP_OF_ANY_SWARM_NODE 8668\n\n\n\nOtherwise, open your favourite API tester and send the fake notification shown below to QuantumLeap to later see it persisted in the database through the Crate Dashboard.\n\n\n# Simple fake payload to send to IP_OF_ANY_SWARM_NODE:8668.\n{\n    \"subscriptionId\": \"5947d174793fe6f7eb5e3961\",\n    \"data\": [\n        {\n            \"id\": \"Room1\",\n            \"type\": \"Room\",\n            \"temperature\": {\n                \"type\": \"Number\",\n                \"value\": 27.6,\n                \"metadata\": {\n                    \"dateModified\": {\n                        \"type\": \"DateTime\",\n                        \"value\": \"2017-06-19T11:46:45.00Z\"\n                    }\n                }\n            }\n        }\n    ]\n}\n\n\n\nFor further information, please refer to the \nQuantumLeap's User Manual\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#quantumleap", 
            "text": "", 
            "title": "QuantumLeap"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#introduction", 
            "text": "Here you can find recipes aimed at different usages of QuantumLeap. We assume you are already familiar with it, otherwise refer to the  official documentation .  Instructions on how to prepare your environment to test these recipes are given in the  installation section .", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#ha-deployment-overview", 
            "text": "", 
            "title": "HA Deployment overview"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#a-simple-walkthrough", 
            "text": "", 
            "title": "A Simple Walkthrough"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#before-starting", 
            "text": "Before we launch the stack, you need to define a domain for the entrypoint of your docker cluster. Save that domain in an environment variable like this:  $ export CLUSTER_DOMAIN=mydomain.com  If you are just testing locally and don't own one, you can fake it editing your /etc/hosts file to add an entry that points to the IP of any of the nodes of your Swarm Cluster. See the example below.  # End of /etc/hosts file\n192.168.99.100  mydomain.com\n192.168.99.100  crate.mydomain.com  Note we've included one entry for crate because we'll be accessing the CrateDB cluster UI through the  Traefik  proxy.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#deploy", 
            "text": "Now, we're ready to launch the stack with the name \"ql\".  If you want to deploy the basic stack of QuantumLeap you can simply run...  $ docker stack deploy -c docker-compose ql  Otherwise, if you'd like to include some extra services such as Grafana for data visualisation, you can integrate the addons present in the  docker-compose-addons.yml . Unfortunately docker is currently not directly supporting  multiple compose files to do a single deploy . Hence the suggested way to proceed is the following...  # First we merge the two compose files using docker-compose\n$ docker-compose -f docker-compose.yml -f docker-compose-addons.yml config   ql.yml\n# Now we deploy the \"ql\" stack from the generated ql.yml file.\n$ docker stack deploy -c ql.yml ql  Wait until you see all instances up and running (this might take some minutes).  $ docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE                             PORTS\n2vbj18blsqje        ql_traefik          global              1/1                 traefik:1.3.5-alpine              *:80- 80/tcp,*:443- 443/tcp,*:4200- 4200/tcp,*:4300- 4300/tcp,*:8080- 8080/tcp\nbvs32e81jcns        ql_viz              replicated          1/1                 dockersamples/visualizer:latest   *:8282- 8080/tcp\ne8kyp4vylvev        ql_quantumleap      replicated          1/1                 smartsdk/quantumleap:latest       *:8668- 8668/tcp\nignls7l57hzn        ql_crate            global              3/3                 crate:1.0.5                       \ntfszxc2fcmxx        ql_grafana          replicated          1/1                 grafana/grafana:latest            *:3000- 3000/tcp  Now you are ready to scale services according to your needs using simple docker service scale command as explained in  the official docs .", 
            "title": "Deploy"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#explore", 
            "text": "Now, if you open your explorer to http://crate.mydomain.com you should see the CRATE.IO dashboard. In the \"cluster\" tab you should see the same number of nodes you have in the swarm cluster.  For a quick test, you can use the  insert.sh  script in this folder.  $ sh insert.sh IP_OF_ANY_SWARM_NODE 8668  Otherwise, open your favourite API tester and send the fake notification shown below to QuantumLeap to later see it persisted in the database through the Crate Dashboard.  # Simple fake payload to send to IP_OF_ANY_SWARM_NODE:8668.\n{\n    \"subscriptionId\": \"5947d174793fe6f7eb5e3961\",\n    \"data\": [\n        {\n            \"id\": \"Room1\",\n            \"type\": \"Room\",\n            \"temperature\": {\n                \"type\": \"Number\",\n                \"value\": 27.6,\n                \"metadata\": {\n                    \"dateModified\": {\n                        \"type\": \"DateTime\",\n                        \"value\": \"2017-06-19T11:46:45.00Z\"\n                    }\n                }\n            }\n        }\n    ]\n}  For further information, please refer to the  QuantumLeap's User Manual .", 
            "title": "Explore"
        }, 
        {
            "location": "/iot-services/readme/", 
            "text": "Comming soon...", 
            "title": "IoT Broker"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/", 
            "text": "MongoDB Replica Set\n\n\nThis recipe aims to deploy and control a \nreplica set\n of MongoDB instances in a Docker Swarm.\n\n\n\n\nIMPORTANT:\n This recipe is not yet ready for production environments. See \nfurther improvements\n section for more details.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you don't have one, checkout the \ntools\n section for a quick way to setup a local swarm.\n\n\nminiswarm start 3\neval $(docker-machine env ms-manager0)\n\n\n\nThen, simply run...\n\n\nsh deploy.sh\n\n\n\nAllow some time while images are pulled in the nodes and services are deployed. After a couple of minutes, you can check if all services are up, as usual, running...\n\n\n$ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      3/3       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n\n\nA Walkthrough\n\n\nAs shown before, the recipe consists of basically two services, namely, one for mongo instances and one for controlling the replica-set.\n\n\nThe mongo service is deployed in \"global\" mode, meaning that docker will run one instance of mongod per swarm node in the cluster.\n\n\nAt the swarm's master node, a python-based controller script will be deployed to configure and maintain the mongodb replica-set.\n\n\nLet's now check that the controller worked fine inspecting the logs of the mongo-replica_mongo-controller service. This can be done with either...\n\n\n$ docker service logs mongo-replica_mongo-controller\n\n\n\nor running the following...\n\n\n$  docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\nINFO:__main__:Waiting some time before starting\nINFO:__main__:Initial config: {'version': 1, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}]}\nINFO:__main__:replSetInitiate: {'ok': 1.0}\n\n\n\nAs you can see, the replica-set was configured with 3 replicas represented by containers running in the same overlay network. You can also run a mongo command in any of the mongo containers and execute \nrs.status()\n to see the same results.\n\n\n$ docker exec -ti d56d17c40f8f mongo\nrs:SECONDARY\n rs.status()\n\n\n\nRescaling the replica-set\n\n\nLet's add a new node to the swarm to see how docker deploys a new task of the mongo service and the controller automatically adds it to the replica-set.\n\n\n# First get the token to join the swarm\n$ docker swarm join-token worker\n\n# Create the new node\n$ docker-machine create -d virtualbox ms-worker2\n$ docker-machine ssh ms-worker2\n\ndocker@ms-worker2:~$ docker swarm join \\\n--token INSERT_TOKEN_HERE \\\n192.168.99.100:2377\n\ndocker@ms-worker2:~$ exit\n\n\n\nBack to the host, some minutes later...\n\n\n$ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      4/4       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n$ docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\n...\nINFO:__main__:To add: {'10.0.0.8'}\nINFO:__main__:New config: {'version': 2, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}, {'_id': 3, 'host': '10.0.0.8:27017'}]}\nINFO:__main__:replSetReconfig: {'ok': 1.0}\n\n\n\nIf a node goes down, the replica-set will be automatically reconfigured at the application level by mongo. Docker, on the other hand, will not reschedule the replica because it's expected to run one only one per node.\n\n\nNOTE\n: If you don't want to have a replica in every node of the swarm, the solution for now is using a combination of constraints and node tags. You can read more about this in \nthis Github issue\n.\n\n\nFor further details, refer to the \nmongo-rs-controller-swarm\n repository, in particular the \ndocker-compose.yml\n file or the \nreplica_ctrl.py\n controller script.", 
            "title": "MongoDB Replica Set"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#mongodb-replica-set", 
            "text": "This recipe aims to deploy and control a  replica set  of MongoDB instances in a Docker Swarm.   IMPORTANT:  This recipe is not yet ready for production environments. See  further improvements  section for more details.", 
            "title": "MongoDB Replica Set"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you don't have one, checkout the  tools  section for a quick way to setup a local swarm.  miniswarm start 3\neval $(docker-machine env ms-manager0)  Then, simply run...  sh deploy.sh  Allow some time while images are pulled in the nodes and services are deployed. After a couple of minutes, you can check if all services are up, as usual, running...  $ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      3/3       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest", 
            "title": "How to use"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#a-walkthrough", 
            "text": "As shown before, the recipe consists of basically two services, namely, one for mongo instances and one for controlling the replica-set.  The mongo service is deployed in \"global\" mode, meaning that docker will run one instance of mongod per swarm node in the cluster.  At the swarm's master node, a python-based controller script will be deployed to configure and maintain the mongodb replica-set.  Let's now check that the controller worked fine inspecting the logs of the mongo-replica_mongo-controller service. This can be done with either...  $ docker service logs mongo-replica_mongo-controller  or running the following...  $  docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\nINFO:__main__:Waiting some time before starting\nINFO:__main__:Initial config: {'version': 1, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}]}\nINFO:__main__:replSetInitiate: {'ok': 1.0}  As you can see, the replica-set was configured with 3 replicas represented by containers running in the same overlay network. You can also run a mongo command in any of the mongo containers and execute  rs.status()  to see the same results.  $ docker exec -ti d56d17c40f8f mongo\nrs:SECONDARY  rs.status()", 
            "title": "A Walkthrough"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#rescaling-the-replica-set", 
            "text": "Let's add a new node to the swarm to see how docker deploys a new task of the mongo service and the controller automatically adds it to the replica-set.  # First get the token to join the swarm\n$ docker swarm join-token worker\n\n# Create the new node\n$ docker-machine create -d virtualbox ms-worker2\n$ docker-machine ssh ms-worker2\n\ndocker@ms-worker2:~$ docker swarm join \\\n--token INSERT_TOKEN_HERE \\\n192.168.99.100:2377\n\ndocker@ms-worker2:~$ exit  Back to the host, some minutes later...  $ docker service ls\nID            NAME                            MODE        REPLICAS  IMAGE\nfjxof1n5ce58  mongo-replica_mongo             global      4/4       mongo:latest\nyzsur7rb4mg1  mongo-replica_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n$ docker logs $(docker ps -f \"name=mongo-replica_mongo-controller\" -q)\n...\nINFO:__main__:To add: {'10.0.0.8'}\nINFO:__main__:New config: {'version': 2, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}, {'_id': 3, 'host': '10.0.0.8:27017'}]}\nINFO:__main__:replSetReconfig: {'ok': 1.0}  If a node goes down, the replica-set will be automatically reconfigured at the application level by mongo. Docker, on the other hand, will not reschedule the replica because it's expected to run one only one per node.  NOTE : If you don't want to have a replica in every node of the swarm, the solution for now is using a combination of constraints and node tags. You can read more about this in  this Github issue .  For further details, refer to the  mongo-rs-controller-swarm  repository, in particular the  docker-compose.yml  file or the  replica_ctrl.py  controller script.", 
            "title": "Rescaling the replica-set"
        }, 
        {
            "location": "/tools/readme/", 
            "text": "Tools\n\n\nThis section contains useful (and sometimes temporary) scripts as well as references to tools, projects and pieces of documentation used for the development of the recipes.\n\n\nThe basic environment setup is explained in the \nInstallation\n part of the docs.\n\n\nEnv-related\n\n\n\n\n\n\nminiswarm\n\n\nHelpful tool to help you quickly setup a local virtualbox-based swarm cluster for testing purposes.\n\n\n\n\n\n\nwait-for-it\n\n\nUseful shell script used when you need to wait for a service to be started.\n\n\nNote\n: This might no longer be needed since docker introduced the \nhealthchecks\n feature.\n\n\n\n\n\n\nportainer\n\n\nIf you'd like an UI with info about your swarm:\n\n\ndocker service create \\\n--name portainer \\\n--publish 9000:9000 \\\n--constraint 'node.role == manager' \\\n--mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\nportainer/portainer \\\n-H unix:///var/run/docker.sock\n\n\n\n\n\n\n\ndocker-swarm-visualizer\n\n\nIf you'd like to have a basic view of the distribution of containers in your swarm cluster, add the following service to your docker-compose.yml file.\n\n\nviz:\n  image: dockersamples/visualizer\n  volumes:\n    - /var/run/docker.sock:/var/run/docker.sock\n  deploy:\n    placement:\n      constraints: [node.role == manager]\n  ports:\n    - \"8282:8080\"\n  networks:\n    - backend\n    - frontend\n\n\n\n\n\n\n\nDocs-related\n\n\n\n\n\n\ngravizo\n\n\nWe use this tool in the docs for \nsimple diagrams\n. Problems with formatting? try the \nconverter\n.\n\n\n\n\n\n\ndraw.io\n\n\nUse this tool when the diagrams start getting too complex of when you foresee the diagram will be complex from the scratch.\n\n\nComplex in the sense that making a simple change takes more time understanding the \n.dot\n than making a manual gui-based change.\n\n\nWhen using draw.io, keep the source file in the repository under a /doc subfolder of the corresponding recipe.\n\n\n\n\n\n\ncolor names\n\n\nThe reference for color names used in \n.dot\n files.\n\n\n\n\n\n\nExperimental\n\n\n\n\n\n\ndiagramr\n\nTo give more docker-related details we could use this tool to create diagrams from docker-compose files. The tools gives also the .dot file, which would be eventually customized and then turned into a png file using \ngraphviz\n.\n$ dot compose.dot -Tpng -o compose.png", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/readme/#tools", 
            "text": "This section contains useful (and sometimes temporary) scripts as well as references to tools, projects and pieces of documentation used for the development of the recipes.  The basic environment setup is explained in the  Installation  part of the docs.", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/readme/#env-related", 
            "text": "", 
            "title": "Env-related"
        }, 
        {
            "location": "/tools/readme/#miniswarm", 
            "text": "Helpful tool to help you quickly setup a local virtualbox-based swarm cluster for testing purposes.", 
            "title": "miniswarm"
        }, 
        {
            "location": "/tools/readme/#wait-for-it", 
            "text": "Useful shell script used when you need to wait for a service to be started.  Note : This might no longer be needed since docker introduced the  healthchecks  feature.", 
            "title": "wait-for-it"
        }, 
        {
            "location": "/tools/readme/#portainer", 
            "text": "If you'd like an UI with info about your swarm:  docker service create \\\n--name portainer \\\n--publish 9000:9000 \\\n--constraint 'node.role == manager' \\\n--mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\nportainer/portainer \\\n-H unix:///var/run/docker.sock", 
            "title": "portainer"
        }, 
        {
            "location": "/tools/readme/#docker-swarm-visualizer", 
            "text": "If you'd like to have a basic view of the distribution of containers in your swarm cluster, add the following service to your docker-compose.yml file.  viz:\n  image: dockersamples/visualizer\n  volumes:\n    - /var/run/docker.sock:/var/run/docker.sock\n  deploy:\n    placement:\n      constraints: [node.role == manager]\n  ports:\n    - \"8282:8080\"\n  networks:\n    - backend\n    - frontend", 
            "title": "docker-swarm-visualizer"
        }, 
        {
            "location": "/tools/readme/#docs-related", 
            "text": "", 
            "title": "Docs-related"
        }, 
        {
            "location": "/tools/readme/#gravizo", 
            "text": "We use this tool in the docs for  simple diagrams . Problems with formatting? try the  converter .", 
            "title": "gravizo"
        }, 
        {
            "location": "/tools/readme/#drawio", 
            "text": "Use this tool when the diagrams start getting too complex of when you foresee the diagram will be complex from the scratch.  Complex in the sense that making a simple change takes more time understanding the  .dot  than making a manual gui-based change.  When using draw.io, keep the source file in the repository under a /doc subfolder of the corresponding recipe.", 
            "title": "draw.io"
        }, 
        {
            "location": "/tools/readme/#color-names", 
            "text": "The reference for color names used in  .dot  files.", 
            "title": "color names"
        }, 
        {
            "location": "/tools/readme/#experimental", 
            "text": "", 
            "title": "Experimental"
        }, 
        {
            "location": "/tools/readme/#diagramr", 
            "text": "To give more docker-related details we could use this tool to create diagrams from docker-compose files. The tools gives also the .dot file, which would be eventually customized and then turned into a png file using  graphviz . $ dot compose.dot -Tpng -o compose.png", 
            "title": "diagramr"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributions\n\n\nContributions are more than welcome in the form of \nPull Requests\n.\n\n\nFeel free to \nopen issues\n if something looks wrong.\n\n\nDocumentation\n\n\nFor now we are using \nMkdocs\n deploying on \nGithub Pages\n.\n\n\nYou will also notice that instead of having a separate \ndocs\n folder, the documentation is composed of the README's content of all subfolders so as to keep docs as close to the respective recipes as possible.\n\n\nIf you change the structure of the index or add new pages, remember to update \nmkdocs.yml\n accordingly.\n\n\nNote you can preview your changes locally by running\n\n\n# from the location of mkdocs.yml\n$ mkdocs serve\n\n\n\nAfter all your changes, remember to run\n\n\n# from the location of mkdocs.yml\n$ mkdocs gh-deploy", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributions", 
            "text": "Contributions are more than welcome in the form of  Pull Requests .  Feel free to  open issues  if something looks wrong.", 
            "title": "Contributions"
        }, 
        {
            "location": "/contributing/#documentation", 
            "text": "For now we are using  Mkdocs  deploying on  Github Pages .  You will also notice that instead of having a separate  docs  folder, the documentation is composed of the README's content of all subfolders so as to keep docs as close to the respective recipes as possible.  If you change the structure of the index or add new pages, remember to update  mkdocs.yml  accordingly.  Note you can preview your changes locally by running  # from the location of mkdocs.yml\n$ mkdocs serve  After all your changes, remember to run  # from the location of mkdocs.yml\n$ mkdocs gh-deploy", 
            "title": "Documentation"
        }
    ]
}