{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome\n\n\nThis site is dedicated to the documentation of the recipes brought by\n\nSmartSDK\n to launch, test and experiment different\n\nFIWARE Generic Enablers\n and\ncombinations of them known as common use cases.\n\n\nRecipes will be based on \nDocker\n and maybe some\ncomplementary scripts. For simple cases, you may be able to run them on a single\nDocker host using \ndocker-compose\n.\nHowever, the more interesting scenarios are aimed to be run in a\n\nDocker Swarm\n cluster.\n\n\nRecipes are organized in folders respecting the\n\nFIWARE Chapters\n and\nthe \nGeneric Enablers\n they are aimed\nto work with. Shared functionality can be found in the \nutils\n folder.\n\n\nAlso, you may want to have a look at the \nTools\n section,\nspecially if you want to test the recipes in a local environment.\n\n\nGetting started\n\n\nHead first to the \nInstallation\n section and follow\nthe instructions to get the code and set-up your environment.\n\n\nThen, feel free to explore the different recipes and their referenced parts\nof the corresponding FIWARE documentation. Note that the structure of this\ndocumentation is mapped to the structure of the repository recipes folders to\nmake your navigation easier.\n\n\nDon't miss the \nContributing\n section in case you'd like\nto improve this repo.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "This site is dedicated to the documentation of the recipes brought by SmartSDK  to launch, test and experiment different FIWARE Generic Enablers  and\ncombinations of them known as common use cases.  Recipes will be based on  Docker  and maybe some\ncomplementary scripts. For simple cases, you may be able to run them on a single\nDocker host using  docker-compose .\nHowever, the more interesting scenarios are aimed to be run in a Docker Swarm  cluster.  Recipes are organized in folders respecting the FIWARE Chapters  and\nthe  Generic Enablers  they are aimed\nto work with. Shared functionality can be found in the  utils  folder.  Also, you may want to have a look at the  Tools  section,\nspecially if you want to test the recipes in a local environment.", 
            "title": "Welcome"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Head first to the  Installation  section and follow\nthe instructions to get the code and set-up your environment.  Then, feel free to explore the different recipes and their referenced parts\nof the corresponding FIWARE documentation. Note that the structure of this\ndocumentation is mapped to the structure of the repository recipes folders to\nmake your navigation easier.  Don't miss the  Contributing  section in case you'd like\nto improve this repo.", 
            "title": "Getting started"
        }, 
        {
            "location": "/installation/", 
            "text": "Getting the Recipes\n\n\nGet the latest version from the git repository.\n\n\n    $ git clone https://github.com/smartsdk/smartsdk-recipes\n\n\n\n\nRequirements\n\n\nThe recipes are prepared to run using the latest\n\nDocker\n version (minimum 1.13+, ideally \n= 17.06.0+).\nTo install Docker refer to the\n\nInstallation instructions\n.\n\n\nFor some testing and walkthroughs, you may also need to install\n\ncurl\n if it's not already available in your system.\n\n\nFinally, you should install\n\nVirtualBox\n if you want to create\nclusters in your local environment to test the recipes (see next section).\n\n\nNote For Windows Users: Many of the walkthroughs and verification steps are\ndesigned to run tools typically found in a Linux/macOS environment.\nTherefore, you will need to consider compatible workarounds from time to time.\n\n\nPreparing a Local Swarm Cluster\n\n\nCreating the Cluster\n\n\nAlthough you can run most (if not all) of the recipes using\n\ndocker-compose\n, the recipes are\ntailored to be deployed as services on Docker Swarm Clusters.\n\n\nYou can turn your local Docker client into a single-node Swarm cluster by simply\nrunning\n\n\n    $ docker swarm init\n\n\n\n\nHowever, things get more interesting when you're actually working on\na multi-node cluster.\n\n\nThe fastest way to create one is using\n\nminiswarm\n.\nGetting started is as simple as:\n\n\n    # First-time only to install miniswarm\n    $ curl -sSL https://raw.githubusercontent.com/aelsabbahy/miniswarm/master/miniswarm -o /usr/local/bin/miniswarm\n    $ chmod +rx /usr/local/bin/miniswarm\n\n    # Every time you create/destroy a swarm\n    $ miniswarm start 3\n    $ miniswarm delete\n\n\n\n\nOtherwise, you can create your own using\n\ndocker-machine\n.\n\n\nCreating the networks\n\n\nFor convenience reasons, most if not all of the recipes will be using overlay\nnetworks to connect the services. We have agreed-upon the convention of having\nat least two overlay networks available: \"backend\" and \"frontend\". The latest\ntypically connects services that need some exposure to the outside world.\n\n\nIf you want to buy you some time, you can now create the two networks before\nstarting the trials with the recipes. This can be done by running the following\ncommands:\n\n\n    $ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} backend\n    $ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} frontend\n\n\n\n\nOr, if you are lazy, there is a script in the \ntools\n folder.\n\n\n    $ sh tools/create_networks.sh\n\n\n\n\nAgain, this is a convention to simplify the experimentation with the recipes.\nIn the end, you may want to edit the recipes to adapt to your specific\nnetworking needs.\n\n\nOn virtualised environments\n\n\nIf you are running the recipes in a virtualised environment such as your FIWARE\nLab, if at some point you experience problems with the connectivity of\nthe containers to the outside world, chances are that the cause of the package\ndropping is due to a mismatch of the\n\nMTU\n settings.\n\n\nIn FIWARE Lab, the default MTU for the vm's bridge is set to \n1400\n,\nhence you will notice that this is the default MTU for the networks used\nin the recipes.\nIf you need to change that value, feel free to set a \nDOCKER_MTU\n env variable\nwith the value you want before you create the networks.", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#getting-the-recipes", 
            "text": "Get the latest version from the git repository.      $ git clone https://github.com/smartsdk/smartsdk-recipes", 
            "title": "Getting the Recipes"
        }, 
        {
            "location": "/installation/#requirements", 
            "text": "The recipes are prepared to run using the latest Docker  version (minimum 1.13+, ideally  = 17.06.0+).\nTo install Docker refer to the Installation instructions .  For some testing and walkthroughs, you may also need to install curl  if it's not already available in your system.  Finally, you should install VirtualBox  if you want to create\nclusters in your local environment to test the recipes (see next section).  Note For Windows Users: Many of the walkthroughs and verification steps are\ndesigned to run tools typically found in a Linux/macOS environment.\nTherefore, you will need to consider compatible workarounds from time to time.", 
            "title": "Requirements"
        }, 
        {
            "location": "/installation/#preparing-a-local-swarm-cluster", 
            "text": "", 
            "title": "Preparing a Local Swarm Cluster"
        }, 
        {
            "location": "/installation/#creating-the-cluster", 
            "text": "Although you can run most (if not all) of the recipes using docker-compose , the recipes are\ntailored to be deployed as services on Docker Swarm Clusters.  You can turn your local Docker client into a single-node Swarm cluster by simply\nrunning      $ docker swarm init  However, things get more interesting when you're actually working on\na multi-node cluster.  The fastest way to create one is using miniswarm .\nGetting started is as simple as:      # First-time only to install miniswarm\n    $ curl -sSL https://raw.githubusercontent.com/aelsabbahy/miniswarm/master/miniswarm -o /usr/local/bin/miniswarm\n    $ chmod +rx /usr/local/bin/miniswarm\n\n    # Every time you create/destroy a swarm\n    $ miniswarm start 3\n    $ miniswarm delete  Otherwise, you can create your own using docker-machine .", 
            "title": "Creating the Cluster"
        }, 
        {
            "location": "/installation/#creating-the-networks", 
            "text": "For convenience reasons, most if not all of the recipes will be using overlay\nnetworks to connect the services. We have agreed-upon the convention of having\nat least two overlay networks available: \"backend\" and \"frontend\". The latest\ntypically connects services that need some exposure to the outside world.  If you want to buy you some time, you can now create the two networks before\nstarting the trials with the recipes. This can be done by running the following\ncommands:      $ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} backend\n    $ docker network create -d overlay --opt com.docker.network.driver.mtu=${DOCKER_MTU:-1400} frontend  Or, if you are lazy, there is a script in the  tools  folder.      $ sh tools/create_networks.sh  Again, this is a convention to simplify the experimentation with the recipes.\nIn the end, you may want to edit the recipes to adapt to your specific\nnetworking needs.", 
            "title": "Creating the networks"
        }, 
        {
            "location": "/installation/#on-virtualised-environments", 
            "text": "If you are running the recipes in a virtualised environment such as your FIWARE\nLab, if at some point you experience problems with the connectivity of\nthe containers to the outside world, chances are that the cause of the package\ndropping is due to a mismatch of the MTU  settings.  In FIWARE Lab, the default MTU for the vm's bridge is set to  1400 ,\nhence you will notice that this is the default MTU for the networks used\nin the recipes.\nIf you need to change that value, feel free to set a  DOCKER_MTU  env variable\nwith the value you want before you create the networks.", 
            "title": "On virtualised environments"
        }, 
        {
            "location": "/data-management/context-broker/readme/", 
            "text": "Orion\n\n\nHere you can find recipes aimed at different usages of the Orion Context Broker.\nWe assume you are already familiar with Orion. If not, refer to the\n\nofficial documentation\n.\n\n\nThe easiest and simplest way to try Orion is as explained in\n\nOrion's official docker image docs\n\nusing  \nthis docker-compose\n\nfile. But here, we will explore a distributed configuration for this Generic\nEnabler.\n\n\nIf you are new to docker as well you probably want to start looking at the\n\nsimple recipe\n to have a first view of how a single-host\nsimple scenario looks like.\n\n\nInstructions on how to prepare your environment to test these recipes are given\nin \nhttps://github.com/smartsdk/smartsdk-recipes\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/context-broker/readme/#orion", 
            "text": "Here you can find recipes aimed at different usages of the Orion Context Broker.\nWe assume you are already familiar with Orion. If not, refer to the official documentation .  The easiest and simplest way to try Orion is as explained in Orion's official docker image docs \nusing   this docker-compose \nfile. But here, we will explore a distributed configuration for this Generic\nEnabler.  If you are new to docker as well you probably want to start looking at the simple recipe  to have a first view of how a single-host\nsimple scenario looks like.  Instructions on how to prepare your environment to test these recipes are given\nin  https://github.com/smartsdk/smartsdk-recipes .", 
            "title": "Orion"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/", 
            "text": "Single Host Scenario\n\n\nIntroduction\n\n\nThis simple recipe triggers an\n\nOrion Context Broker\n\ninstance backed with a \nMongoDB\n instance everything\nrunning \non an single host\n.\n\n\n\n\nBoth services will be running in docker containers, defined in the\n\n[./docker-compose.yml](https://github.com/smartsdk/smartsdk-recipes/blob/master/recipes/data-management/context-broker/simple/docker-compose.yml)\n\nfile.\n\n\nData will be persisted in a local folder defined by the value of \nDATA_PATH\n\nvariable in the\n\n[.env](https://github.com/smartsdk/smartsdk-recipes/blob/master/recipes/data-management/context-broker/simple/.env)\n\nfile.\n\n\nHow to use\n\n\nThis recipes has some default values, but optionally you can explore different\nconfigurations by modifying the \n.env\n file, the \ndocker-compose.yml\n or even\nthe \nscripts/setup.sh\n.\n\n\nThen, from this folder simply run:\n\n\n    $ docker-compose up -d\n\n\n\n\nHow to validate\n\n\nBefore testing make sure docker finished downloading the images and spinning-off\nthe containers. You can check that by running:\n\n\n    $ docker ps\n\n\n\n\nYou should see the two containers listed and with status \"up\".\n\n\nThen, to test if orion is truly up and running run:\n\n\n    $ sh ../query.sh\n\n\n\n\nIt should return something like:\n\n\n    {\n    \norion\n : {\n      \nversion\n : \n1.6.0-next\n,\n      \nuptime\n : \n0 d, 0 h, 5 m, 24 s\n,\n      \ngit_hash\n : \n61be6c26c59469621a664d7aeb1490d6363cad38\n,\n      \ncompile_time\n : \nTue Jan 24 10:52:30 UTC 2017\n,\n      \ncompiled_by\n : \nroot\n,\n      \ncompiled_in\n : \nb99744612d0b\n\n    }\n    }\n    []", 
            "title": "Simple"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#single-host-scenario", 
            "text": "", 
            "title": "Single Host Scenario"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#introduction", 
            "text": "This simple recipe triggers an Orion Context Broker \ninstance backed with a  MongoDB  instance everything\nrunning  on an single host .   Both services will be running in docker containers, defined in the [./docker-compose.yml](https://github.com/smartsdk/smartsdk-recipes/blob/master/recipes/data-management/context-broker/simple/docker-compose.yml) \nfile.  Data will be persisted in a local folder defined by the value of  DATA_PATH \nvariable in the [.env](https://github.com/smartsdk/smartsdk-recipes/blob/master/recipes/data-management/context-broker/simple/.env) \nfile.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#how-to-use", 
            "text": "This recipes has some default values, but optionally you can explore different\nconfigurations by modifying the  .env  file, the  docker-compose.yml  or even\nthe  scripts/setup.sh .  Then, from this folder simply run:      $ docker-compose up -d", 
            "title": "How to use"
        }, 
        {
            "location": "/data-management/context-broker/simple/readme/#how-to-validate", 
            "text": "Before testing make sure docker finished downloading the images and spinning-off\nthe containers. You can check that by running:      $ docker ps  You should see the two containers listed and with status \"up\".  Then, to test if orion is truly up and running run:      $ sh ../query.sh  It should return something like:      {\n     orion  : {\n       version  :  1.6.0-next ,\n       uptime  :  0 d, 0 h, 5 m, 24 s ,\n       git_hash  :  61be6c26c59469621a664d7aeb1490d6363cad38 ,\n       compile_time  :  Tue Jan 24 10:52:30 UTC 2017 ,\n       compiled_by  :  root ,\n       compiled_in  :  b99744612d0b \n    }\n    }\n    []", 
            "title": "How to validate"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/", 
            "text": "Orion in HA\n\n\nThis recipe shows how to deploy an scalable\n\nOrion Context Broker\n\nservice backed with an scalable\n\nreplica set\n of MongoDB instances.\n\n\nAll elements will be running in docker containers, defined in docker-compose\nfiles. Actually, this recipe focuses on the deployment of the Orion frontend,\nreusing the \nmongodb replica recipe\n\nfor its backend.\n\n\nThe final deployment is represented by the following picture:\n\n\n\n\nPrerequisites\n\n\nPlease make sure you read the \nwelcome page\n and followed\nthe steps explained in the \ninstallation guide\n.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup.\nIf you don't have one, checkout the \ntools\n section\nfor a quick way to setup a local swarm.\n\n\n    $ miniswarm start 3\n    $ eval $(docker-machine env ms-manager0)\n\n\n\n\nOrion needs a mongo database for its backend. If you have already deployed Mongo\nwithin your cluster and would like to reuse that database, you can skip the next\nstep (deploying backend). You will just need to pay attention to the variables\nyou define for Orion to link to Mongo, namely, \nMONGO_SERVICE_URI\n\nand \nREPLICASET_NAME\n. Make sure you have the correct values in \nfrontend.env\n.\nThe value of \nMONGO_SERVICE_URI\n should be a routable address for mongo.\nIf deployed within the swarm, the service name (with stack prefix)\nwould suffice. You can read more in the\n\nofficial docker docs\n.\nThe default values should be fine for you if you used the\n\nMongo ReplicaSet Recipe\n.\n\n\nOtherwise, if you prefer to make a new deployment of Mongo just for Orion,\nyou can take a shortcut and run...\n\n\n    $ sh deploy_back.sh\n\n\n\n\nWait some time until the backend is ready and then...\n\n\n    $ sh deploy_front.sh\n\n\n\n\nAt some point, your deployment should look like this...\n\n\n    $ docker service ls\n    ID            NAME                            MODE        REPLICAS  IMAGE\n    nrxbm6k0a2yn  mongo-rs_mongo             global      3/3       mongo:3.2\n    rgws8vumqye2  mongo-rs_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n    zk7nu592vsde  orion_orion                     replicated  3/3       fiware/orion:1.3.0\n\n\n\n\nAs shown above, if you see \n3/3\nin the replicas column it means the 3 replicas\nare up and running.\n\n\nA walkthrough\n\n\nYou can check the distribution of the containers of a service (a.k.a tasks)\nthrough the swarm running the following...\n\n\n    $ docker service ps orion_orion\n    ID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE               ERROR  PORTS\n    wwgt3q6nqqg3  orion_orion.1  fiware/orion:1.3.0  ms-worker0   Running        Running 9 minutes ago          \n    l1wavgqra8ry  orion_orion.2  fiware/orion:1.3.0  ms-worker1   Running        Running 9 minutes ago          \n    z20v0pnym8ky  orion_orion.3  fiware/orion:1.3.0  ms-manager0  Running        Running 25 minutes ago    \n\n\n\n\nThe good news is that, as you can see from the above output, by default docker\nalready took care of deploying all the replicas of the service\n\ncontext-broker_orion\n to different hosts.\n\n\nOf course, with the use of labels, constraints or deploying mode you have\nthe power to customize the distribution of tasks among swarm nodes. You can see\nthe \nmongo replica recipe\n to understand\nthe deployment of the \nmongo-replica_mongo\n service.\n\n\nNow, let's query Orion to check it's truly up and running.\nThe question now is... where is Orion actually running? We'll cover the network\ninternals later, but for now let's query the manager node...\n\n\n    $ sh ../query.sh $(docker-machine ip ms-manager0)\n\n\n\n\nYou will get something like...\n\n\n    {\n      \norion\n : {\n      \nversion\n : \n1.3.0\n,\n      \nuptime\n : \n0 d, 0 h, 18 m, 13 s\n,\n      \ngit_hash\n : \ncb6813f044607bc01895296223a27e4466ab0913\n,\n      \ncompile_time\n : \nFri Sep 2 08:19:12 UTC 2016\n,\n      \ncompiled_by\n : \nroot\n,\n      \ncompiled_in\n : \nba19f7d3be65\n\n    }\n    }\n    []\n\n\n\n\nThanks to the docker swarm internal routing mesh, you can actually perform\nthe previous query to any node of the swarm, it will be redirected to a node\nwhere the request on port \n1026\n can be attended (i.e, any node running Orion).\n\n\nLet's insert some data...\n\n\n    $ sh ../insert.sh $(docker-machine ip ms-worker1)\n\n\n\n\nAnd check it's there...\n\n\n    $ sh ../query.sh $(docker-machine ip ms-worker0)\n    ...\n    [\n        {\n            \nid\n: \nRoom1\n,\n            \npressure\n: {\n                \nmetadata\n: {},\n                \ntype\n: \nInteger\n,\n                \nvalue\n: 720\n            },\n            \ntemperature\n: {\n                \nmetadata\n: {},\n                \ntype\n: \nFloat\n,\n                \nvalue\n: 23\n            },\n            \ntype\n: \nRoom\n\n        }\n    ]\n\n\n\n\nYes, you can query any of the three nodes.\n\n\nSwarm's internal load balancer will be load-balancing in a round-robin approach\nall the requests for an orion service among the orion tasks running\nin the swarm.\n\n\nRescaling Orion\n\n\nScaling up and down orion is a simple as runnnig something like...\n\n\n    $ docker service scale orion_orion=2\n\n\n\n\n(this maps to the \nreplicas\n argument in the docker-compose)\n\n\nConsequently, one of the nodes (ms-worker1 in my case) is no longer running\nOrion...\n\n\n    $ docker service ps orion_orion\n    ID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE           ERROR  PORTS\n    2tibpye24o5q  orion_orion.2  fiware/orion:1.3.0  ms-manager0  Running        Running 11 minutes ago         \n    w9zmn8pp61ql  orion_orion.3  fiware/orion:1.3.0  ms-worker0   Running        Running 11 minutes ago\n\n\n\n\nBut still responds to the querying as mentioned above...\n\n\n    $ sh ../query.sh $(docker-machine ip ms-worker1)\n    {\n      \norion\n : {\n      \nversion\n : \n1.3.0\n,\n      \nuptime\n : \n0 d, 0 h, 14 m, 30 s\n,\n      \ngit_hash\n : \ncb6813f044607bc01895296223a27e4466ab0913\n,\n      \ncompile_time\n : \nFri Sep 2 08:19:12 UTC 2016\n,\n      \ncompiled_by\n : \nroot\n,\n      \ncompiled_in\n : \nba19f7d3be65\n\n    }\n    }\n    []\n\n\n\n\nYou can see the \nmongo replica recipe\n to see\nhow to scale the mongodb backend. But basically, due to the fact that it's a\n\"global\" service, you can scale it down like shown before. However, scaling it\nup might require adding a new node to the swarm because there can be only one\ninstance per node.\n\n\nDealing with failures\n\n\nDocker is taking care of the reconciliation of the services in case a container\ngoes down. Let's show this by running the following (always on the manager\nnode):\n\n\n    $ docker ps\n    CONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n    abc5e37037f0        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \n/usr/bin/contextB...\n   2 minutes ago       Up 2 minutes        1026/tcp            orion_orion.1.o9ebbardwvzn1gr11pmf61er8\n    1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \npython /src/repli...\n   About an hour ago   Up About an hour                        mongo-rs_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n    8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \n/usr/bin/mongod -...\n   About an hour ago   Up About an hour    27017/tcp           mongo-rs_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n\n\n\nSuppose orion container goes down...\n\n\n    $ docker rm -f abc5e37037f0\n\n\n\n\nYou will see it gone, but after a while it will automatically come back.\n\n\n    $ docker ps\n    CONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n    1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \npython /src/repli...\n   About an hour ago   Up About an hour                        mongo-rs_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n    8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \n/usr/bin/mongod -...\n   About an hour ago   Up About an hour    27017/tcp           mongo-rs_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n    $ docker ps\n    CONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS                  PORTS               NAMES\n    60ba3f431d9d        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                 \n/usr/bin/contextB...\n   6 seconds ago       Up Less than a second   1026/tcp            orion_orion.1.uj1gghehb2s1gnoestup2ugs5\n    1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a   \npython /src/repli...\n   About an hour ago   Up About an hour                            mongo-rs_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n    8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                        \n/usr/bin/mongod -...\n   About an hour ago   Up About an hour        27017/tcp           mongo-rs_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n\n\n\nEven if a whole node goes down, the service will remain working because you had\nboth redundant orion instances and redundant db replicas.\n\n\n    $ docker-machine rm ms-worker0\n\n\n\n\nYou will still get replies to...\n\n\n    $ sh ../query.sh $(docker-machine ip ms-manager0)\n    $ sh ../query.sh $(docker-machine ip ms-worker1)\n\n\n\n\nNetworks considerations\n\n\nIn this case, all containers are attached to the same overlay network (backend)\nover which they communicate to each other. However, if you have a different\nconfiguration and are running any of the containers behind a firewall, remember\nto keep traffic open for TCP at ports 1026 (Orion's default) and 27017\n(Mongo's default).\n\n\nWhen containers (tasks) of a service are launched, they get assigned an IP\naddress in this overlay network. Other services of your application's\narchitecture should not be relying on these IPs because they may change\n(for example, due to a dynamic rescheduling). The good think is that docker\ncreates a virtual ip for the service as a whole, so all traffic to this address\nwill be load-balanced to the tasks addresses.\n\n\nThanks to swarms docker internal DNS you can also use the name of the service\nto connect to. If you look at the \ndocker-compose.yml\n file of this recipe,\norion is started with the name of the mongo service as \ndbhost\n param\n(regardless if it was a single mongo instance of a whole replica-set).\n\n\nHowever, to access the container from outside of the overlay network (for\nexample from the host) you would need to access the ip of the container's\ninterface to the \ndocker_gwbridge\n. It seem there's no easy way to get that\ninformation from the outside (see\n\nthis open issue\n.\nIn the walkthrough, we queried orion through one of the swarm nodes because we\nrely on docker ingress network routing the traffic all the way to one of the\ncontainerized orion services.\n\n\nOpen interesting issues\n\n\n\n\n\n\nhttps://github.com/docker/swarm/issues/1106\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/27082\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/29816\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/26696\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/23813\n\n\n\n\n\n\nMore info about docker network internals can be read at:\n\n\n\n\nDocker Reference Architecture", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#orion-in-ha", 
            "text": "This recipe shows how to deploy an scalable Orion Context Broker \nservice backed with an scalable replica set  of MongoDB instances.  All elements will be running in docker containers, defined in docker-compose\nfiles. Actually, this recipe focuses on the deployment of the Orion frontend,\nreusing the  mongodb replica recipe \nfor its backend.  The final deployment is represented by the following picture:", 
            "title": "Orion in HA"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#prerequisites", 
            "text": "Please make sure you read the  welcome page  and followed\nthe steps explained in the  installation guide .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup.\nIf you don't have one, checkout the  tools  section\nfor a quick way to setup a local swarm.      $ miniswarm start 3\n    $ eval $(docker-machine env ms-manager0)  Orion needs a mongo database for its backend. If you have already deployed Mongo\nwithin your cluster and would like to reuse that database, you can skip the next\nstep (deploying backend). You will just need to pay attention to the variables\nyou define for Orion to link to Mongo, namely,  MONGO_SERVICE_URI \nand  REPLICASET_NAME . Make sure you have the correct values in  frontend.env .\nThe value of  MONGO_SERVICE_URI  should be a routable address for mongo.\nIf deployed within the swarm, the service name (with stack prefix)\nwould suffice. You can read more in the official docker docs .\nThe default values should be fine for you if you used the Mongo ReplicaSet Recipe .  Otherwise, if you prefer to make a new deployment of Mongo just for Orion,\nyou can take a shortcut and run...      $ sh deploy_back.sh  Wait some time until the backend is ready and then...      $ sh deploy_front.sh  At some point, your deployment should look like this...      $ docker service ls\n    ID            NAME                            MODE        REPLICAS  IMAGE\n    nrxbm6k0a2yn  mongo-rs_mongo             global      3/3       mongo:3.2\n    rgws8vumqye2  mongo-rs_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n    zk7nu592vsde  orion_orion                     replicated  3/3       fiware/orion:1.3.0  As shown above, if you see  3/3 in the replicas column it means the 3 replicas\nare up and running.", 
            "title": "How to use"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#a-walkthrough", 
            "text": "You can check the distribution of the containers of a service (a.k.a tasks)\nthrough the swarm running the following...      $ docker service ps orion_orion\n    ID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE               ERROR  PORTS\n    wwgt3q6nqqg3  orion_orion.1  fiware/orion:1.3.0  ms-worker0   Running        Running 9 minutes ago          \n    l1wavgqra8ry  orion_orion.2  fiware/orion:1.3.0  ms-worker1   Running        Running 9 minutes ago          \n    z20v0pnym8ky  orion_orion.3  fiware/orion:1.3.0  ms-manager0  Running        Running 25 minutes ago      The good news is that, as you can see from the above output, by default docker\nalready took care of deploying all the replicas of the service context-broker_orion  to different hosts.  Of course, with the use of labels, constraints or deploying mode you have\nthe power to customize the distribution of tasks among swarm nodes. You can see\nthe  mongo replica recipe  to understand\nthe deployment of the  mongo-replica_mongo  service.  Now, let's query Orion to check it's truly up and running.\nThe question now is... where is Orion actually running? We'll cover the network\ninternals later, but for now let's query the manager node...      $ sh ../query.sh $(docker-machine ip ms-manager0)  You will get something like...      {\n       orion  : {\n       version  :  1.3.0 ,\n       uptime  :  0 d, 0 h, 18 m, 13 s ,\n       git_hash  :  cb6813f044607bc01895296223a27e4466ab0913 ,\n       compile_time  :  Fri Sep 2 08:19:12 UTC 2016 ,\n       compiled_by  :  root ,\n       compiled_in  :  ba19f7d3be65 \n    }\n    }\n    []  Thanks to the docker swarm internal routing mesh, you can actually perform\nthe previous query to any node of the swarm, it will be redirected to a node\nwhere the request on port  1026  can be attended (i.e, any node running Orion).  Let's insert some data...      $ sh ../insert.sh $(docker-machine ip ms-worker1)  And check it's there...      $ sh ../query.sh $(docker-machine ip ms-worker0)\n    ...\n    [\n        {\n             id :  Room1 ,\n             pressure : {\n                 metadata : {},\n                 type :  Integer ,\n                 value : 720\n            },\n             temperature : {\n                 metadata : {},\n                 type :  Float ,\n                 value : 23\n            },\n             type :  Room \n        }\n    ]  Yes, you can query any of the three nodes.  Swarm's internal load balancer will be load-balancing in a round-robin approach\nall the requests for an orion service among the orion tasks running\nin the swarm.", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#rescaling-orion", 
            "text": "Scaling up and down orion is a simple as runnnig something like...      $ docker service scale orion_orion=2  (this maps to the  replicas  argument in the docker-compose)  Consequently, one of the nodes (ms-worker1 in my case) is no longer running\nOrion...      $ docker service ps orion_orion\n    ID            NAME                    IMAGE               NODE         DESIRED STATE  CURRENT STATE           ERROR  PORTS\n    2tibpye24o5q  orion_orion.2  fiware/orion:1.3.0  ms-manager0  Running        Running 11 minutes ago         \n    w9zmn8pp61ql  orion_orion.3  fiware/orion:1.3.0  ms-worker0   Running        Running 11 minutes ago  But still responds to the querying as mentioned above...      $ sh ../query.sh $(docker-machine ip ms-worker1)\n    {\n       orion  : {\n       version  :  1.3.0 ,\n       uptime  :  0 d, 0 h, 14 m, 30 s ,\n       git_hash  :  cb6813f044607bc01895296223a27e4466ab0913 ,\n       compile_time  :  Fri Sep 2 08:19:12 UTC 2016 ,\n       compiled_by  :  root ,\n       compiled_in  :  ba19f7d3be65 \n    }\n    }\n    []  You can see the  mongo replica recipe  to see\nhow to scale the mongodb backend. But basically, due to the fact that it's a\n\"global\" service, you can scale it down like shown before. However, scaling it\nup might require adding a new node to the swarm because there can be only one\ninstance per node.", 
            "title": "Rescaling Orion"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#dealing-with-failures", 
            "text": "Docker is taking care of the reconciliation of the services in case a container\ngoes down. Let's show this by running the following (always on the manager\nnode):      $ docker ps\n    CONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n    abc5e37037f0        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                  /usr/bin/contextB...    2 minutes ago       Up 2 minutes        1026/tcp            orion_orion.1.o9ebbardwvzn1gr11pmf61er8\n    1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a    python /src/repli...    About an hour ago   Up About an hour                        mongo-rs_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n    8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                         /usr/bin/mongod -...    About an hour ago   Up About an hour    27017/tcp           mongo-rs_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz  Suppose orion container goes down...      $ docker rm -f abc5e37037f0  You will see it gone, but after a while it will automatically come back.      $ docker ps\n    CONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS              PORTS               NAMES\n    1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a    python /src/repli...    About an hour ago   Up About an hour                        mongo-rs_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n    8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                         /usr/bin/mongod -...    About an hour ago   Up About an hour    27017/tcp           mongo-rs_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz\n\n    $ docker ps\n    CONTAINER ID        IMAGE                                                                                                COMMAND                  CREATED             STATUS                  PORTS               NAMES\n    60ba3f431d9d        fiware/orion@sha256:734c034d078d22f4479e8d08f75b0486ad5a05bfb36b2a1f1ba90ecdba2040a9                  /usr/bin/contextB...    6 seconds ago       Up Less than a second   1026/tcp            orion_orion.1.uj1gghehb2s1gnoestup2ugs5\n    1d79dca4ff28        martel/mongo-replica-ctrl@sha256:f53d1ebe53624dcf7220fe02b3d764f1b0a34f75cb9fff309574a8be0625553a    python /src/repli...    About an hour ago   Up About an hour                            mongo-rs_mongo-controller.1.xomw6zf1o0wq0wbut9t5jx99j\n    8ea3b24bee1c        mongo@sha256:0d4453308cc7f0fff863df2ecb7aae226ee7fe0c5257f857fd892edf6d2d9057                         /usr/bin/mongod -...    About an hour ago   Up About an hour        27017/tcp           mongo-rs_mongo.ta8olaeg1u1wobs3a2fprwhm6.3akgzz28zp81beovcqx182nkz  Even if a whole node goes down, the service will remain working because you had\nboth redundant orion instances and redundant db replicas.      $ docker-machine rm ms-worker0  You will still get replies to...      $ sh ../query.sh $(docker-machine ip ms-manager0)\n    $ sh ../query.sh $(docker-machine ip ms-worker1)", 
            "title": "Dealing with failures"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#networks-considerations", 
            "text": "In this case, all containers are attached to the same overlay network (backend)\nover which they communicate to each other. However, if you have a different\nconfiguration and are running any of the containers behind a firewall, remember\nto keep traffic open for TCP at ports 1026 (Orion's default) and 27017\n(Mongo's default).  When containers (tasks) of a service are launched, they get assigned an IP\naddress in this overlay network. Other services of your application's\narchitecture should not be relying on these IPs because they may change\n(for example, due to a dynamic rescheduling). The good think is that docker\ncreates a virtual ip for the service as a whole, so all traffic to this address\nwill be load-balanced to the tasks addresses.  Thanks to swarms docker internal DNS you can also use the name of the service\nto connect to. If you look at the  docker-compose.yml  file of this recipe,\norion is started with the name of the mongo service as  dbhost  param\n(regardless if it was a single mongo instance of a whole replica-set).  However, to access the container from outside of the overlay network (for\nexample from the host) you would need to access the ip of the container's\ninterface to the  docker_gwbridge . It seem there's no easy way to get that\ninformation from the outside (see this open issue .\nIn the walkthrough, we queried orion through one of the swarm nodes because we\nrely on docker ingress network routing the traffic all the way to one of the\ncontainerized orion services.", 
            "title": "Networks considerations"
        }, 
        {
            "location": "/data-management/context-broker/ha/readme/#open-interesting-issues", 
            "text": "https://github.com/docker/swarm/issues/1106    https://github.com/docker/docker/issues/27082    https://github.com/docker/docker/issues/29816    https://github.com/docker/docker/issues/26696    https://github.com/docker/docker/issues/23813    More info about docker network internals can be read at:   Docker Reference Architecture", 
            "title": "Open interesting issues"
        }, 
        {
            "location": "/data-management/sth/readme/", 
            "text": "Comet\n\n\nHere you can find recipes aimed at different usages of Comet, the Reference\nImplementation of the STH Generic Enabler. We assume you are already familiar\nwith Comet. If not, refer to the\n\nofficial documentation\n.\n\n\nThe easiest and simplest way to try Comet is to follow the\n\nStandalone Walkthrough\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given in\n\nhttps://github.com/smartsdk/smartsdk-recipes\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/sth/readme/#comet", 
            "text": "Here you can find recipes aimed at different usages of Comet, the Reference\nImplementation of the STH Generic Enabler. We assume you are already familiar\nwith Comet. If not, refer to the official documentation .  The easiest and simplest way to try Comet is to follow the Standalone Walkthrough .  Instructions on how to prepare your environment to test these recipes are given in https://github.com/smartsdk/smartsdk-recipes .", 
            "title": "Comet"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/", 
            "text": "Standalone\n\n\nIntroduction\n\n\nThe idea of this standalone walkthrough is to test and showcase the Comet\nGeneric Enabler within a simple notification-based scenario, like the one\nillustrated below.\n\n\n\n\nA walkthrough\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you\ndon't have one, checkout the \ntools\n section for a\nquick way to setup a local swarm.\n\n\n    $ miniswarm start 3\n    $ eval $(docker-machine env ms-manager0)\n\n\n\n\nTo start the whole stack simply run, as usual:\n\n\n    $ docker stack deploy -c docker-compose.yml comet\n\n\n\n\nThen, wait until you see all the replicas up and running:\n\n\n    $ docker service ls\n    ID            NAME               MODE        REPLICAS  IMAGE\n    1ysxmrxrqvp4  comet_comet-mongo  replicated  1/1       mongo:3.2\n    8s9acybjxo0m  comet_orion        replicated  1/1       fiware/orion:latest\n    ra84eex0zsd0  comet_comet        replicated  3/3       telefonicaiot/fiware-sth-comet:latest\n    xg8ds3szkoi7  comet_orion-mongo  replicated  1/1       mongo:3.2\n\n\n\n\nNow let's start some checkups. For convenience, let's save the IP address of\nthe Orion and Comet services. In this scenario, since both are deployed on\nSwarm exposing their services ports, only one entry-point to the Swarm's\ningress network will suffice.\n\n\n    ORION=http://$(docker-machine ip ms-manager0)\n    COMET=http://$(docker-machine ip ms-manager0)\n\n\n\n\nLet's start some checkups, first making sure Orion is up and running.\n\n\n    $ sh ../../context-broker/query.sh $ORION\n    {\n    \norion\n : {\n      \nversion\n : \n1.7.0-next\n,\n      \nuptime\n : \n0 d, 0 h, 1 m, 39 s\n,\n      \ngit_hash\n : \nf710ee525f0fa55f665e578e309fc716c12cfd99\n,\n      \ncompile_time\n : \nWed Feb 22 10:14:18 UTC 2017\n,\n      \ncompiled_by\n : \nroot\n,\n      \ncompiled_in\n : \nb99744612d0b\n\n    }\n    }\n    []\n\n\n\n\nLet's insert some simple data (Room1 measurements):\n\n\n    $ sh ../../context-broker/insert.sh $ORION\n\n\n\n\nNow, let's subscribe Comet to the notifications of changes in temperature\nof Room1.\n\n\n    $ sh ../subscribe.sh $COMET\n    {\n      \nsubscribeResponse\n : {\n        \nsubscriptionId\n : \n58b98c0cdb69948641065907\n,\n        \nduration\n : \nPT24H\n\n      }\n    }\n\n\n\n\nLet's update the temperature value in Orion...\n\n\n    $ sh ../../context-broker/update.sh $ORION\n\n\n\n\nAnd check you can see the Short-Term-Historical view of both measurements.\n\n\n    $ sh ../query_sth.sh $COMET\n    {\n        \ncontextResponses\n: [\n            {\n                \ncontextElement\n: {\n                    \nattributes\n: [\n                        {\n                            \nname\n: \ntemperature\n,\n                            \nvalues\n: [\n                                {\n                                    \nattrType\n: \nFloat\n,\n                                    \nattrValue\n: 23,\n                                    \nrecvTime\n: \n2017-03-03T15:30:20.650Z\n\n                                },\n                                {\n                                    \nattrType\n: \nFloat\n,\n                                    \nattrValue\n: 29.3,\n                                    \nrecvTime\n: \n2017-03-03T15:32:48.741Z\n\n                                }\n                            ]\n                        }\n                    ],\n                    \nid\n: \nRoom1\n,\n                    \nisPattern\n: false,\n                    \ntype\n: \nRoom\n\n                },\n                \nstatusCode\n: {\n                    \ncode\n: \n200\n,\n                    \nreasonPhrase\n: \nOK\n\n                }\n            }\n        ]\n    }", 
            "title": "Standalone"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#standalone", 
            "text": "", 
            "title": "Standalone"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#introduction", 
            "text": "The idea of this standalone walkthrough is to test and showcase the Comet\nGeneric Enabler within a simple notification-based scenario, like the one\nillustrated below.", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/sth/standalone/readme/#a-walkthrough", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup. If you\ndon't have one, checkout the  tools  section for a\nquick way to setup a local swarm.      $ miniswarm start 3\n    $ eval $(docker-machine env ms-manager0)  To start the whole stack simply run, as usual:      $ docker stack deploy -c docker-compose.yml comet  Then, wait until you see all the replicas up and running:      $ docker service ls\n    ID            NAME               MODE        REPLICAS  IMAGE\n    1ysxmrxrqvp4  comet_comet-mongo  replicated  1/1       mongo:3.2\n    8s9acybjxo0m  comet_orion        replicated  1/1       fiware/orion:latest\n    ra84eex0zsd0  comet_comet        replicated  3/3       telefonicaiot/fiware-sth-comet:latest\n    xg8ds3szkoi7  comet_orion-mongo  replicated  1/1       mongo:3.2  Now let's start some checkups. For convenience, let's save the IP address of\nthe Orion and Comet services. In this scenario, since both are deployed on\nSwarm exposing their services ports, only one entry-point to the Swarm's\ningress network will suffice.      ORION=http://$(docker-machine ip ms-manager0)\n    COMET=http://$(docker-machine ip ms-manager0)  Let's start some checkups, first making sure Orion is up and running.      $ sh ../../context-broker/query.sh $ORION\n    {\n     orion  : {\n       version  :  1.7.0-next ,\n       uptime  :  0 d, 0 h, 1 m, 39 s ,\n       git_hash  :  f710ee525f0fa55f665e578e309fc716c12cfd99 ,\n       compile_time  :  Wed Feb 22 10:14:18 UTC 2017 ,\n       compiled_by  :  root ,\n       compiled_in  :  b99744612d0b \n    }\n    }\n    []  Let's insert some simple data (Room1 measurements):      $ sh ../../context-broker/insert.sh $ORION  Now, let's subscribe Comet to the notifications of changes in temperature\nof Room1.      $ sh ../subscribe.sh $COMET\n    {\n       subscribeResponse  : {\n         subscriptionId  :  58b98c0cdb69948641065907 ,\n         duration  :  PT24H \n      }\n    }  Let's update the temperature value in Orion...      $ sh ../../context-broker/update.sh $ORION  And check you can see the Short-Term-Historical view of both measurements.      $ sh ../query_sth.sh $COMET\n    {\n         contextResponses : [\n            {\n                 contextElement : {\n                     attributes : [\n                        {\n                             name :  temperature ,\n                             values : [\n                                {\n                                     attrType :  Float ,\n                                     attrValue : 23,\n                                     recvTime :  2017-03-03T15:30:20.650Z \n                                },\n                                {\n                                     attrType :  Float ,\n                                     attrValue : 29.3,\n                                     recvTime :  2017-03-03T15:32:48.741Z \n                                }\n                            ]\n                        }\n                    ],\n                     id :  Room1 ,\n                     isPattern : false,\n                     type :  Room \n                },\n                 statusCode : {\n                     code :  200 ,\n                     reasonPhrase :  OK \n                }\n            }\n        ]\n    }", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/sth/ha/readme/", 
            "text": "HA\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed\nthe steps explained in the \ninstallation guide\n.\n\n\nIntroduction\n\n\nLet's test a deployment of Comet with multiple replicas in both its front-end\nand backend. The idea now is to get the scenario illustrated below.\n\n\n\n\nLater, this could be combined for example with an\n\nHA deployment of Orion Context Broker\n.\n\n\nA walkthrough\n\n\nFirst, you need to have a Docker Swarm (docker \n= 1.13) already setup. If you\ndon't have one, checkout the \ntools\n section for\na quick way to setup a local swarm.\n\n\n    miniswarm start 3\n    eval $(docker-machine env ms-manager0)\n\n\n\n\nComet needs a mongo database for its backend. If you have already deployed\nMongo within your cluster and would like to reuse that database, you can skip\nthe next step (deploying backend). You will just need to pay attention to\nthe variables you define for Comet to link to Mongo, namely,\n\nMONGO_SERVICE_URI\n and \nREPLICASET_NAME\n. Make sure you have the correct\nvalues in \nfrontend.env\n. The value of \nMONGO_SERVICE_URI\n should be a routable\naddress for mongo. If deployed within the swarm, the service name (with stack\nprefix) would suffice. You can read more in the\n\nofficial docker docs\n.\nThe default values should be fine for you if you used the\n\nMongo Replicaset Recipe\n.\n\n\nOtherwise, if you prefer to make a new deployment of Mongo just for Comet, you\ncan take a shortcut and run...\n\n\n    sh deploy_back.sh\n\n\n\n\nAfter a while, when the replica-set is ready, you can deploy comet by running...\n\n\n    sh deploy_front.sh\n\n\n\n\nNow, as usual, a brief test to confirm everything is properly connected.\nAs a source of notifications, we have deployed Orion in the swarm (see\n\nOrion in HA\n for example).\n\n\nFor convenience, let's save the IP address of the Orion and Comet services.\nIn this scenario, since both are deployed on Swarm exposing their services\nports, only one entry-point to the Swarm's ingress network will suffice.\n\n\n    ORION=http://$(docker-machine ip ms-manager0)\n    COMET=http://$(docker-machine ip ms-manager0)\n\n\n\n\nInsert:\n\n\n    sh ../../context-broker/insert.sh $ORION\n    sh ../../context-broker/query.sh $ORION\n    ...\n\n\n\n\nSubscribe:\n\n\n    sh ../subscribe.sh $ORION\n    {\n      \nsubscribeResponse\n : {\n        \nsubscriptionId\n : \n58bd1940b97cc713f5eacdb7\n,\n        \nduration\n : \nPT24H\n\n      }\n    }\n\n\n\n\nUpdate:\n\n\n    sh ../../context-broker/update.sh $ORION\n\n\n\n\nAnd voila:\n\n\n    sh ../query_sth.sh $COMET\n    {\n    \ncontextResponses\n: [\n        {\n            \ncontextElement\n: {\n                \nattributes\n: [\n                    {\n                        \nname\n: \ntemperature\n,\n                        \nvalues\n: [\n                            {\n                                \nattrType\n: \nFloat\n,\n                                \nattrValue\n: 23,\n                                \nrecvTime\n: \n2017-03-06T08:09:36.493Z\n\n                            },\n                            {\n                                \nattrType\n: \nFloat\n,\n                                \nattrValue\n: 29.3,\n                                \nrecvTime\n: \n2017-03-06T08:11:14.044Z\n\n                            }\n                        ]\n                    }\n                ],\n                \nid\n: \nRoom1\n,\n                \nisPattern\n: false,\n                \ntype\n: \nRoom\n\n            },\n            \nstatusCode\n: {\n                \ncode\n: \n200\n,\n                \nreasonPhrase\n: \nOK\n\n            }\n        }\n    ]\n    }", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#ha", 
            "text": "", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed\nthe steps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#introduction", 
            "text": "Let's test a deployment of Comet with multiple replicas in both its front-end\nand backend. The idea now is to get the scenario illustrated below.   Later, this could be combined for example with an HA deployment of Orion Context Broker .", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/sth/ha/readme/#a-walkthrough", 
            "text": "First, you need to have a Docker Swarm (docker  = 1.13) already setup. If you\ndon't have one, checkout the  tools  section for\na quick way to setup a local swarm.      miniswarm start 3\n    eval $(docker-machine env ms-manager0)  Comet needs a mongo database for its backend. If you have already deployed\nMongo within your cluster and would like to reuse that database, you can skip\nthe next step (deploying backend). You will just need to pay attention to\nthe variables you define for Comet to link to Mongo, namely, MONGO_SERVICE_URI  and  REPLICASET_NAME . Make sure you have the correct\nvalues in  frontend.env . The value of  MONGO_SERVICE_URI  should be a routable\naddress for mongo. If deployed within the swarm, the service name (with stack\nprefix) would suffice. You can read more in the official docker docs .\nThe default values should be fine for you if you used the Mongo Replicaset Recipe .  Otherwise, if you prefer to make a new deployment of Mongo just for Comet, you\ncan take a shortcut and run...      sh deploy_back.sh  After a while, when the replica-set is ready, you can deploy comet by running...      sh deploy_front.sh  Now, as usual, a brief test to confirm everything is properly connected.\nAs a source of notifications, we have deployed Orion in the swarm (see Orion in HA  for example).  For convenience, let's save the IP address of the Orion and Comet services.\nIn this scenario, since both are deployed on Swarm exposing their services\nports, only one entry-point to the Swarm's ingress network will suffice.      ORION=http://$(docker-machine ip ms-manager0)\n    COMET=http://$(docker-machine ip ms-manager0)  Insert:      sh ../../context-broker/insert.sh $ORION\n    sh ../../context-broker/query.sh $ORION\n    ...  Subscribe:      sh ../subscribe.sh $ORION\n    {\n       subscribeResponse  : {\n         subscriptionId  :  58bd1940b97cc713f5eacdb7 ,\n         duration  :  PT24H \n      }\n    }  Update:      sh ../../context-broker/update.sh $ORION  And voila:      sh ../query_sth.sh $COMET\n    {\n     contextResponses : [\n        {\n             contextElement : {\n                 attributes : [\n                    {\n                         name :  temperature ,\n                         values : [\n                            {\n                                 attrType :  Float ,\n                                 attrValue : 23,\n                                 recvTime :  2017-03-06T08:09:36.493Z \n                            },\n                            {\n                                 attrType :  Float ,\n                                 attrValue : 29.3,\n                                 recvTime :  2017-03-06T08:11:14.044Z \n                            }\n                        ]\n                    }\n                ],\n                 id :  Room1 ,\n                 isPattern : false,\n                 type :  Room \n            },\n             statusCode : {\n                 code :  200 ,\n                 reasonPhrase :  OK \n            }\n        }\n    ]\n    }", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/data-management/cygnus/readme/", 
            "text": "Cygnus\n\n\nHere you can find recipes aimed at different usages of Cygnus, in particular,\ncygnus-ngsi. We assume you are already familiar with it, but if not, refer to\nthe \nofficial documentation\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given\nin the \nInstallation\n section of the docs.\n\n\nSome Considerations regarding HA\n\n\nThe first thing to note is that when we talk about high availability while using\ncygnus, we refer to the availability of data processed by cygnus agents before\nit's dropped to the final storage solution. So, as you can read from the\n\nofficial documentation\n,\ndifferent sinks can give you persistence in different storage solutions\n(mongodb, mysql, hdfs, etc). Keeping the persisted data in HA mode is a\ndifferent challenge and the implementation will depend on the used solution.\nFor the case of MongoDB, you can have a look at the\n\nMongoDB Replicaset Recipe\n.\nSo, this recipes will show you how to connect to some backends, but how you\nmanage them is up to you.\n\n\nMoreover, we will be discussing the deployment of the agent as a single\nconfigurable entity. But note that within an agent, there exist multiple\navailable configurations (using single and multiple sources, channels and\nsinks), as described in\n\nAdvanced Cygnus Architectures\n.\nHow you setup those internal advanced architectures and the advantages of each\nwill not be covered here, since this is already discussed in the official\ndocumentation.\n\n\nThat being said, in order to deploy Cygnus, we need to understand whether it's\na stateless or stateful service. It turns out that source and sink parts of the\nagent do not persist data, however, the channels do, at least for a short period\nof time until data is processed and taken out of the channel by the sink[s]. As\nyou can read from Cygnus and Flume's documentation, channels come in the form of\nMemoryChannel and FileChannel.\n\n\nIt's easy to see that MemoryChannel and Batching has a potential for data loss,\nfor example, in the case when an agent crashes before events were taken out of\nthe channel. In fact, to avoid storage access for each event, Cygnus comes with\ndefault values of batch size and batch flush timeouts. As a side note, it'd be\nnice if this could be dynamically changed according to dynamic demands, but this\nis an interesting point to investigate at a later point.\n\n\nTherefore, the FileChannel could be used, at the cost of a higher latency, to\ngive persistence to the \"inflight data\" and this way prevent a complete loss\nif there happens to be a software failure/crash at the agent. Note however that\nthe location of this file within the container and not customizable, hence\na container crash will cause those un-flushed values to be lost.\n\n\nOne could explore ways to persist those channels somewhere else, or share\nchannels across different containers. Or maybe looking for Message-based\nsolutions such as Solace to be used at the Channel level. But this of course,\nwould involve some updates to Cygnus which are beyond the scope of this project.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/cygnus/readme/#cygnus", 
            "text": "Here you can find recipes aimed at different usages of Cygnus, in particular,\ncygnus-ngsi. We assume you are already familiar with it, but if not, refer to\nthe  official documentation .  Instructions on how to prepare your environment to test these recipes are given\nin the  Installation  section of the docs.", 
            "title": "Cygnus"
        }, 
        {
            "location": "/data-management/cygnus/readme/#some-considerations-regarding-ha", 
            "text": "The first thing to note is that when we talk about high availability while using\ncygnus, we refer to the availability of data processed by cygnus agents before\nit's dropped to the final storage solution. So, as you can read from the official documentation ,\ndifferent sinks can give you persistence in different storage solutions\n(mongodb, mysql, hdfs, etc). Keeping the persisted data in HA mode is a\ndifferent challenge and the implementation will depend on the used solution.\nFor the case of MongoDB, you can have a look at the MongoDB Replicaset Recipe .\nSo, this recipes will show you how to connect to some backends, but how you\nmanage them is up to you.  Moreover, we will be discussing the deployment of the agent as a single\nconfigurable entity. But note that within an agent, there exist multiple\navailable configurations (using single and multiple sources, channels and\nsinks), as described in Advanced Cygnus Architectures .\nHow you setup those internal advanced architectures and the advantages of each\nwill not be covered here, since this is already discussed in the official\ndocumentation.  That being said, in order to deploy Cygnus, we need to understand whether it's\na stateless or stateful service. It turns out that source and sink parts of the\nagent do not persist data, however, the channels do, at least for a short period\nof time until data is processed and taken out of the channel by the sink[s]. As\nyou can read from Cygnus and Flume's documentation, channels come in the form of\nMemoryChannel and FileChannel.  It's easy to see that MemoryChannel and Batching has a potential for data loss,\nfor example, in the case when an agent crashes before events were taken out of\nthe channel. In fact, to avoid storage access for each event, Cygnus comes with\ndefault values of batch size and batch flush timeouts. As a side note, it'd be\nnice if this could be dynamically changed according to dynamic demands, but this\nis an interesting point to investigate at a later point.  Therefore, the FileChannel could be used, at the cost of a higher latency, to\ngive persistence to the \"inflight data\" and this way prevent a complete loss\nif there happens to be a software failure/crash at the agent. Note however that\nthe location of this file within the container and not customizable, hence\na container crash will cause those un-flushed values to be lost.  One could explore ways to persist those channels somewhere else, or share\nchannels across different containers. Or maybe looking for Message-based\nsolutions such as Solace to be used at the Channel level. But this of course,\nwould involve some updates to Cygnus which are beyond the scope of this project.", 
            "title": "Some Considerations regarding HA"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/", 
            "text": "Cygnus\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed the\n steps explained in the \ninstallation guide\n.\n\n\nGetting started\n\n\nThis recipe will show you how to deploy a default cygnus-ngsi configuration with\n a MySQL backend. Note that this generic enabler can actually be deployed with\n \nmany other backends\n.\n\n\nThis recipe in particular requires the use of\n\ndocker \"configs\"\n\nand hence depends on a docker-compose file version \"3.3\", supported in docker\nversions 17.06.0+.\n\n\nInstructions on how to prepare your environment to test these recipes are given\nin the \nInstallation\n section of the docs. Assuming\nyou have created a 3-nodes Swarm setup, this deployment will look as follows...\n\n\n\n\nAs you may already know\n\nfrom the docs\n,\nin order to configure cygnus, you need to provide a\nspecific agent configuration file. In this case, you can customize the\n\ncygnus_agent.conf\n and \ncartodb_keys.conf\n files within the \nconf\n folder.\nThe content of these files will be loaded by docker into their corresponding\nconfigs, which will be available for all the replicas of the cygnus service.\n\n\nIf you inspect the \ndocker-compose.yml\n you will realize that you can\ncustomize the values of the MySQL user and password by setting the environment\nvariables \nCYGNUS_MYSQL_USER\n and \nCYGNUS_MYSQL_PASS\n.\n\n\nTo launch the example as it is, simply run:\n\n\n    docker stack deploy -c docker-compose.yml cygnus\n\n\n\n\nAfter a couple of minutes you should be able to see the two services up and\nrunning.\n\n\n    $ docker service ls\n    ID                  NAME                   MODE                REPLICAS            IMAGE                       PORTS\n    l3h1fsk36v35        cygnus_mysql           replicated          1/1                 mysql:latest                *:3306-\n3306/tcp\n    vmju1turlizr        cygnus_cygnus-common   replicated          3/3                 fiware/cygnus-ngsi:latest   *:5050-\n5050/tcp\n\n\n\n\nFor illustration purposes, let's send an NGSI notification to cygnus' entrypoint\nusing the simple \nnotification.sh\n script.\n\n\n    $ sh notification.sh http://0.0.0.0:5050/notify\n    *   Trying 0.0.0.0...\n    * TCP_NODELAY set\n    * Connected to 0.0.0.0 (127.0.0.1) port 5050 (#0)\n    \n POST /notify HTTP/1.1\n    \n Host: 0.0.0.0:5050\n    \n User-Agent: curl/7.54.0\n    \n Content-Type: application/json; charset=utf-8\n    \n Accept: application/json\n    \n Fiware-Service: default\n    \n Fiware-ServicePath: /\n    \n Content-Length: 607\n    \n\n    * upload completely sent off: 607 out of 607 bytes\n    \n HTTP/1.1 200 OK\n    \n Transfer-Encoding: chunked\n    \n Server: Jetty(6.1.26)\n    \n\n    * Connection #0 to host 0.0.0.0 left intact\n\n\n\n\nBy now, the data sent by the script has been processed by cygnus and will be\navailable in the configured sink (MySQL in this case).\n\n\nHaving cygnus running as a service on a Docker Swarm cluster, scaling it can be\nachieved as with any other docker service. For more details, refer to the\n\nOrion recipe\n to see how this can be done\nwith Docker. Otherwise, refer to the\n\nDocker service docs\n.\n\n\nCustomisations\n\n\nWhat if I wanted a different backend?\n\n\nIf you wanted to try a different backend for your cygnus deployment, there are 3\nsteps you need to follow.\n\n\n\n\n\n\nConfigure your \ncygnus_agent.conf\n according to your needs. More info\n  \nin the docs\n.\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n, specifically the environment variables\n  configured for the cygnus service.\n  For example, if you wanted to use MongoDB instead of MySQL, you'll need to\n  use variables CYGNUS_MONGO_USER and CYGNUS_MONGO_PASS. For a complete list\n  of required variables, refer to the\n  \ncygnus docs\n.\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n, removing the definition of the mysql service\n  and introducing the one of your preference. Also, don't forget to update the\n  \ndepends_on:\n section of cygnus with the name of your new service.\n\n\n\n\n\n\nUsing a different channel\n\n\nIf you take a look at the configuration file in \nconf/cygnus_agent.conf\n, you\ncan choose between a Memory-based or a File-based channel. Feel free to\ncomment/uncomment (i.e, leave/remove the \n#\n character) from the channel type\nconfiguration.\n\n\ncygnus-ngsi.channels.main-channel.type = memory\n#cygnus-ngsi.channels.main-channel.type = file\n\n\n\nFor more info on channels, checkout the\n\nchannels considerations\n\nin the official docs.", 
            "title": "HA"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#cygnus", 
            "text": "", 
            "title": "Cygnus"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed the\n steps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#getting-started", 
            "text": "This recipe will show you how to deploy a default cygnus-ngsi configuration with\n a MySQL backend. Note that this generic enabler can actually be deployed with\n  many other backends .  This recipe in particular requires the use of docker \"configs\" \nand hence depends on a docker-compose file version \"3.3\", supported in docker\nversions 17.06.0+.  Instructions on how to prepare your environment to test these recipes are given\nin the  Installation  section of the docs. Assuming\nyou have created a 3-nodes Swarm setup, this deployment will look as follows...   As you may already know from the docs ,\nin order to configure cygnus, you need to provide a\nspecific agent configuration file. In this case, you can customize the cygnus_agent.conf  and  cartodb_keys.conf  files within the  conf  folder.\nThe content of these files will be loaded by docker into their corresponding\nconfigs, which will be available for all the replicas of the cygnus service.  If you inspect the  docker-compose.yml  you will realize that you can\ncustomize the values of the MySQL user and password by setting the environment\nvariables  CYGNUS_MYSQL_USER  and  CYGNUS_MYSQL_PASS .  To launch the example as it is, simply run:      docker stack deploy -c docker-compose.yml cygnus  After a couple of minutes you should be able to see the two services up and\nrunning.      $ docker service ls\n    ID                  NAME                   MODE                REPLICAS            IMAGE                       PORTS\n    l3h1fsk36v35        cygnus_mysql           replicated          1/1                 mysql:latest                *:3306- 3306/tcp\n    vmju1turlizr        cygnus_cygnus-common   replicated          3/3                 fiware/cygnus-ngsi:latest   *:5050- 5050/tcp  For illustration purposes, let's send an NGSI notification to cygnus' entrypoint\nusing the simple  notification.sh  script.      $ sh notification.sh http://0.0.0.0:5050/notify\n    *   Trying 0.0.0.0...\n    * TCP_NODELAY set\n    * Connected to 0.0.0.0 (127.0.0.1) port 5050 (#0)\n      POST /notify HTTP/1.1\n      Host: 0.0.0.0:5050\n      User-Agent: curl/7.54.0\n      Content-Type: application/json; charset=utf-8\n      Accept: application/json\n      Fiware-Service: default\n      Fiware-ServicePath: /\n      Content-Length: 607\n     \n    * upload completely sent off: 607 out of 607 bytes\n      HTTP/1.1 200 OK\n      Transfer-Encoding: chunked\n      Server: Jetty(6.1.26)\n     \n    * Connection #0 to host 0.0.0.0 left intact  By now, the data sent by the script has been processed by cygnus and will be\navailable in the configured sink (MySQL in this case).  Having cygnus running as a service on a Docker Swarm cluster, scaling it can be\nachieved as with any other docker service. For more details, refer to the Orion recipe  to see how this can be done\nwith Docker. Otherwise, refer to the Docker service docs .", 
            "title": "Getting started"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#customisations", 
            "text": "", 
            "title": "Customisations"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#what-if-i-wanted-a-different-backend", 
            "text": "If you wanted to try a different backend for your cygnus deployment, there are 3\nsteps you need to follow.    Configure your  cygnus_agent.conf  according to your needs. More info\n   in the docs .    Update the  docker-compose.yml , specifically the environment variables\n  configured for the cygnus service.\n  For example, if you wanted to use MongoDB instead of MySQL, you'll need to\n  use variables CYGNUS_MONGO_USER and CYGNUS_MONGO_PASS. For a complete list\n  of required variables, refer to the\n   cygnus docs .    Update the  docker-compose.yml , removing the definition of the mysql service\n  and introducing the one of your preference. Also, don't forget to update the\n   depends_on:  section of cygnus with the name of your new service.", 
            "title": "What if I wanted a different backend?"
        }, 
        {
            "location": "/data-management/cygnus/ha/readme/#using-a-different-channel", 
            "text": "If you take a look at the configuration file in  conf/cygnus_agent.conf , you\ncan choose between a Memory-based or a File-based channel. Feel free to\ncomment/uncomment (i.e, leave/remove the  #  character) from the channel type\nconfiguration.  cygnus-ngsi.channels.main-channel.type = memory\n#cygnus-ngsi.channels.main-channel.type = file  For more info on channels, checkout the channels considerations \nin the official docs.", 
            "title": "Using a different channel"
        }, 
        {
            "location": "/data-management/quantumleap/readme/", 
            "text": "QuantumLeap\n\n\nIntroduction\n\n\nHere you can find recipes aimed at different usages of QuantumLeap. We assume\nyou are already familiar with it, otherwise refer to the\n\nofficial documentation\n.\n\n\nInstructions on how to prepare your environment to test these recipes are given\nin the \ninstallation section\n.\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed the\nsteps explained in the \ninstallation guide\n.\n\n\nHA Deployment overview\n\n\n\n\nA Simple Walkthrough\n\n\nBefore Starting\n\n\nBefore we launch the stack, you need to define a domain for the entrypoint\nof your docker cluster. Save that domain in an environment variable like this:\n\n\n    $ export CLUSTER_DOMAIN=mydomain.com\n\n\n\n\nIf you are just testing locally and don't own one, you can fake it editing your\n\n/etc/hosts\n file to add an entry that points to the IP of any of the nodes of\nyour Swarm Cluster (replace 192.168.99.100 with the IP if your cluster\nentrypoint). See the example below.\n\n\n    # End of /etc/hosts file\n    192.168.99.100  mydomain.com\n    192.168.99.100  crate.mydomain.com\n\n\n\n\nNote we've included one entry for \ncrate.mydomain.com\n because we'll be\naccessing the CrateDB cluster UI through the \nTraefik\n\nproxy.\n\n\nYou will also need to set the values for the following 3 special environment\nvariables, depending on the structure of your cluster. The default values will\nassume your cluster has only 1 node, which is not ideal if your cluster has\nmultiple nodes.\n\n\n\n\n\n\nEXPECTED_NODES\n: How many nodes to wait for until the cluster state is\n  recovered. The value should be equal to the number of nodes in the cluster.\n\n\n\n\n\n\nRECOVER_AFTER_NODES\n: The number of nodes that need to be started before any\n  cluster state recovery will start.\n\n\n\n\n\n\nMINIMUM_MASTER_NODES\n: It\u2019s highly recommend to set the quorum greater than\n  half the maximum number of nodes in the cluster. I.e, (N / 2) + 1, where N is\n  the maximum number of nodes in the cluster.\n\n\n\n\n\n\nFor more details, check out\n\nthese docs\n\nor the corresponding section in\n\nelasticsearch docs\n.\n\n\nIn addition to those, you can also customise your deployment with the following\nenvironment variables:\n\n\n\n\nQL_VERSION\n: The docker tag of the \nQuantumleap image\n\n you want to deploy.\n\n\n\n\nDeploy\n\n\nNow, we're ready to launch the stack with the name \nql\n.\n\n\nIf you want to deploy the basic stack of QuantumLeap you can simply run...\n\n\n    $ docker stack deploy -c docker-compose ql\n\n\n\n\nOtherwise, if you'd like to include some extra services such as Grafana for data\nvisualisation, you can integrate the addons present in the\n\ndocker-compose-addons.yml\n. Unfortunately docker is currently not directly\nsupporting\n\nmultiple compose files to do a single deploy\n.\nHence the suggested way to proceed is the following...\n\n\n    # First we merge the two compose files using docker-compose\n    $ docker-compose -f docker-compose.yml -f docker-compose-addons.yml config \n ql.yml\n    # Now we deploy the \nql\n stack from the generated ql.yml file.\n    $ docker stack deploy -c ql.yml ql\n\n\n\n\nWait until you see all instances up and running (this might take some minutes).\n\n\n    $ docker service ls\n    ID                  NAME                MODE                REPLICAS            IMAGE                             PORTS\n    2vbj18blsqje        ql_traefik          global              1/1                 traefik:1.3.5-alpine              *:80-\n80/tcp,*:443-\n443/tcp,*:4200-\n4200/tcp,*:4300-\n4300/tcp,*:8080-\n8080/tcp\n    bvs32e81jcns        ql_viz              replicated          1/1                 dockersamples/visualizer:latest   *:8282-\n8080/tcp\n    e8kyp4vylvev        ql_quantumleap      replicated          1/1                 smartsdk/quantumleap:latest       *:8668-\n8668/tcp\n    ignls7l57hzn        ql_crate            global              3/3                 crate:1.0.5                       \n    tfszxc2fcmxx        ql_grafana          replicated          1/1                 grafana/grafana:latest            *:3000-\n3000/tcp\n\n\n\n\nNow you are ready to scale services according to your needs using simple docker\nservice scale command as explained in\n\nthe official docs\n.\n\n\nExplore\n\n\nNow, if you open your explorer to\n\nhttp://crate.mydomain.com\n you should see the\nCRATE.IO dashboard. In the \"cluster\" tab you should see the same number of nodes\nyou have in the swarm cluster.\n\n\nFor a quick test, you can use the \ninsert.sh\n script in this folder.\n\n\n    $ sh insert.sh IP_OF_ANY_SWARM_NODE 8668\n\n\n\n\nOtherwise, open your favourite API tester and send the fake notification shown\nbelow to QuantumLeap to later see it persisted in the database through the Crate\nDashboard.\n\n\n    # Simple examples payload to send to IP_OF_ANY_SWARM_NODE:8668/notify\n    {\n        \nsubscriptionId\n: \n5947d174793fe6f7eb5e3961\n,\n        \ndata\n: [\n            {\n                \nid\n: \nRoom1\n,\n                \ntype\n: \nRoom\n,\n                \ntemperature\n: {\n                    \ntype\n: \nNumber\n,\n                    \nvalue\n: 27.6,\n                    \nmetadata\n: {\n                        \ndateModified\n: {\n                            \ntype\n: \nDateTime\n,\n                            \nvalue\n: \n2017-06-19T11:46:45.00Z\n\n                        }\n                    }\n                }\n            }\n        ]\n    }\n\n\n\n\nRemember in the typical scenario is not the client that will be sending payloads\ndirectly to the \n/notify\n endpoint, but rather \nOrion Context Broker\n in the\nform of notifications. More info in the\n\nofficial documentation\n.\n\n\nYou can use the postman collection available in the\n\ntools section\n.\n\n\nFor further information, please refer to the\n\nQuantumLeap's User Manual\n.", 
            "title": "Intro"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#quantumleap", 
            "text": "", 
            "title": "QuantumLeap"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#introduction", 
            "text": "Here you can find recipes aimed at different usages of QuantumLeap. We assume\nyou are already familiar with it, otherwise refer to the official documentation .  Instructions on how to prepare your environment to test these recipes are given\nin the  installation section .", 
            "title": "Introduction"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed the\nsteps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#ha-deployment-overview", 
            "text": "", 
            "title": "HA Deployment overview"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#a-simple-walkthrough", 
            "text": "", 
            "title": "A Simple Walkthrough"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#before-starting", 
            "text": "Before we launch the stack, you need to define a domain for the entrypoint\nof your docker cluster. Save that domain in an environment variable like this:      $ export CLUSTER_DOMAIN=mydomain.com  If you are just testing locally and don't own one, you can fake it editing your /etc/hosts  file to add an entry that points to the IP of any of the nodes of\nyour Swarm Cluster (replace 192.168.99.100 with the IP if your cluster\nentrypoint). See the example below.      # End of /etc/hosts file\n    192.168.99.100  mydomain.com\n    192.168.99.100  crate.mydomain.com  Note we've included one entry for  crate.mydomain.com  because we'll be\naccessing the CrateDB cluster UI through the  Traefik \nproxy.  You will also need to set the values for the following 3 special environment\nvariables, depending on the structure of your cluster. The default values will\nassume your cluster has only 1 node, which is not ideal if your cluster has\nmultiple nodes.    EXPECTED_NODES : How many nodes to wait for until the cluster state is\n  recovered. The value should be equal to the number of nodes in the cluster.    RECOVER_AFTER_NODES : The number of nodes that need to be started before any\n  cluster state recovery will start.    MINIMUM_MASTER_NODES : It\u2019s highly recommend to set the quorum greater than\n  half the maximum number of nodes in the cluster. I.e, (N / 2) + 1, where N is\n  the maximum number of nodes in the cluster.    For more details, check out these docs \nor the corresponding section in elasticsearch docs .  In addition to those, you can also customise your deployment with the following\nenvironment variables:   QL_VERSION : The docker tag of the  Quantumleap image \n you want to deploy.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#deploy", 
            "text": "Now, we're ready to launch the stack with the name  ql .  If you want to deploy the basic stack of QuantumLeap you can simply run...      $ docker stack deploy -c docker-compose ql  Otherwise, if you'd like to include some extra services such as Grafana for data\nvisualisation, you can integrate the addons present in the docker-compose-addons.yml . Unfortunately docker is currently not directly\nsupporting multiple compose files to do a single deploy .\nHence the suggested way to proceed is the following...      # First we merge the two compose files using docker-compose\n    $ docker-compose -f docker-compose.yml -f docker-compose-addons.yml config   ql.yml\n    # Now we deploy the  ql  stack from the generated ql.yml file.\n    $ docker stack deploy -c ql.yml ql  Wait until you see all instances up and running (this might take some minutes).      $ docker service ls\n    ID                  NAME                MODE                REPLICAS            IMAGE                             PORTS\n    2vbj18blsqje        ql_traefik          global              1/1                 traefik:1.3.5-alpine              *:80- 80/tcp,*:443- 443/tcp,*:4200- 4200/tcp,*:4300- 4300/tcp,*:8080- 8080/tcp\n    bvs32e81jcns        ql_viz              replicated          1/1                 dockersamples/visualizer:latest   *:8282- 8080/tcp\n    e8kyp4vylvev        ql_quantumleap      replicated          1/1                 smartsdk/quantumleap:latest       *:8668- 8668/tcp\n    ignls7l57hzn        ql_crate            global              3/3                 crate:1.0.5                       \n    tfszxc2fcmxx        ql_grafana          replicated          1/1                 grafana/grafana:latest            *:3000- 3000/tcp  Now you are ready to scale services according to your needs using simple docker\nservice scale command as explained in the official docs .", 
            "title": "Deploy"
        }, 
        {
            "location": "/data-management/quantumleap/readme/#explore", 
            "text": "Now, if you open your explorer to http://crate.mydomain.com  you should see the\nCRATE.IO dashboard. In the \"cluster\" tab you should see the same number of nodes\nyou have in the swarm cluster.  For a quick test, you can use the  insert.sh  script in this folder.      $ sh insert.sh IP_OF_ANY_SWARM_NODE 8668  Otherwise, open your favourite API tester and send the fake notification shown\nbelow to QuantumLeap to later see it persisted in the database through the Crate\nDashboard.      # Simple examples payload to send to IP_OF_ANY_SWARM_NODE:8668/notify\n    {\n         subscriptionId :  5947d174793fe6f7eb5e3961 ,\n         data : [\n            {\n                 id :  Room1 ,\n                 type :  Room ,\n                 temperature : {\n                     type :  Number ,\n                     value : 27.6,\n                     metadata : {\n                         dateModified : {\n                             type :  DateTime ,\n                             value :  2017-06-19T11:46:45.00Z \n                        }\n                    }\n                }\n            }\n        ]\n    }  Remember in the typical scenario is not the client that will be sending payloads\ndirectly to the  /notify  endpoint, but rather  Orion Context Broker  in the\nform of notifications. More info in the official documentation .  You can use the postman collection available in the tools section .  For further information, please refer to the QuantumLeap's User Manual .", 
            "title": "Explore"
        }, 
        {
            "location": "/iot-services/readme/", 
            "text": "Backend Device Management (IDAS)\n\n\nFor more info about this Chapter, see\n\nhere\n.\n\n\nFor more info about this GE, see\n\nhere\n.\n\n\nIoT Agents\n\n\nWhy using an IoT Agent?\n\n\n\n\n\n\nTransform IoT Device specific protocol to NGSI (a.k.a Active attributes)\n\n\n\n\n\n\nRequest data from the IoT Device at some intervals (a.k.a Lazy attributes)\n\n\n\n\n\n\nExecute commands on the IoT Device communication based on context in the\n  Broker.\n\n\n\n\n\n\nExplore the subfolders of each of the available IoT Agent recipes. The Agents\ncan be considered stateless services and hence be deployed in the replicated\nmode of swarm with any required amount of replicas, provided that you keep your\nconfigurations using the service name of the agent for routing purposes.\n\n\nFor good scalability, when deploying the mongo databases that your agents may\nuse in case of persistence need, make sure you deploy with the provided recipe\nfor mongo replica sets. More info \nhere\n.\n\n\nTesting\n\n\nIf you would like to try the IoT Agents in a local development environment,\nyou can deploy the complementary services (Orion and Mongo) by using their\ncorresponding recipes.\nSee \nOrion's\n and \nMongo's\n\n\nIf you are following the step-by-step of the official documentation guides,\nyou may find the scripts in the \ntest\n subfolders useful. Remember you no longer\nhave everything running in local host so you will have to adjust the urls\nsometimes. You can use the service names if you are inside the docker network or\nthe IP of your docker swarm node (any of them) if you are outside the cluster.\nCheckout the \nsetup.sh\n.", 
            "title": "IDAS"
        }, 
        {
            "location": "/iot-services/readme/#backend-device-management-idas", 
            "text": "For more info about this Chapter, see here .  For more info about this GE, see here .", 
            "title": "Backend Device Management (IDAS)"
        }, 
        {
            "location": "/iot-services/readme/#iot-agents", 
            "text": "", 
            "title": "IoT Agents"
        }, 
        {
            "location": "/iot-services/readme/#why-using-an-iot-agent", 
            "text": "Transform IoT Device specific protocol to NGSI (a.k.a Active attributes)    Request data from the IoT Device at some intervals (a.k.a Lazy attributes)    Execute commands on the IoT Device communication based on context in the\n  Broker.    Explore the subfolders of each of the available IoT Agent recipes. The Agents\ncan be considered stateless services and hence be deployed in the replicated\nmode of swarm with any required amount of replicas, provided that you keep your\nconfigurations using the service name of the agent for routing purposes.  For good scalability, when deploying the mongo databases that your agents may\nuse in case of persistence need, make sure you deploy with the provided recipe\nfor mongo replica sets. More info  here .", 
            "title": "Why using an IoT Agent?"
        }, 
        {
            "location": "/iot-services/readme/#testing", 
            "text": "If you would like to try the IoT Agents in a local development environment,\nyou can deploy the complementary services (Orion and Mongo) by using their\ncorresponding recipes.\nSee  Orion's  and  Mongo's  If you are following the step-by-step of the official documentation guides,\nyou may find the scripts in the  test  subfolders useful. Remember you no longer\nhave everything running in local host so you will have to adjust the urls\nsometimes. You can use the service names if you are inside the docker network or\nthe IP of your docker swarm node (any of them) if you are outside the cluster.\nCheckout the  setup.sh .", 
            "title": "Testing"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/", 
            "text": "IoT Agent (JSON)\n\n\nOfficial documentation of this IoT Agent:\n\nhere\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed the\nsteps explained in the \ninstallation guide\n.\n\n\nMQTT Transport\n\n\nWhat you can customise\n\n\nVia ENV variables\n\n\nFor the documentation of the variables please refer to the\n\nglobal configuration docs\n.\n\n\n\n\n\n\nMOSQUITTO_VERSION\n: Version number (tag) of the\n  \nMosquitto Docker Image\n.\n  Defaults to \n1.4.12\n.\n\n\n\n\n\n\nIOTA_MQTT_HOST\n: Defaults to \nmosquitto\n, which is the name of the docker\n  service.\n\n\n\n\n\n\nIOTA_MQTT_PORT\n: Defaults to \n1883\n.\n\n\n\n\n\n\nIOTA_VERSION\n: Version number (tag) of the\n  \nAgent Docker Image\n.\n  Defaults to \n1.6.0\n.\n\n\n\n\n\n\nIOTA_LOG_LEVEL\n: Defaults to \nDEBUG\n.\n\n\n\n\n\n\nIOTA_TIMESTAMP\n: Defaults to \ntrue\n.\n\n\n\n\n\n\nIOTA_CB_HOST\n: Defaults to \norion\n.\n\n\n\n\n\n\nIOTA_CB_PORT\n: Defaults to \n1026\n.\n\n\n\n\n\n\nIOTA_NORTH_PORT\n: Defaults to \n4041\n.\n\n\n\n\n\n\nIOTA_REGISTRY_TYPE\n: Defaults to \nmongodb\n.\n\n\n\n\n\n\nIOTA_MONGO_HOST\n: Defaults to \nmongo\n.\n\n\n\n\n\n\nIOTA_MONGO_PORT\n: Defaults to \n27017\n.\n\n\n\n\n\n\nIOTA_MONGO_DB\n: Defaults to \niotagentjson\n.\n\n\n\n\n\n\nIOTA_MONGO_REPLICASET\n: Defaults to \nrs\n. Unset to disable replicaset option.\n\n\n\n\n\n\nIOTA_HTTP_PORT\n: Defaults to \n7896\n.\n\n\n\n\n\n\nIOTA_PROVIDER_URL\n: Defaults to \nhttp://iotagent:4041\n.\n\n\n\n\n\n\nVia Files\n\n\n\n\n\n\nconfig.js\n: Feel free to edit this file before deployment, it will be used by\n  the agent as its config file. It is treated by docker as a\n  \nconfig\n. Remember that\n  values specified via ENV variables will override those set in the file.\n\n\n\n\n\n\nmosquitto.conf\n: Feel free to edit this file before deployment, it will be\n  used by mosquitto as its config file. It is treated by docker as a\n  \nconfig\n.\n\n\n\n\n\n\nDeploying this recipe\n\n\nWe assume you have already setup your environment as explained in the\n\nInstallation\n.\n\n\n    docker stack deploy -c docker-compose.yml iota-json\n\n\n\n\nThe deployed services will be:\n\n\n\n\n\n\nIoTAgent-json\n\n\n\n\n\n\nMosquitto\n as MQTT Broker\n\n\n\n\n\n\nImportant Things to keep in mind\n\n\n\n\n\n\nAs of today, the official Mosquitto Docker Image is not including the\n  mosquitto-clients, so if you want to execute commands like \nmosquitto_sub\n\n  and \nmosquitto_pub\n, you basically have 2 options:\n\n\n\n\n\n\nInstall them in your system and add the host parameter to point to the\n     docker mosquitto service.\n\n\n\n\n\n\nInstall the clients in the mosquitto container. Note this will not persist\n     after a container restart! If you need this to persist create your docker\n     image accordingly.\n\n\n\n\n\n\n       docker exec -ti mosquitto_container sh -c \napk --no-cache add mosquitto-clients\n    \n\n\n\n\nTODO\n\n\n\n\nComplete testing of the step-by-step guide to make sure this recipe provides\n  all the minimum requirements for a first successful walkthrough with\n  the Agent. Depends on\n  \nthis issue\n.", 
            "title": "IoT Agent JSON"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#iot-agent-json", 
            "text": "Official documentation of this IoT Agent: here", 
            "title": "IoT Agent (JSON)"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed the\nsteps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#mqtt-transport", 
            "text": "", 
            "title": "MQTT Transport"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#what-you-can-customise", 
            "text": "", 
            "title": "What you can customise"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#via-env-variables", 
            "text": "For the documentation of the variables please refer to the global configuration docs .    MOSQUITTO_VERSION : Version number (tag) of the\n   Mosquitto Docker Image .\n  Defaults to  1.4.12 .    IOTA_MQTT_HOST : Defaults to  mosquitto , which is the name of the docker\n  service.    IOTA_MQTT_PORT : Defaults to  1883 .    IOTA_VERSION : Version number (tag) of the\n   Agent Docker Image .\n  Defaults to  1.6.0 .    IOTA_LOG_LEVEL : Defaults to  DEBUG .    IOTA_TIMESTAMP : Defaults to  true .    IOTA_CB_HOST : Defaults to  orion .    IOTA_CB_PORT : Defaults to  1026 .    IOTA_NORTH_PORT : Defaults to  4041 .    IOTA_REGISTRY_TYPE : Defaults to  mongodb .    IOTA_MONGO_HOST : Defaults to  mongo .    IOTA_MONGO_PORT : Defaults to  27017 .    IOTA_MONGO_DB : Defaults to  iotagentjson .    IOTA_MONGO_REPLICASET : Defaults to  rs . Unset to disable replicaset option.    IOTA_HTTP_PORT : Defaults to  7896 .    IOTA_PROVIDER_URL : Defaults to  http://iotagent:4041 .", 
            "title": "Via ENV variables"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#via-files", 
            "text": "config.js : Feel free to edit this file before deployment, it will be used by\n  the agent as its config file. It is treated by docker as a\n   config . Remember that\n  values specified via ENV variables will override those set in the file.    mosquitto.conf : Feel free to edit this file before deployment, it will be\n  used by mosquitto as its config file. It is treated by docker as a\n   config .", 
            "title": "Via Files"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#deploying-this-recipe", 
            "text": "We assume you have already setup your environment as explained in the Installation .      docker stack deploy -c docker-compose.yml iota-json  The deployed services will be:    IoTAgent-json    Mosquitto  as MQTT Broker", 
            "title": "Deploying this recipe"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#important-things-to-keep-in-mind", 
            "text": "As of today, the official Mosquitto Docker Image is not including the\n  mosquitto-clients, so if you want to execute commands like  mosquitto_sub \n  and  mosquitto_pub , you basically have 2 options:    Install them in your system and add the host parameter to point to the\n     docker mosquitto service.    Install the clients in the mosquitto container. Note this will not persist\n     after a container restart! If you need this to persist create your docker\n     image accordingly.           docker exec -ti mosquitto_container sh -c  apk --no-cache add mosquitto-clients", 
            "title": "Important Things to keep in mind"
        }, 
        {
            "location": "/iot-services/iotagent-json/README/#todo", 
            "text": "Complete testing of the step-by-step guide to make sure this recipe provides\n  all the minimum requirements for a first successful walkthrough with\n  the Agent. Depends on\n   this issue .", 
            "title": "TODO"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/", 
            "text": "IoT Agent (LWM2M)\n\n\nOfficial documentation of this IoT Agent:\n\nhere\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed the\nsteps explained in the \ninstallation guide\n.\n\n\nHTTP Transport\n\n\nWhat you can customise\n\n\nVia ENV variables\n\n\n\n\n\n\nIOTA_VERSION:\n Version number (tag) of the\n  \nAgent Docker Image\n.\n  Defaults to \nlatest\n.\n\n\n\n\n\n\nIOTA_LWM2M_PORT\n: Defaults to \n5684\n.\n\n\n\n\n\n\nIOTA_LOG_LEVEL\n: Defaults to \nDEBUG\n.\n\n\n\n\n\n\nIOTA_CB_HOST\n: Defaults to \norion\n.\n\n\n\n\n\n\nIOTA_CB_PORT\n: Defaults to \n1026\n.\n\n\n\n\n\n\nIOTA_NORTH_PORT\n: Defaults to \n4041\n.\n\n\n\n\n\n\nIOTA_REGISTRY_TYPE\n: Defaults to \nmongodb\n.\n\n\n\n\n\n\nIOTA_MONGO_HOST\n: Defaults to \nmongo\n.\n\n\n\n\n\n\nIOTA_MONGO_PORT\n: Defaults to \n27017\n.\n\n\n\n\n\n\nIOTA_MONGO_DB\n: Defaults to \niotagentjson\n.\n\n\n\n\n\n\nIOTA_MONGO_REPLICASET\n: Defaults to \nrs\n. Unset to disable replicaset option.\n\n\n\n\n\n\nIOTA_PROVIDER_URL\n: Defaults to \nhttp://iotagent:4041\n.\n\n\n\n\n\n\nFor the documentation of the variables please refer to the\n\nglobal configuration docs\n.\n\n\nVia Files\n\n\n\n\nconfig.js\n: Feel free to edit this file before deployment, it will be used by\n  the agent as its config file. It is treated by docker as a\n  \nconfig\n.\n\n\n\n\nDeploying this recipe\n\n\nWe assume you have already setup your environment as explained in the\n\nInstallation\n.\n\n\n    docker stack deploy -c docker-compose.yml iota-lwm2m\n\n\n\n\nThe deployed services will be:\n\n\n\n\nIoTAgent-lwm2m\n\n\n\n\nNote\n\n\nIf you are following the\n\nofficial step-by-step guide\n,\nyou can quickly launch the lwm2m client as:\n\n\n    docker exec -ti [AGENT_CONTAINER_ID_HERE] node_modules/lwm2m-node-lib/bin/iotagent-lwm2m-client.js", 
            "title": "IoT Agent LWM2M"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#iot-agent-lwm2m", 
            "text": "Official documentation of this IoT Agent: here", 
            "title": "IoT Agent (LWM2M)"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed the\nsteps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#http-transport", 
            "text": "", 
            "title": "HTTP Transport"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#what-you-can-customise", 
            "text": "", 
            "title": "What you can customise"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#via-env-variables", 
            "text": "IOTA_VERSION:  Version number (tag) of the\n   Agent Docker Image .\n  Defaults to  latest .    IOTA_LWM2M_PORT : Defaults to  5684 .    IOTA_LOG_LEVEL : Defaults to  DEBUG .    IOTA_CB_HOST : Defaults to  orion .    IOTA_CB_PORT : Defaults to  1026 .    IOTA_NORTH_PORT : Defaults to  4041 .    IOTA_REGISTRY_TYPE : Defaults to  mongodb .    IOTA_MONGO_HOST : Defaults to  mongo .    IOTA_MONGO_PORT : Defaults to  27017 .    IOTA_MONGO_DB : Defaults to  iotagentjson .    IOTA_MONGO_REPLICASET : Defaults to  rs . Unset to disable replicaset option.    IOTA_PROVIDER_URL : Defaults to  http://iotagent:4041 .    For the documentation of the variables please refer to the global configuration docs .", 
            "title": "Via ENV variables"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#via-files", 
            "text": "config.js : Feel free to edit this file before deployment, it will be used by\n  the agent as its config file. It is treated by docker as a\n   config .", 
            "title": "Via Files"
        }, 
        {
            "location": "/iot-services/iotagent-lwm2m/readme/#deploying-this-recipe", 
            "text": "We assume you have already setup your environment as explained in the Installation .      docker stack deploy -c docker-compose.yml iota-lwm2m  The deployed services will be:   IoTAgent-lwm2m   Note  If you are following the official step-by-step guide ,\nyou can quickly launch the lwm2m client as:      docker exec -ti [AGENT_CONTAINER_ID_HERE] node_modules/lwm2m-node-lib/bin/iotagent-lwm2m-client.js", 
            "title": "Deploying this recipe"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/", 
            "text": "IoT Agent (UL)\n\n\nOfficial documentation of this IoT Agent:\n\nhere\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed the\nsteps explained in the \ninstallation guide\n.\n\n\nHTTP Transport\n\n\nWhat you can customise\n\n\nVia ENV variables\n\n\nFor the documentation of the variables please refer to the\n\nglobal configuration docs\n.\n\n\n\n\n\n\nIOTA_VERSION\n: Version number (tag) of the\n  \nAgent Docker Image\n.\n\n\n\n\n\n\nIOTA_LOG_LEVEL\n: Defaults to \nDEBUG\n.\n\n\n\n\n\n\nIOTA_TIMESTAMP\n: Defaults to \ntrue\n.\n\n\n\n\n\n\nIOTA_CB_HOST\n: Defaults to \norion\n.\n\n\n\n\n\n\nIOTA_CB_PORT\n: Defaults to \n1026\n.\n\n\n\n\n\n\nIOTA_NORTH_PORT\n: Defaults to \n4041\n.\n\n\n\n\n\n\nIOTA_REGISTRY_TYPE\n: Defaults to \nmongodb\n.\n\n\n\n\n\n\nIOTA_MONGO_HOST\n: Defaults to \nmongo\n.\n\n\n\n\n\n\nIOTA_MONGO_PORT\n: Defaults to \n27017\n.\n\n\n\n\n\n\nIOTA_MONGO_DB\n: Defaults to \niotagentjson\n.\n\n\n\n\n\n\nIOTA_MONGO_REPLICASET\n: Defaults to \nrs\n. Unset to disable replicaset option.\n\n\n\n\n\n\nIOTA_HTTP_PORT\n: Defaults to \n7896\n.\n\n\n\n\n\n\nIOTA_PROVIDER_URL\n: Defaults to \nhttp://iotagent:4041\n.\n\n\n\n\n\n\nVia Files\n\n\n\n\nconfig.js\n: Feel free to edit this file before deployment, it will be used by\n  the agent as its config file. It is treated by docker as a\n  \nconfig\n.\n\n\n\n\nDeploying this recipe\n\n\nWe assume you have already setup your environment as explained in the\n\nInstallation\n.\n\n\n    docker stack deploy -c docker-compose.yml iota-ul\n\n\n\n\nThe deployed services will be:\n\n\n\n\nIoTAgent-ul", 
            "title": "IoT Agent UL"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#iot-agent-ul", 
            "text": "Official documentation of this IoT Agent: here", 
            "title": "IoT Agent (UL)"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed the\nsteps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#http-transport", 
            "text": "", 
            "title": "HTTP Transport"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#what-you-can-customise", 
            "text": "", 
            "title": "What you can customise"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#via-env-variables", 
            "text": "For the documentation of the variables please refer to the global configuration docs .    IOTA_VERSION : Version number (tag) of the\n   Agent Docker Image .    IOTA_LOG_LEVEL : Defaults to  DEBUG .    IOTA_TIMESTAMP : Defaults to  true .    IOTA_CB_HOST : Defaults to  orion .    IOTA_CB_PORT : Defaults to  1026 .    IOTA_NORTH_PORT : Defaults to  4041 .    IOTA_REGISTRY_TYPE : Defaults to  mongodb .    IOTA_MONGO_HOST : Defaults to  mongo .    IOTA_MONGO_PORT : Defaults to  27017 .    IOTA_MONGO_DB : Defaults to  iotagentjson .    IOTA_MONGO_REPLICASET : Defaults to  rs . Unset to disable replicaset option.    IOTA_HTTP_PORT : Defaults to  7896 .    IOTA_PROVIDER_URL : Defaults to  http://iotagent:4041 .", 
            "title": "Via ENV variables"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#via-files", 
            "text": "config.js : Feel free to edit this file before deployment, it will be used by\n  the agent as its config file. It is treated by docker as a\n   config .", 
            "title": "Via Files"
        }, 
        {
            "location": "/iot-services/iotagent-ul/README/#deploying-this-recipe", 
            "text": "We assume you have already setup your environment as explained in the Installation .      docker stack deploy -c docker-compose.yml iota-ul  The deployed services will be:   IoTAgent-ul", 
            "title": "Deploying this recipe"
        }, 
        {
            "location": "/security/readme/", 
            "text": "Security Management\n\n\nWithin FIWARE security is guaranteed by different services that provide the\nfollowing functionalities:\n\n\n\n\nIdentity Management (IDM): the reference implementation is currently\n    \nKeyRock\n\n\nPolicy Decision Point PDP service: the reference implementation is currently\n    \nauthzforce\n\n\nPolicy Enforcement Point (PEP): the reference implementation is currently\n    \nWilma\n, but is\n    going to be replaced soon by an extension of\n    \nAPI Umbrella\n.\n\n\n\n\nAll the above elements combined together provide you with an AAA solution for\nFIWARE APIs.\n\n\nIn the recipes, at the time being, we cover only the ongoing implementation\nof the PEP Proxy based on API Umbrella. This is due to the fact that:\n\n\n\n\nKeyRock is undergoing strong developments and new release will be announced\n    soon, and the current FIWARE Lab IDM can be used for any project without\n    need to deploy your own instance.\n\n\nPDP is required only in complex scenarios, and the PDP available in FIWARE Lab\n    can be used for any project without need to deploy your own instance.", 
            "title": "Intro"
        }, 
        {
            "location": "/security/readme/#security-management", 
            "text": "Within FIWARE security is guaranteed by different services that provide the\nfollowing functionalities:   Identity Management (IDM): the reference implementation is currently\n     KeyRock  Policy Decision Point PDP service: the reference implementation is currently\n     authzforce  Policy Enforcement Point (PEP): the reference implementation is currently\n     Wilma , but is\n    going to be replaced soon by an extension of\n     API Umbrella .   All the above elements combined together provide you with an AAA solution for\nFIWARE APIs.  In the recipes, at the time being, we cover only the ongoing implementation\nof the PEP Proxy based on API Umbrella. This is due to the fact that:   KeyRock is undergoing strong developments and new release will be announced\n    soon, and the current FIWARE Lab IDM can be used for any project without\n    need to deploy your own instance.  PDP is required only in complex scenarios, and the PDP available in FIWARE Lab\n    can be used for any project without need to deploy your own instance.", 
            "title": "Security Management"
        }, 
        {
            "location": "/security/api-umbrella/readme/", 
            "text": "API Umbrella in HA\n\n\nThis recipe shows how to deploy an scalable\n\nAPI Umbrella instance\n\nservice backed with an scalable\n\nreplica set\n of MongoDB instances.\n\n\nAll elements will be running in docker containers, defined in docker-compose\nfiles. Actually, this recipe focuses on the deployment of the API Umbrella\nfrontend, reusing the \nmongodb replica recipe\n\nas its backend.\n\n\nAt the time being, other services, such as \nElastic Search\n\nfor logging api interactions and QoS are not deployed.\nThis is mostly due to the fact that API Umbrella supports only obsolete versions\nof Elastic Search (i.e. version 2, while current version is 6).\n\n\nThe final deployment is represented by the following picture:\n\n\n\n\nPrerequisites\n\n\nPlease make sure you read the \nwelcome page\n and followed\nthe steps explained in the \ninstallation guide\n.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 17.06-ce) already setup.\nIf you don't have one, checkout the \ntools\n section\nfor a quick way to setup a local swarm.\n\n\n$ miniswarm start 3\n$ eval $(docker-machine env ms-manager0)\n\n\n\n\nIn case you haven't done it yet for other recipes, deploy \nbackend\n and\n\nfrontend\n networks as described in the\n\ninstallation guide\n.\n\n\nAPI Umbrella needs a mongo database for its backend. If you have already\ndeployed Mongo within your cluster and would like to reuse that database, you\ncan skip the next step (deploying backend).\nYou will just need to pay attention to the variables\nyou define for API Umbrella to link to Mongo, namely, \nMONGO_SERVICE_URI\n\nand \nREPLICASET_NAME\n.\n\n\nOtherwise, if you prefer to make a new deployment of MongoDB just for API\nUmbrella, you can take a shortcut and run...\n\n\n$ sh deploy_back.sh\n\nCreating config mongo-rs_mongo-healthcheck\nCreating service mongo-rs_mongo\nCreating service mongo-rs_controller\n\n\n\n\nBeside that, given that Ruby driver for MongoDB is not supporting service\ndiscovery, you will need to expose the ports of the MongoDB server on the\ncluster to allow the connection to the Replica Set from API Umbrella.\n\n\nBe aware that this works only when you deploy (as in the script), your MongoDB\nin global mode.\n\n\n$ docker service update --publish-add published=27017,target=27017,protocol=tcp,mode=host mongo-rs_mongo\n\nmongo-rs_mongo\noverall progress: 1 out of 1 tasks \nw697ke0djs3c: running   [==================================================\n] \nverify: Service converged\n\n\n\n\nWait some time until the backend is ready, you can check the backed deployment\nrunning:\n\n\n$ docker stack ps mongo-rs\nID                  NAME                                       IMAGE                              NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS\nmxxrlexvj0r9        mongo-rs_mongo.z69rvapjce827l69b6zehceal   mongo:3.2                          ms-worker1          Running             Starting 9 seconds ago                        \nd74orl0f0q7a        mongo-rs_mongo.fw2ajm8zw4f12ut3sgffgdwsl   mongo:3.2                          ms-worker0          Running             Starting 15 seconds ago                       \na2wddzw2g2fg        mongo-rs_mongo.w697ke0djs3cfdf3bgbrcblam   mongo:3.2                          ms-manager0         Running             Starting 6 seconds ago                        \nnero0vahaa8h        mongo-rs_controller.1                      martel/mongo-replica-ctrl:latest   ms-manager0         Running             Running 5 seconds ago\n\n\n\n\nSet the connection url for mongo based on the IPs of your Swarm Cluster\n(alternatively edit the \nfrontend.env\n file):\n\n\n$ MONGO_REPLICATE_SET_IPS=192.168.99.100:27017,192.168.99.101:27017,192.168.99.102:27017\n$ export MONGO_REPLICATE_SET_IPS\n\n\n\n\nIf you used \nminiswarm\n to create your cluster, you can get the different IPs\nusing the \ndocker-machine ip\n command, e.g.:\n\n\n$ docker-machine ip ms-manager0\n\n$ docker-machine ip ms-worker0\n\n$ docker-machine ip ms-worker1\n\n\n\n\nwhen all services will be in status ready, your backend is ready to be used:\n\n\n$ sh deploy_front.sh\n\ngenerating config file\nreplacing target file  api-umbrella.yml\nreplace mongodb with mongo-rs_mongo\nreplacing target file  api-umbrella.yml\nreplace rs_name with rs\nCreating config api_api-umbrella\nCreating service api_api-umbrella\n\n\n\n\nWhen also the frontend services will be running, your deployment\nwill look like this:\n\n\n$ docker service ls\n\nID                  NAME                  MODE                REPLICAS            IMAGE                                 PORTS\nca11lmx40tu5        api_api-umbrella      replicated          2/2                 martel/api-umbrella:0.14.4-1-fiware   *:80-\n80/tcp,*:443-\n443/tcp\nte1i0vhwtmnw        mongo-rs_controller   replicated          1/1                 martel/mongo-replica-ctrl:latest      \nrbo2oe2y0d72        mongo-rs_mongo        global              3/3                 mongo:3.2\n\n\n\n\nIf you see \n3/3\nin the replicas column it means the 3 out of 3 planned replicas\nare up and running.\n\n\nA walkthrough\n\n\nIn the following walkthrough we will explain how to do the initial configuration\nof API Umbrella and register your first API. For more details read \n\nAPI Umbrella's documentation\n.\n\n\n\n\nLet's create the admin user in API Umbrella. As first thing,\n    get the IP of your master node:\n\n\n\n\nbash\n  $ docker-machine ip ms-manager0\n\n\nOpen the browser at the following endpoint:\n`http://\nyour-cluster-manager-ip\n/admin`.\n\nUnless you also created certificates for your server, API Umbrella\nwill ask you to accept the connection to an insecure instance.\n\nIn the page displayed you can enter the admin user name and the password.\n\nNow you are logged in and you can configure the backend APIs.\n\n**N.B.:** The usage of the cluster master IP is just a convention, you can\nreach the services also at the IPs of the worker nodes.\n\n\n\n\n\n\n\nRetrieve \nX-Admin-Auth-Token\n Access and \nX-Api-Key\n.\n    In the menu select \nUsers-\nAdmin Accounts\n and click on the username\n    you just created. Copy the \nAdmin API Access\n for your account.\n\n\nIn the menu select \nUsers-\nApi Users\n click on the username\n\nweb.admin.ajax@internal.apiumbrella\n and copy the API\nKey (of course you can create new ones instead of reusing API Umbrella\ndefaults).\n\n\n\n\n\n\nRegister an new API. Create a simple API to test that everything works:\n\n\n\n\n\n\n```bash\n  $ curl -k -X POST \"https://\n/api-umbrella/v1/apis\" \\\n    -H \"X-Api-Key: \n\" \\\n    -H \"X-Admin-Auth-Token: \n\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Content-Type: application/json\" -d @- \n\",\n      \"backend_host\": \"maps.googleapis.com\", \n      \"servers\": [\n        {\n          \"host\": \"maps.googleapis.com\",\n          \"port\": 80\n        }\n      ],\n      \"url_matches\": [\n        {\n          \"frontend_prefix\": \"/distance2/\",\n          \"backend_prefix\": \"/\"\n        }\n      ],\n      \"balance_algorithm\": \"least_conn\",\n      \"settings\": {\n        \"require_https\":\"required_return_error\",\n        \"require_idp\": \"fiware-oauth2\",\n        \"disable_api_key\":\"false\",\n        \"api_key_verification_level\":\"none\",\n        \"rate_limit_mode\":\"unlimited\",\n        \"error_templates\": {},\n        \"error_data\": {}\n      }  \n\n    }\n  }\n  EOF\n\n\nResponse:\n  {\n    \"api\": {\n      \"backend_host\": \"maps.googleapis.com\",\n      \"backend_protocol\": \"http\",\n      \"balance_algorithm\": \"least_conn\",\n      \"created_at\": \"2018-02-26T13:47:02Z\",\n      \"created_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n      \"deleted_at\": null,\n      \"frontend_host\": \"\n\",\n      \"name\": \"distance FIWARE REST\",\n      \"servers\": [\n        {\n          \"host\": \"maps.googleapis.com\",\n          \"port\": 80,\n          \"id\": \"f0f7a039-d88c-4ef8-8798-a00ad3c8fcdb\"\n        }\n      ],\n      \"settings\": {\n        \"allowed_ips\": null,\n        \"allowed_referers\": null,\n        \"anonymous_rate_limit_behavior\": null,\n        \"api_key_verification_level\": \"none\",\n        \"api_key_verification_transition_start_at\": null,\n        \"append_query_string\": null,\n        \"authenticated_rate_limit_behavior\": null,\n        \"disable_api_key\": false,\n        \"error_data\": null,\n        \"error_templates\": {},\n        \"http_basic_auth\": null,\n        \"pass_api_key_header\": null,\n        \"pass_api_key_query_param\": null,\n        \"rate_limit_mode\": \"unlimited\",\n        \"require_https\": \"required_return_error\",\n        \"require_https_transition_start_at\": null,\n        \"require_idp\": \"fiware-oauth2\",\n        \"required_roles\": null,\n        \"required_roles_override\": null,\n        \"error_data_yaml_strings\": {},\n        \"headers_string\": \"\",\n        \"default_response_headers_string\": \"\",\n        \"override_response_headers_string\": \"\",\n        \"id\": \"4dfe22af-c12a-4733-807d-0a668c413a96\",\n        \"default_response_headers\": null,\n        \"headers\": null,\n        \"override_response_headers\": null,\n        \"rate_limits\": null\n      },\n      \"sort_order\": 100000,\n      \"updated_at\": \"2018-02-26T13:47:02Z\",\n      \"updated_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n      \"url_matches\": [\n        {\n          \"backend_prefix\": \"/\",\n          \"frontend_prefix\": \"/distance2/\",\n          \"id\": \"ec719b9f-2020-4eb9-8744-5cb2bae4b625\"\n        }\n      ],\n      \"version\": 1,\n      \"id\": \"cbe24047-7f74-4eb5-bd7e-211c3f8ede22\",\n      \"rewrites\": null,\n      \"sub_settings\": null,\n      \"creator\": {\n        \"username\": \"xxx\"\n      },\n      \"updater\": {\n        \"username\": \"xxx\"\n      }\n    }\n  }\n  EOF\n  ```\n\n\n\n\nPublish the newly registered API.\n\n\n\n\n```bash\n  $ curl -k -X POST \"https://\n/api-umbrella/v1/config/publish\" \\\n    -H \"X-Api-Key: \n\" \\\n    -H \"X-Admin-Auth-Token: \n\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Content-Type: application/json\" -d @- \nEOF\n  {\n    \"config\": {\n      \"apis\": {\n        \"cbe24047-7f74-4eb5-bd7e-211c3f8ede22\": {\n          \"publish\": \"1\"\n        }\n      },\n      \"website_backends\": {\n      }\n    }\n  }\n  EOF\n\n\nResponse:\n\n\n{\n    \"config_version\": {\n      \"config\": {\n        \"apis\": [\n          {\n            \"_id\": \"cbe24047-7f74-4eb5-bd7e-211c3f8ede22\",\n            \"version\": 2,\n            \"deleted_at\": null,\n            \"name\": \"distance FIWARE REST\",\n            \"sort_order\": 100000,\n            \"backend_protocol\": \"http\",\n            \"frontend_host\": \"192.168.99.100\",\n            \"backend_host\": \"maps.googleapis.com\",\n            \"balance_algorithm\": \"least_conn\",\n            \"updated_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n            \"updated_at\": \"2018-02-26T14:02:08Z\",\n            \"created_at\": \"2018-02-26T13:47:02Z\",\n            \"created_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n            \"settings\": {\n              \"require_https\": \"required_return_error\",\n              \"disable_api_key\": false,\n              \"api_key_verification_level\": \"none\",\n              \"require_idp\": \"fiware-oauth2\",\n              \"rate_limit_mode\": \"unlimited\",\n              \"error_templates\": {},\n              \"_id\": \"4dfe22af-c12a-4733-807d-0a668c413a96\",\n              \"anonymous_rate_limit_behavior\": \"ip_fallback\",\n              \"authenticated_rate_limit_behavior\": \"all\",\n              \"error_data\": {}\n            },\n            \"servers\": [\n              {\n                \"host\": \"maps.googleapis.com\",\n                \"port\": 80,\n                \"_id\": \"f0f7a039-d88c-4ef8-8798-a00ad3c8fcdb\"\n              }\n            ],\n            \"url_matches\": [\n              {\n                \"frontend_prefix\": \"/distance2/\",\n                \"backend_prefix\": \"/\",\n                \"_id\": \"ec719b9f-2020-4eb9-8744-5cb2bae4b625\"\n              }\n            ]\n          }\n        ],\n        \"website_backends\": []\n      },\n      \"created_at\": \"2018-02-26T14:03:53Z\",\n      \"updated_at\": \"2018-02-26T14:03:53Z\",\n      \"version\": \"2018-02-26T14:03:53Z\",\n      \"id\": {\n        \"$oid\": \"5a9413c99f9d04008c5a0b6c\"\n      }\n    }\n  }\n  ```\n\n\n\n\n\n\nTest your new API, by issuing a query:\n\n\n\n\n\n\nGet a token from FIWARE:\n\n\n```bash\n$ wget --no-check-certificate https://raw.githubusercontent.com/fgalan/oauth2-example-orion-client/master/token_script.sh\n$ bash token_script.sh\n\n\nUsername: your_email@example.com\nPassword:\nToken: \n\n```\n\n\n\n\n\n\nUse it to make a query to your API:\n\n\n```bash\n$ curl -k \"https://\n/distance2/maps/api/distancematrix/json?units=imperial\norigins=Washington,DC\ndestinations=New+York+City,NY\ntoken=\n\"\n\n\nResponse:\n{\n   \"destination_addresses\" : [ \"New York, NY, USA\" ],\n   \"origin_addresses\" : [ \"Washington, DC, USA\" ],\n   \"rows\" : [\n      {\n         \"elements\" : [\n            {\n               \"distance\" : {\n                  \"text\" : \"225 mi\",\n                  \"value\" : 361940\n               },\n               \"duration\" : {\n                  \"text\" : \"3 hours 50 mins\",\n                  \"value\" : 13816\n               },\n               \"status\" : \"OK\"\n            }\n         ]\n      }\n   ],\n   \"status\" : \"OK\"\n}\n```\n\n\n\n\n\n\nNetworks considerations\n\n\nIn this case, all containers are attached to the same overlay network (backend)\nover which they communicate to each other. However, if you have a different\nconfiguration and are running any of the containers behind a firewall, remember\nto keep traffic open for TCP at ports 80 and 443 (API Umbrellas's default) and\n27017 (Mongo's default).\n\n\nWhen containers (tasks) of a service are launched, they get assigned an IP\naddress in this overlay network. Other services of your application's\narchitecture should not be relying on these IPs because they may change\n(for example, due to a dynamic rescheduling). The good think is that docker\ncreates a virtual ip for the service as a whole, so all traffic to this address\nwill be load-balanced to the tasks addresses.\n\n\nThanks to Docker Swarm internal DNS you can also use the name of the service\nto connect to. If you look at the \ndocker-compose.yml\n file of this recipe,\norion is started with the name of the mongo service as \ndbhost\n param\n(regardless if it was a single mongo instance of a whole replica-set).\n\n\nHowever, to access the container from outside of the overlay network (for\nexample from the host) you would need to access the ip of the container's\ninterface to the \ndocker_gwbridge\n. It seem there's no easy way to get that\ninformation from the outside (see\n\nthis open issue\n.\nIn the walkthrough, we queried API Umbrella through one of the swarm nodes\nbecause we rely on docker ingress network routing the traffic all the way\nto one of the containerized API Umbrella services.\n\n\nOpen interesting issues\n\n\n\n\n\n\nhttps://github.com/docker/swarm/issues/1106\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/27082\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/29816\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/26696\n\n\n\n\n\n\nhttps://github.com/docker/docker/issues/23813\n\n\n\n\n\n\nMore info about docker network internals can be read at:\n\n\n\n\nDocker Reference Architecture", 
            "title": "API Umbrella"
        }, 
        {
            "location": "/security/api-umbrella/readme/#api-umbrella-in-ha", 
            "text": "This recipe shows how to deploy an scalable API Umbrella instance \nservice backed with an scalable replica set  of MongoDB instances.  All elements will be running in docker containers, defined in docker-compose\nfiles. Actually, this recipe focuses on the deployment of the API Umbrella\nfrontend, reusing the  mongodb replica recipe \nas its backend.  At the time being, other services, such as  Elastic Search \nfor logging api interactions and QoS are not deployed.\nThis is mostly due to the fact that API Umbrella supports only obsolete versions\nof Elastic Search (i.e. version 2, while current version is 6).  The final deployment is represented by the following picture:", 
            "title": "API Umbrella in HA"
        }, 
        {
            "location": "/security/api-umbrella/readme/#prerequisites", 
            "text": "Please make sure you read the  welcome page  and followed\nthe steps explained in the  installation guide .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/security/api-umbrella/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 17.06-ce) already setup.\nIf you don't have one, checkout the  tools  section\nfor a quick way to setup a local swarm.  $ miniswarm start 3\n$ eval $(docker-machine env ms-manager0)  In case you haven't done it yet for other recipes, deploy  backend  and frontend  networks as described in the installation guide .  API Umbrella needs a mongo database for its backend. If you have already\ndeployed Mongo within your cluster and would like to reuse that database, you\ncan skip the next step (deploying backend).\nYou will just need to pay attention to the variables\nyou define for API Umbrella to link to Mongo, namely,  MONGO_SERVICE_URI \nand  REPLICASET_NAME .  Otherwise, if you prefer to make a new deployment of MongoDB just for API\nUmbrella, you can take a shortcut and run...  $ sh deploy_back.sh\n\nCreating config mongo-rs_mongo-healthcheck\nCreating service mongo-rs_mongo\nCreating service mongo-rs_controller  Beside that, given that Ruby driver for MongoDB is not supporting service\ndiscovery, you will need to expose the ports of the MongoDB server on the\ncluster to allow the connection to the Replica Set from API Umbrella.  Be aware that this works only when you deploy (as in the script), your MongoDB\nin global mode.  $ docker service update --publish-add published=27017,target=27017,protocol=tcp,mode=host mongo-rs_mongo\n\nmongo-rs_mongo\noverall progress: 1 out of 1 tasks \nw697ke0djs3c: running   [================================================== ] \nverify: Service converged  Wait some time until the backend is ready, you can check the backed deployment\nrunning:  $ docker stack ps mongo-rs\nID                  NAME                                       IMAGE                              NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS\nmxxrlexvj0r9        mongo-rs_mongo.z69rvapjce827l69b6zehceal   mongo:3.2                          ms-worker1          Running             Starting 9 seconds ago                        \nd74orl0f0q7a        mongo-rs_mongo.fw2ajm8zw4f12ut3sgffgdwsl   mongo:3.2                          ms-worker0          Running             Starting 15 seconds ago                       \na2wddzw2g2fg        mongo-rs_mongo.w697ke0djs3cfdf3bgbrcblam   mongo:3.2                          ms-manager0         Running             Starting 6 seconds ago                        \nnero0vahaa8h        mongo-rs_controller.1                      martel/mongo-replica-ctrl:latest   ms-manager0         Running             Running 5 seconds ago  Set the connection url for mongo based on the IPs of your Swarm Cluster\n(alternatively edit the  frontend.env  file):  $ MONGO_REPLICATE_SET_IPS=192.168.99.100:27017,192.168.99.101:27017,192.168.99.102:27017\n$ export MONGO_REPLICATE_SET_IPS  If you used  miniswarm  to create your cluster, you can get the different IPs\nusing the  docker-machine ip  command, e.g.:  $ docker-machine ip ms-manager0\n\n$ docker-machine ip ms-worker0\n\n$ docker-machine ip ms-worker1  when all services will be in status ready, your backend is ready to be used:  $ sh deploy_front.sh\n\ngenerating config file\nreplacing target file  api-umbrella.yml\nreplace mongodb with mongo-rs_mongo\nreplacing target file  api-umbrella.yml\nreplace rs_name with rs\nCreating config api_api-umbrella\nCreating service api_api-umbrella  When also the frontend services will be running, your deployment\nwill look like this:  $ docker service ls\n\nID                  NAME                  MODE                REPLICAS            IMAGE                                 PORTS\nca11lmx40tu5        api_api-umbrella      replicated          2/2                 martel/api-umbrella:0.14.4-1-fiware   *:80- 80/tcp,*:443- 443/tcp\nte1i0vhwtmnw        mongo-rs_controller   replicated          1/1                 martel/mongo-replica-ctrl:latest      \nrbo2oe2y0d72        mongo-rs_mongo        global              3/3                 mongo:3.2  If you see  3/3 in the replicas column it means the 3 out of 3 planned replicas\nare up and running.", 
            "title": "How to use"
        }, 
        {
            "location": "/security/api-umbrella/readme/#a-walkthrough", 
            "text": "In the following walkthrough we will explain how to do the initial configuration\nof API Umbrella and register your first API. For more details read  API Umbrella's documentation .   Let's create the admin user in API Umbrella. As first thing,\n    get the IP of your master node:   bash\n  $ docker-machine ip ms-manager0  Open the browser at the following endpoint:\n`http:// your-cluster-manager-ip /admin`.\n\nUnless you also created certificates for your server, API Umbrella\nwill ask you to accept the connection to an insecure instance.\n\nIn the page displayed you can enter the admin user name and the password.\n\nNow you are logged in and you can configure the backend APIs.\n\n**N.B.:** The usage of the cluster master IP is just a convention, you can\nreach the services also at the IPs of the worker nodes.    Retrieve  X-Admin-Auth-Token  Access and  X-Api-Key .\n    In the menu select  Users- Admin Accounts  and click on the username\n    you just created. Copy the  Admin API Access  for your account.  In the menu select  Users- Api Users  click on the username web.admin.ajax@internal.apiumbrella  and copy the API\nKey (of course you can create new ones instead of reusing API Umbrella\ndefaults).    Register an new API. Create a simple API to test that everything works:    ```bash\n  $ curl -k -X POST \"https:// /api-umbrella/v1/apis\" \\\n    -H \"X-Api-Key:  \" \\\n    -H \"X-Admin-Auth-Token:  \" \\\n    -H \"Accept: application/json\" \\\n    -H \"Content-Type: application/json\" -d @-  \",\n      \"backend_host\": \"maps.googleapis.com\", \n      \"servers\": [\n        {\n          \"host\": \"maps.googleapis.com\",\n          \"port\": 80\n        }\n      ],\n      \"url_matches\": [\n        {\n          \"frontend_prefix\": \"/distance2/\",\n          \"backend_prefix\": \"/\"\n        }\n      ],\n      \"balance_algorithm\": \"least_conn\",\n      \"settings\": {\n        \"require_https\":\"required_return_error\",\n        \"require_idp\": \"fiware-oauth2\",\n        \"disable_api_key\":\"false\",\n        \"api_key_verification_level\":\"none\",\n        \"rate_limit_mode\":\"unlimited\",\n        \"error_templates\": {},\n        \"error_data\": {}\n      }   \n    }\n  }\n  EOF  Response:\n  {\n    \"api\": {\n      \"backend_host\": \"maps.googleapis.com\",\n      \"backend_protocol\": \"http\",\n      \"balance_algorithm\": \"least_conn\",\n      \"created_at\": \"2018-02-26T13:47:02Z\",\n      \"created_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n      \"deleted_at\": null,\n      \"frontend_host\": \" \",\n      \"name\": \"distance FIWARE REST\",\n      \"servers\": [\n        {\n          \"host\": \"maps.googleapis.com\",\n          \"port\": 80,\n          \"id\": \"f0f7a039-d88c-4ef8-8798-a00ad3c8fcdb\"\n        }\n      ],\n      \"settings\": {\n        \"allowed_ips\": null,\n        \"allowed_referers\": null,\n        \"anonymous_rate_limit_behavior\": null,\n        \"api_key_verification_level\": \"none\",\n        \"api_key_verification_transition_start_at\": null,\n        \"append_query_string\": null,\n        \"authenticated_rate_limit_behavior\": null,\n        \"disable_api_key\": false,\n        \"error_data\": null,\n        \"error_templates\": {},\n        \"http_basic_auth\": null,\n        \"pass_api_key_header\": null,\n        \"pass_api_key_query_param\": null,\n        \"rate_limit_mode\": \"unlimited\",\n        \"require_https\": \"required_return_error\",\n        \"require_https_transition_start_at\": null,\n        \"require_idp\": \"fiware-oauth2\",\n        \"required_roles\": null,\n        \"required_roles_override\": null,\n        \"error_data_yaml_strings\": {},\n        \"headers_string\": \"\",\n        \"default_response_headers_string\": \"\",\n        \"override_response_headers_string\": \"\",\n        \"id\": \"4dfe22af-c12a-4733-807d-0a668c413a96\",\n        \"default_response_headers\": null,\n        \"headers\": null,\n        \"override_response_headers\": null,\n        \"rate_limits\": null\n      },\n      \"sort_order\": 100000,\n      \"updated_at\": \"2018-02-26T13:47:02Z\",\n      \"updated_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n      \"url_matches\": [\n        {\n          \"backend_prefix\": \"/\",\n          \"frontend_prefix\": \"/distance2/\",\n          \"id\": \"ec719b9f-2020-4eb9-8744-5cb2bae4b625\"\n        }\n      ],\n      \"version\": 1,\n      \"id\": \"cbe24047-7f74-4eb5-bd7e-211c3f8ede22\",\n      \"rewrites\": null,\n      \"sub_settings\": null,\n      \"creator\": {\n        \"username\": \"xxx\"\n      },\n      \"updater\": {\n        \"username\": \"xxx\"\n      }\n    }\n  }\n  EOF\n  ```   Publish the newly registered API.   ```bash\n  $ curl -k -X POST \"https:// /api-umbrella/v1/config/publish\" \\\n    -H \"X-Api-Key:  \" \\\n    -H \"X-Admin-Auth-Token:  \" \\\n    -H \"Accept: application/json\" \\\n    -H \"Content-Type: application/json\" -d @-  EOF\n  {\n    \"config\": {\n      \"apis\": {\n        \"cbe24047-7f74-4eb5-bd7e-211c3f8ede22\": {\n          \"publish\": \"1\"\n        }\n      },\n      \"website_backends\": {\n      }\n    }\n  }\n  EOF  Response:  {\n    \"config_version\": {\n      \"config\": {\n        \"apis\": [\n          {\n            \"_id\": \"cbe24047-7f74-4eb5-bd7e-211c3f8ede22\",\n            \"version\": 2,\n            \"deleted_at\": null,\n            \"name\": \"distance FIWARE REST\",\n            \"sort_order\": 100000,\n            \"backend_protocol\": \"http\",\n            \"frontend_host\": \"192.168.99.100\",\n            \"backend_host\": \"maps.googleapis.com\",\n            \"balance_algorithm\": \"least_conn\",\n            \"updated_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n            \"updated_at\": \"2018-02-26T14:02:08Z\",\n            \"created_at\": \"2018-02-26T13:47:02Z\",\n            \"created_by\": \"c9d7c2cf-737c-46ae-974b-22ebc12cce0c\",\n            \"settings\": {\n              \"require_https\": \"required_return_error\",\n              \"disable_api_key\": false,\n              \"api_key_verification_level\": \"none\",\n              \"require_idp\": \"fiware-oauth2\",\n              \"rate_limit_mode\": \"unlimited\",\n              \"error_templates\": {},\n              \"_id\": \"4dfe22af-c12a-4733-807d-0a668c413a96\",\n              \"anonymous_rate_limit_behavior\": \"ip_fallback\",\n              \"authenticated_rate_limit_behavior\": \"all\",\n              \"error_data\": {}\n            },\n            \"servers\": [\n              {\n                \"host\": \"maps.googleapis.com\",\n                \"port\": 80,\n                \"_id\": \"f0f7a039-d88c-4ef8-8798-a00ad3c8fcdb\"\n              }\n            ],\n            \"url_matches\": [\n              {\n                \"frontend_prefix\": \"/distance2/\",\n                \"backend_prefix\": \"/\",\n                \"_id\": \"ec719b9f-2020-4eb9-8744-5cb2bae4b625\"\n              }\n            ]\n          }\n        ],\n        \"website_backends\": []\n      },\n      \"created_at\": \"2018-02-26T14:03:53Z\",\n      \"updated_at\": \"2018-02-26T14:03:53Z\",\n      \"version\": \"2018-02-26T14:03:53Z\",\n      \"id\": {\n        \"$oid\": \"5a9413c99f9d04008c5a0b6c\"\n      }\n    }\n  }\n  ```    Test your new API, by issuing a query:    Get a token from FIWARE:  ```bash\n$ wget --no-check-certificate https://raw.githubusercontent.com/fgalan/oauth2-example-orion-client/master/token_script.sh\n$ bash token_script.sh  Username: your_email@example.com\nPassword:\nToken:  \n```    Use it to make a query to your API:  ```bash\n$ curl -k \"https:// /distance2/maps/api/distancematrix/json?units=imperial origins=Washington,DC destinations=New+York+City,NY token= \"  Response:\n{\n   \"destination_addresses\" : [ \"New York, NY, USA\" ],\n   \"origin_addresses\" : [ \"Washington, DC, USA\" ],\n   \"rows\" : [\n      {\n         \"elements\" : [\n            {\n               \"distance\" : {\n                  \"text\" : \"225 mi\",\n                  \"value\" : 361940\n               },\n               \"duration\" : {\n                  \"text\" : \"3 hours 50 mins\",\n                  \"value\" : 13816\n               },\n               \"status\" : \"OK\"\n            }\n         ]\n      }\n   ],\n   \"status\" : \"OK\"\n}\n```", 
            "title": "A walkthrough"
        }, 
        {
            "location": "/security/api-umbrella/readme/#networks-considerations", 
            "text": "In this case, all containers are attached to the same overlay network (backend)\nover which they communicate to each other. However, if you have a different\nconfiguration and are running any of the containers behind a firewall, remember\nto keep traffic open for TCP at ports 80 and 443 (API Umbrellas's default) and\n27017 (Mongo's default).  When containers (tasks) of a service are launched, they get assigned an IP\naddress in this overlay network. Other services of your application's\narchitecture should not be relying on these IPs because they may change\n(for example, due to a dynamic rescheduling). The good think is that docker\ncreates a virtual ip for the service as a whole, so all traffic to this address\nwill be load-balanced to the tasks addresses.  Thanks to Docker Swarm internal DNS you can also use the name of the service\nto connect to. If you look at the  docker-compose.yml  file of this recipe,\norion is started with the name of the mongo service as  dbhost  param\n(regardless if it was a single mongo instance of a whole replica-set).  However, to access the container from outside of the overlay network (for\nexample from the host) you would need to access the ip of the container's\ninterface to the  docker_gwbridge . It seem there's no easy way to get that\ninformation from the outside (see this open issue .\nIn the walkthrough, we queried API Umbrella through one of the swarm nodes\nbecause we rely on docker ingress network routing the traffic all the way\nto one of the containerized API Umbrella services.", 
            "title": "Networks considerations"
        }, 
        {
            "location": "/security/api-umbrella/readme/#open-interesting-issues", 
            "text": "https://github.com/docker/swarm/issues/1106    https://github.com/docker/docker/issues/27082    https://github.com/docker/docker/issues/29816    https://github.com/docker/docker/issues/26696    https://github.com/docker/docker/issues/23813    More info about docker network internals can be read at:   Docker Reference Architecture", 
            "title": "Open interesting issues"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/", 
            "text": "MongoDB Replica Set\n\n\nThis recipe aims to deploy and control a\n\nreplica set\n of MongoDB\ninstances in a Docker Swarm.\n\n\n\n\nRequirements\n\n\nPlease make sure you read the \nwelcome page\n and followed the\nsteps explained in the \ninstallation guide\n.\n\n\nHow to use\n\n\nFirstly, you need to have a Docker Swarm (docker \n= 1.13) already setup.\nIf you don't have one, checkout the \ntools\n section for\na quick way to setup a local swarm.\n\n\n    miniswarm start 3\n    eval $(docker-machine env ms-manager0)\n\n\n\n\nThen, simply run...\n\n\n    sh deploy.sh\n\n\n\n\nAllow some time while images are pulled in the nodes and services are deployed.\nAfter a couple of minutes, you can check if all services are up, as usual,\nrunning...\n\n\n    $ docker service ls\n    ID            NAME                            MODE        REPLICAS  IMAGE\n    fjxof1n5ce58  mongo-rs_mongo             global      3/3       mongo:latest\n    yzsur7rb4mg1  mongo-rs_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n\n\n\nA Walkthrough\n\n\nAs shown before, the recipe consists of basically two services, namely, one for\nmongo instances and one for controlling the replica-set.\n\n\nThe mongo service is deployed in \"global\" mode, meaning that docker will run one\ninstance of mongod per swarm node in the cluster.\n\n\nAt the swarm's master node, a python-based controller script will be deployed to\nconfigure and maintain the mongodb replica-set.\n\n\nLet's now check that the controller worked fine inspecting the logs of the\n\nmongo_mongo-controller\n service. This can be done with either...\n\n\n    $ docker service logs mongo-rs_mongo-controller\n\n\n\n\nor running the following...\n\n\n    $  docker logs $(docker ps -f \nname=mongo-rs_mongo-controller\n -q)\n    INFO:__main__:Waiting some time before starting\n    INFO:__main__:Initial config: {'version': 1, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}]}\n    INFO:__main__:replSetInitiate: {'ok': 1.0}\n\n\n\n\nAs you can see, the replica-set was configured with 3 replicas represented by\ncontainers running in the same overlay network. You can also run a mongo command\nin any of the mongo containers and execute \nrs.status()\n to see the same\nresults.\n\n\n    $ docker exec -ti d56d17c40f8f mongo rs:SECONDARY\n rs.status()\n\n\n\n\nRescaling the replica-set\n\n\nLet's add a new node to the swarm to see how docker deploys a new task of the\nmongo service and the controller automatically adds it to the replica-set.\n\n\n    # First get the token to join the swarm\n    $ docker swarm join-token worker\n\n    # Create the new node\n    $ docker-machine create -d virtualbox ms-worker2\n    $ docker-machine ssh ms-worker2\n\n    docker@ms-worker2:~$ docker swarm join \\\n    --token INSERT_TOKEN_HERE \\\n    192.168.99.100:2377\n\n    docker@ms-worker2:~$ exit\n\n\n\n\nBack to the host, some minutes later...\n\n\n    $ docker service ls\n    ID            NAME                            MODE        REPLICAS  IMAGE\n    fjxof1n5ce58  mongo-rs_mongo             global      4/4       mongo:latest\n    yzsur7rb4mg1  mongo_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n    $ docker logs $(docker ps -f \nname=mongo_mongo-controller\n -q)\n    ...\n    INFO:__main__:To add: {'10.0.0.8'}\n    INFO:__main__:New config: {'version': 2, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}, {'_id': 3, 'host': '10.0.0.8:27017'}]}\n    INFO:__main__:replSetReconfig: {'ok': 1.0}\n\n\n\n\nIf a node goes down, the replica-set will be automatically reconfigured at\nthe application level by mongo. Docker, on the other hand, will not reschedule\nthe replica because it's expected to run one only one per node.\n\n\nNOTE\n: If you don't want to have a replica in every node of the swarm,\nthe solution for now is using a combination of constraints and node tags.\nYou can read more about this in\n\nthis Github issue\n.\n\n\nFor further details, refer to the \nmongo-rs-controller-swarm\n\nrepository, in particular the\n\n[docker-compose.yml](https://github.com/smartsdk/mongo-rs-controller-swarm/blob/master/docker-compose.yml)\n\nfile or the\n\n[replica_ctrl.py](https://github.com/smartsdk/mongo-rs-controller-swarm/blob/master/src/replica_ctrl.py)\n\ncontroller script.", 
            "title": "MongoDB Replica Set"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#mongodb-replica-set", 
            "text": "This recipe aims to deploy and control a replica set  of MongoDB\ninstances in a Docker Swarm.", 
            "title": "MongoDB Replica Set"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#requirements", 
            "text": "Please make sure you read the  welcome page  and followed the\nsteps explained in the  installation guide .", 
            "title": "Requirements"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#how-to-use", 
            "text": "Firstly, you need to have a Docker Swarm (docker  = 1.13) already setup.\nIf you don't have one, checkout the  tools  section for\na quick way to setup a local swarm.      miniswarm start 3\n    eval $(docker-machine env ms-manager0)  Then, simply run...      sh deploy.sh  Allow some time while images are pulled in the nodes and services are deployed.\nAfter a couple of minutes, you can check if all services are up, as usual,\nrunning...      $ docker service ls\n    ID            NAME                            MODE        REPLICAS  IMAGE\n    fjxof1n5ce58  mongo-rs_mongo             global      3/3       mongo:latest\n    yzsur7rb4mg1  mongo-rs_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest", 
            "title": "How to use"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#a-walkthrough", 
            "text": "As shown before, the recipe consists of basically two services, namely, one for\nmongo instances and one for controlling the replica-set.  The mongo service is deployed in \"global\" mode, meaning that docker will run one\ninstance of mongod per swarm node in the cluster.  At the swarm's master node, a python-based controller script will be deployed to\nconfigure and maintain the mongodb replica-set.  Let's now check that the controller worked fine inspecting the logs of the mongo_mongo-controller  service. This can be done with either...      $ docker service logs mongo-rs_mongo-controller  or running the following...      $  docker logs $(docker ps -f  name=mongo-rs_mongo-controller  -q)\n    INFO:__main__:Waiting some time before starting\n    INFO:__main__:Initial config: {'version': 1, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}]}\n    INFO:__main__:replSetInitiate: {'ok': 1.0}  As you can see, the replica-set was configured with 3 replicas represented by\ncontainers running in the same overlay network. You can also run a mongo command\nin any of the mongo containers and execute  rs.status()  to see the same\nresults.      $ docker exec -ti d56d17c40f8f mongo rs:SECONDARY  rs.status()", 
            "title": "A Walkthrough"
        }, 
        {
            "location": "/utils/mongo-replicaset/readme/#rescaling-the-replica-set", 
            "text": "Let's add a new node to the swarm to see how docker deploys a new task of the\nmongo service and the controller automatically adds it to the replica-set.      # First get the token to join the swarm\n    $ docker swarm join-token worker\n\n    # Create the new node\n    $ docker-machine create -d virtualbox ms-worker2\n    $ docker-machine ssh ms-worker2\n\n    docker@ms-worker2:~$ docker swarm join \\\n    --token INSERT_TOKEN_HERE \\\n    192.168.99.100:2377\n\n    docker@ms-worker2:~$ exit  Back to the host, some minutes later...      $ docker service ls\n    ID            NAME                            MODE        REPLICAS  IMAGE\n    fjxof1n5ce58  mongo-rs_mongo             global      4/4       mongo:latest\n    yzsur7rb4mg1  mongo_mongo-controller  replicated  1/1       martel/mongo-replica-ctrl:latest\n\n    $ docker logs $(docker ps -f  name=mongo_mongo-controller  -q)\n    ...\n    INFO:__main__:To add: {'10.0.0.8'}\n    INFO:__main__:New config: {'version': 2, '_id': 'rs', 'members': [{'_id': 0, 'host': '10.0.0.5:27017'}, {'_id': 1, 'host': '10.0.0.3:27017'}, {'_id': 2, 'host': '10.0.0.4:27017'}, {'_id': 3, 'host': '10.0.0.8:27017'}]}\n    INFO:__main__:replSetReconfig: {'ok': 1.0}  If a node goes down, the replica-set will be automatically reconfigured at\nthe application level by mongo. Docker, on the other hand, will not reschedule\nthe replica because it's expected to run one only one per node.  NOTE : If you don't want to have a replica in every node of the swarm,\nthe solution for now is using a combination of constraints and node tags.\nYou can read more about this in this Github issue .  For further details, refer to the  mongo-rs-controller-swarm \nrepository, in particular the [docker-compose.yml](https://github.com/smartsdk/mongo-rs-controller-swarm/blob/master/docker-compose.yml) \nfile or the [replica_ctrl.py](https://github.com/smartsdk/mongo-rs-controller-swarm/blob/master/src/replica_ctrl.py) \ncontroller script.", 
            "title": "Rescaling the replica-set"
        }, 
        {
            "location": "/tools/readme/", 
            "text": "Tools\n\n\nThis section contains useful (and sometimes temporary) scripts as well as\nreferences to tools, projects and pieces of documentation used for\nthe development of the recipes.\n\n\nThe basic environment setup is explained in the\n\nInstallation\n part of the docs.\n\n\nPlaying with Recipes?\n\n\nminiswarm\n\n\nHelpful tool to help you quickly setup a local virtualbox-based swarm cluster\nfor testing purposes.\n\n\nwait-for-it\n\n\nUseful shell script used when you need to wait for a service to be started.\n\n\nNote\n: This might no longer be needed since docker introduced the \nhealthchecks\n\nfeature.\n\n\ndocker-swarm-visualizer\n\n\nIf you'd like to have a basic view of the distribution of containers in your\nswarm cluster, you can use the \nvisualzer.yml\n file provided in this folder.\n\n\n        docker stack deploy -c visualizer.yml vis\n\n\n\n\nportainer\n\n\nIf you'd like a more sophisticated UI with info about your swarm, you can deploy\nportainer as follows.\n\n\n        docker service create \\\n        --name portainer \\\n        --publish 9000:9000 \\\n        --constraint 'node.role == manager' \\\n        --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\n        portainer/portainer \\\n        -H unix:///var/run/docker.sock\n\n\n\n\nAlternatively, you can make use of the docker-compose file available in\nthis folder.\n\n\n    docker stack deploy -c portainer.yml portainer\n\n\n\n\npostman\n\n\nA well-known tool for experimenting with APIs. Do you want to try the curl-based\nexamples of the recipes from Postman? Import the \npostman_collection.json\n\navailable in this folder and make your tests easier. Note: this collection is\nwork in progress, feel free to \ncontribute\n!\n\n\nWriting Docs?\n\n\nWe typically write documentation in\n\nmarkdown\n format.\nThen, \nmkdocs\n is used to generate the html format.\nYou can see in the root of this project the \nmkdocs.yml\n config file.\n\n\nFor architecture diagrams, we use \nPlantUML\n. For the\ndiagrams we follow the conventions and leverage on the features you can find\nin \nthis project\n.\n\n\nInstead of uploading pictures in this document, we use\n\ngravizo\n's power to convert the .dot or PlantUML files\nand have them served as pictures online. There is an intermediate conversion\ndone with \ngravizo's converter\n.\nInspect the source of any recipe's \nreadme.md\n to see an example.\n\n\nOther tools for documentation that you may find useful are...\n\n\ndraw.io\n\n\nUse this tool when the diagrams start getting too complex of when you foresee\nthe diagram will be complex from the scratch.\n\n\nComplex in the sense that making a simple change takes more time understanding\nthe \n.dot\n than making a manual gui-based change.\n\n\nWhen using draw.io, keep the source file in the repository under a \n/doc\n\nsubfolder of the corresponding recipe.\n\n\ncolor names\n\n\nThe reference for color names used in \n.dot\n files.\n\n\ndiagramr\n (deprecated)\n\n\nTo give more docker-related details we could use this tool to create diagrams\nfrom docker-compose files. The tools gives also the .dot file, which would\nbe eventually customized and then turned into a png file using\n\ngraphviz\n.\n\n\n        $ dot compose.dot -Tpng -o compose.png", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/readme/#tools", 
            "text": "This section contains useful (and sometimes temporary) scripts as well as\nreferences to tools, projects and pieces of documentation used for\nthe development of the recipes.  The basic environment setup is explained in the Installation  part of the docs.", 
            "title": "Tools"
        }, 
        {
            "location": "/tools/readme/#playing-with-recipes", 
            "text": "", 
            "title": "Playing with Recipes?"
        }, 
        {
            "location": "/tools/readme/#miniswarm", 
            "text": "Helpful tool to help you quickly setup a local virtualbox-based swarm cluster\nfor testing purposes.", 
            "title": "miniswarm"
        }, 
        {
            "location": "/tools/readme/#wait-for-it", 
            "text": "Useful shell script used when you need to wait for a service to be started.  Note : This might no longer be needed since docker introduced the  healthchecks \nfeature.", 
            "title": "wait-for-it"
        }, 
        {
            "location": "/tools/readme/#docker-swarm-visualizer", 
            "text": "If you'd like to have a basic view of the distribution of containers in your\nswarm cluster, you can use the  visualzer.yml  file provided in this folder.          docker stack deploy -c visualizer.yml vis", 
            "title": "docker-swarm-visualizer"
        }, 
        {
            "location": "/tools/readme/#portainer", 
            "text": "If you'd like a more sophisticated UI with info about your swarm, you can deploy\nportainer as follows.          docker service create \\\n        --name portainer \\\n        --publish 9000:9000 \\\n        --constraint 'node.role == manager' \\\n        --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \\\n        portainer/portainer \\\n        -H unix:///var/run/docker.sock  Alternatively, you can make use of the docker-compose file available in\nthis folder.      docker stack deploy -c portainer.yml portainer", 
            "title": "portainer"
        }, 
        {
            "location": "/tools/readme/#postman", 
            "text": "A well-known tool for experimenting with APIs. Do you want to try the curl-based\nexamples of the recipes from Postman? Import the  postman_collection.json \navailable in this folder and make your tests easier. Note: this collection is\nwork in progress, feel free to  contribute !", 
            "title": "postman"
        }, 
        {
            "location": "/tools/readme/#writing-docs", 
            "text": "We typically write documentation in markdown  format.\nThen,  mkdocs  is used to generate the html format.\nYou can see in the root of this project the  mkdocs.yml  config file.  For architecture diagrams, we use  PlantUML . For the\ndiagrams we follow the conventions and leverage on the features you can find\nin  this project .  Instead of uploading pictures in this document, we use gravizo 's power to convert the .dot or PlantUML files\nand have them served as pictures online. There is an intermediate conversion\ndone with  gravizo's converter .\nInspect the source of any recipe's  readme.md  to see an example.  Other tools for documentation that you may find useful are...", 
            "title": "Writing Docs?"
        }, 
        {
            "location": "/tools/readme/#drawio", 
            "text": "Use this tool when the diagrams start getting too complex of when you foresee\nthe diagram will be complex from the scratch.  Complex in the sense that making a simple change takes more time understanding\nthe  .dot  than making a manual gui-based change.  When using draw.io, keep the source file in the repository under a  /doc \nsubfolder of the corresponding recipe.", 
            "title": "draw.io"
        }, 
        {
            "location": "/tools/readme/#color-names", 
            "text": "The reference for color names used in  .dot  files.", 
            "title": "color names"
        }, 
        {
            "location": "/tools/readme/#diagramr-deprecated", 
            "text": "To give more docker-related details we could use this tool to create diagrams\nfrom docker-compose files. The tools gives also the .dot file, which would\nbe eventually customized and then turned into a png file using graphviz .          $ dot compose.dot -Tpng -o compose.png", 
            "title": "diagramr (deprecated)"
        }, 
        {
            "location": "/contributing/", 
            "text": "Contributing\n\n\nContributions are more than welcome in the form of\n\nPull Requests\n.\n\n\nFeel free to \nopen issues\n\nif something looks wrong.\n\n\nBe aware that during the CI process, a number of linters are run:\n\n\n\n\n\n\nTo ensure correctness of yml files, we recommend you to check\n  \nyaml linting rules\n.\n\n\n\n\n\n\nTo ensure consistency of the documentation style, we recommend you to adhere\n  to the MD \nlinting rules\n.\n\n\n\n\n\n\nOnce you make a pull request to the repository, you will be able to observe\nthe results of the compliancy verification in your PR. Merge will be only possible\nif CI process is passed successfully.\n\n\nDocumentation\n\n\nFor now we are using \nMkdocs\n deploying on\n\nGithub Pages\n.\n\n\nYou will also notice that instead of having a separate \ndocs\n folder,\nthe documentation is composed of the README's content of all subfolders so as\nto keep docs as close to the respective recipes as possible.\n\n\nIf you change the structure of the index or add new pages, remember to update\n\nmkdocs.yml\n accordingly.\n\n\nNote you can preview your changes locally by running\n\n\n    # from the location of mkdocs.yml\n    $ mkdocs serve\n\n\n\n\nAfter all your changes, remember to run\n\n\n    # from the location of mkdocs.yml\n    $ mkdocs gh-deploy", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#contributing", 
            "text": "Contributions are more than welcome in the form of Pull Requests .  Feel free to  open issues \nif something looks wrong.  Be aware that during the CI process, a number of linters are run:    To ensure correctness of yml files, we recommend you to check\n   yaml linting rules .    To ensure consistency of the documentation style, we recommend you to adhere\n  to the MD  linting rules .    Once you make a pull request to the repository, you will be able to observe\nthe results of the compliancy verification in your PR. Merge will be only possible\nif CI process is passed successfully.", 
            "title": "Contributing"
        }, 
        {
            "location": "/contributing/#documentation", 
            "text": "For now we are using  Mkdocs  deploying on Github Pages .  You will also notice that instead of having a separate  docs  folder,\nthe documentation is composed of the README's content of all subfolders so as\nto keep docs as close to the respective recipes as possible.  If you change the structure of the index or add new pages, remember to update mkdocs.yml  accordingly.  Note you can preview your changes locally by running      # from the location of mkdocs.yml\n    $ mkdocs serve  After all your changes, remember to run      # from the location of mkdocs.yml\n    $ mkdocs gh-deploy", 
            "title": "Documentation"
        }
    ]
}